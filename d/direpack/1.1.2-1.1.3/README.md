# Comparing `tmp/direpack-1.1.2-py3-none-any.whl.zip` & `tmp/direpack-1.1.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,44 @@
-Zip file size: 80068 bytes, number of entries: 42
--rw-rw-r--  2.0 unx     1106 b- defN 24-Feb-23 19:12 direpack/__init__.py
--rw-rw-r--  2.0 unx      255 b- defN 20-Apr-04 20:40 direpack/cross_validation/__init__.py
--rw-rw-r--  2.0 unx     2110 b- defN 20-Apr-04 20:40 direpack/cross_validation/_cv_support_functions.py
--rw-rw-r--  2.0 unx      248 b- defN 24-Feb-15 19:12 direpack/dicomo/__init__.py
--rw-rw-r--  2.0 unx    13152 b- defN 24-Feb-15 19:12 direpack/dicomo/_dicomo_utils.py
--rw-rw-r--  2.0 unx    18162 b- defN 24-Feb-15 19:12 direpack/dicomo/dicomo.py
--rw-rw-r--  2.0 unx      397 b- defN 21-Apr-15 20:47 direpack/ipopt_temp/__init__.py
--rw-rw-r--  2.0 unx     8058 b- defN 20-Apr-12 23:58 direpack/ipopt_temp/ipopt_wrapper.py
--rw-rw-r--  2.0 unx     2191 b- defN 20-Apr-12 23:29 direpack/ipopt_temp/jacobian.py
--rw-rw-r--  2.0 unx      245 b- defN 20-Apr-18 18:34 direpack/plot/__init__.py
--rw-rw-r--  2.0 unx     6468 b- defN 20-Jul-28 19:51 direpack/plot/ppdire_plot.py
--rw-rw-r--  2.0 unx    17165 b- defN 20-Apr-13 21:06 direpack/plot/sprm_plot.py
--rw-rw-r--  2.0 unx     6321 b- defN 20-Jul-28 19:52 direpack/plot/sudire_plot.py
--rw-rw-r--  2.0 unx      243 b- defN 22-Oct-22 18:34 direpack/ppdire/__init__.py
--rw-rw-r--  2.0 unx     6170 b- defN 22-Oct-09 01:21 direpack/ppdire/_ppdire_utils.py
--rw-rw-r--  2.0 unx     6928 b- defN 22-Oct-07 17:55 direpack/ppdire/capi.py
--rw-rw-r--  2.0 unx    34828 b- defN 24-Feb-15 19:12 direpack/ppdire/ppdire.py
--rw-rw-r--  2.0 unx      250 b- defN 24-Feb-23 17:11 direpack/preprocessing/__init__.py
--rw-rw-r--  2.0 unx     3867 b- defN 20-Jul-28 19:50 direpack/preprocessing/_gsspp_utils.py
--rw-rw-r--  2.0 unx     8644 b- defN 24-Feb-23 17:11 direpack/preprocessing/_preproc_utilities.py
--rw-rw-r--  2.0 unx     4934 b- defN 21-Apr-26 22:10 direpack/preprocessing/gsspp.py
--rw-rw-r--  2.0 unx     9187 b- defN 24-Feb-23 17:11 direpack/preprocessing/robcent.py
--rw-rw-r--  2.0 unx      241 b- defN 24-Feb-23 17:11 direpack/sprm/__init__.py
--rw-rw-r--  2.0 unx     1142 b- defN 22-Oct-07 17:55 direpack/sprm/_m_support_functions.py
--rw-rw-r--  2.0 unx    10180 b- defN 24-Feb-15 19:12 direpack/sprm/rm.py
--rw-rw-r--  2.0 unx    11309 b- defN 24-Feb-23 17:11 direpack/sprm/snipls.py
--rw-rw-r--  2.0 unx    21320 b- defN 24-Feb-15 20:58 direpack/sprm/sprm.py
--rw-rw-r--  2.0 unx      268 b- defN 22-Oct-09 23:44 direpack/sudire/__init__.py
--rw-rw-r--  2.0 unx    15804 b- defN 22-Oct-07 17:55 direpack/sudire/_sudire_utils.py
--rw-rw-r--  2.0 unx    30240 b- defN 22-Oct-22 18:34 direpack/sudire/sudire.py
--rw-rw-r--  2.0 unx      123 b- defN 24-Feb-23 17:49 direpack/test/__init__.py
--rw-rw-r--  2.0 unx     3752 b- defN 22-Jul-27 02:01 direpack/test/test_dicomo.py
--rw-rw-r--  2.0 unx     3148 b- defN 24-Feb-23 17:35 direpack/test/test_ppdire.py
--rw-rw-r--  2.0 unx     3091 b- defN 24-Feb-23 17:21 direpack/test/test_sprm.py
--rw-rw-r--  2.0 unx     5122 b- defN 23-Jan-25 20:10 direpack/test/test_sudire.py
--rw-rw-r--  2.0 unx      259 b- defN 24-Feb-23 17:11 direpack/utils/__init__.py
--rw-rw-r--  2.0 unx     2320 b- defN 24-Feb-23 17:11 direpack/utils/utils.py
--rw-rw-r--  2.0 unx     1070 b- defN 24-Feb-23 19:12 direpack-1.1.2.dist-info/LICENSE
--rw-rw-r--  2.0 unx    10221 b- defN 24-Feb-23 19:12 direpack-1.1.2.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 24-Feb-23 19:12 direpack-1.1.2.dist-info/WHEEL
--rw-rw-r--  2.0 unx        9 b- defN 24-Feb-23 19:12 direpack-1.1.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3575 b- defN 24-Feb-23 19:12 direpack-1.1.2.dist-info/RECORD
-42 files, 274215 bytes uncompressed, 74374 bytes compressed:  72.9%
+Zip file size: 80566 bytes, number of entries: 42
+-rw-rw-rw-  2.0 fat     1146 b- defN 24-May-24 22:14 direpack/__init__.py
+-rw-rw-rw-  2.0 fat      270 b- defN 24-May-22 21:04 direpack/cross_validation/__init__.py
+-rw-rw-rw-  2.0 fat     2167 b- defN 24-May-22 21:04 direpack/cross_validation/_cv_support_functions.py
+-rw-rw-rw-  2.0 fat      267 b- defN 24-May-22 21:04 direpack/dicomo/__init__.py
+-rw-rw-rw-  2.0 fat    13616 b- defN 24-May-22 21:04 direpack/dicomo/_dicomo_utils.py
+-rw-rw-rw-  2.0 fat    18614 b- defN 24-May-22 21:04 direpack/dicomo/dicomo.py
+-rw-rw-rw-  2.0 fat      416 b- defN 24-May-22 21:04 direpack/ipopt_temp/__init__.py
+-rw-rw-rw-  2.0 fat     8301 b- defN 24-May-22 21:04 direpack/ipopt_temp/ipopt_wrapper.py
+-rw-rw-rw-  2.0 fat     2256 b- defN 24-May-22 21:04 direpack/ipopt_temp/jacobian.py
+-rw-rw-rw-  2.0 fat      262 b- defN 24-May-22 21:04 direpack/plot/__init__.py
+-rw-rw-rw-  2.0 fat     6468 b- defN 24-May-22 21:04 direpack/plot/ppdire_plot.py
+-rw-rw-rw-  2.0 fat    17544 b- defN 24-May-22 21:04 direpack/plot/sprm_plot.py
+-rw-rw-rw-  2.0 fat     6321 b- defN 24-May-22 21:04 direpack/plot/sudire_plot.py
+-rw-rw-rw-  2.0 fat      256 b- defN 24-May-22 21:04 direpack/ppdire/__init__.py
+-rw-rw-rw-  2.0 fat     6358 b- defN 24-May-22 21:04 direpack/ppdire/_ppdire_utils.py
+-rw-rw-rw-  2.0 fat     7123 b- defN 24-May-22 21:04 direpack/ppdire/capi.py
+-rw-rw-rw-  2.0 fat    35720 b- defN 24-May-22 21:04 direpack/ppdire/ppdire.py
+-rw-rw-rw-  2.0 fat      263 b- defN 24-May-22 21:04 direpack/preprocessing/__init__.py
+-rw-rw-rw-  2.0 fat     4020 b- defN 24-May-22 21:04 direpack/preprocessing/_gsspp_utils.py
+-rw-rw-rw-  2.0 fat     9012 b- defN 24-May-22 21:04 direpack/preprocessing/_preproc_utilities.py
+-rw-rw-rw-  2.0 fat     5094 b- defN 24-May-22 21:04 direpack/preprocessing/gsspp.py
+-rw-rw-rw-  2.0 fat     9512 b- defN 24-May-22 21:04 direpack/preprocessing/robcent.py
+-rw-rw-rw-  2.0 fat      254 b- defN 24-May-24 22:13 direpack/sprm/__init__.py
+-rw-rw-rw-  2.0 fat     1185 b- defN 24-May-22 21:04 direpack/sprm/_m_support_functions.py
+-rw-rw-rw-  2.0 fat    10451 b- defN 24-May-22 21:04 direpack/sprm/rm.py
+-rw-rw-rw-  2.0 fat    11637 b- defN 24-May-22 21:04 direpack/sprm/snipls.py
+-rw-rw-rw-  2.0 fat    21875 b- defN 24-May-22 21:04 direpack/sprm/sprm.py
+-rw-rw-rw-  2.0 fat      268 b- defN 24-May-22 21:04 direpack/sudire/__init__.py
+-rw-rw-rw-  2.0 fat    16310 b- defN 24-May-22 21:04 direpack/sudire/_sudire_utils.py
+-rw-rw-rw-  2.0 fat    30240 b- defN 24-May-22 21:04 direpack/sudire/sudire.py
+-rw-rw-rw-  2.0 fat      128 b- defN 24-May-24 22:13 direpack/test/__init__.py
+-rw-rw-rw-  2.0 fat     3739 b- defN 24-May-24 22:12 direpack/test/test_dicomo.py
+-rw-rw-rw-  2.0 fat     3148 b- defN 24-May-22 21:04 direpack/test/test_ppdire.py
+-rw-rw-rw-  2.0 fat     3091 b- defN 24-May-22 21:04 direpack/test/test_sprm.py
+-rw-rw-rw-  2.0 fat     5122 b- defN 24-May-22 21:04 direpack/test/test_sudire.py
+-rw-rw-rw-  2.0 fat      259 b- defN 24-May-22 21:04 direpack/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2428 b- defN 24-May-22 21:04 direpack/utils/utils.py
+-rw-rw-rw-  2.0 fat     1091 b- defN 24-May-24 22:19 direpack-1.1.3.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    10327 b- defN 24-May-24 22:19 direpack-1.1.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-24 22:19 direpack-1.1.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        9 b- defN 24-May-24 22:19 direpack-1.1.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     3575 b- defN 24-May-24 22:19 direpack-1.1.3.dist-info/RECORD
+42 files, 280235 bytes uncompressed, 74872 bytes compressed:  73.3%
```

## zipnote {}

```diff
@@ -105,23 +105,23 @@
 
 Filename: direpack/utils/__init__.py
 Comment: 
 
 Filename: direpack/utils/utils.py
 Comment: 
 
-Filename: direpack-1.1.2.dist-info/LICENSE
+Filename: direpack-1.1.3.dist-info/LICENSE
 Comment: 
 
-Filename: direpack-1.1.2.dist-info/METADATA
+Filename: direpack-1.1.3.dist-info/METADATA
 Comment: 
 
-Filename: direpack-1.1.2.dist-info/WHEEL
+Filename: direpack-1.1.3.dist-info/WHEEL
 Comment: 
 
-Filename: direpack-1.1.2.dist-info/top_level.txt
+Filename: direpack-1.1.3.dist-info/top_level.txt
 Comment: 
 
-Filename: direpack-1.1.2.dist-info/RECORD
+Filename: direpack-1.1.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## direpack/__init__.py

```diff
@@ -1,40 +1,40 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Jul 22 12:17:17 2018
-
-@author: Sven Serneels, Ponalytics
-"""
-
-__name__ = "direpack"
-__author__ = "Emmanuel Jordy Menvouta, Sven Serneels, Tim Verdonck"
-__license__ = "MIT"
-__version__ = "1.1.2"
-__date__ = "2024-02-23"
-
-# The commented lines can be uncommented if IPOPT has been installed independently.
-
-from .preprocessing.robcent import (
-    VersatileScaler,
-    versatile_scale,
-    Wrapper,
-    wrap,
-)
-from .preprocessing.gsspp import (
-    GenSpatialSignPreProcessor,
-    gen_ss_pp,
-    gen_ss_covmat,
-)
-from .sprm.sprm import sprm
-from .sprm.snipls import snipls
-from .sprm.rm import rm
-from .cross_validation._cv_support_functions import robust_loss
-from .ppdire.ppdire import ppdire
-from .ppdire.capi import capi
-from .dicomo.dicomo import dicomo
-from .sudire.sudire import sudire, estimate_structural_dim
-from .plot.sudire_plot import sudire_plot
-from .plot.ppdire_plot import ppdire_plot
-from .plot.sprm_plot import sprm_plot, sprm_plot_cv
-from .ipopt_temp.ipopt_wrapper import minimize_ipopt
-from .ipopt_temp.jacobian import *
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Jul 22 12:17:17 2018
+
+@author: Sven Serneels, Ponalytics
+"""
+
+__name__ = "direpack"
+__author__ = "Emmanuel Jordy Menvouta, Sven Serneels, Tim Verdonck"
+__license__ = "MIT"
+__version__ = "1.1.3"
+__date__ = "2024-05-23"
+
+# The commented lines can be uncommented if IPOPT has been installed independently.
+
+from .preprocessing.robcent import (
+    VersatileScaler,
+    versatile_scale,
+    Wrapper,
+    wrap,
+)
+from .preprocessing.gsspp import (
+    GenSpatialSignPreProcessor,
+    gen_ss_pp,
+    gen_ss_covmat,
+)
+from .sprm.sprm import sprm
+from .sprm.snipls import snipls
+from .sprm.rm import rm
+from .cross_validation._cv_support_functions import robust_loss
+from .ppdire.ppdire import ppdire
+from .ppdire.capi import capi
+from .dicomo.dicomo import dicomo
+from .sudire.sudire import sudire, estimate_structural_dim
+from .plot.sudire_plot import sudire_plot
+from .plot.ppdire_plot import ppdire_plot
+from .plot.sprm_plot import sprm_plot, sprm_plot_cv
+from .ipopt_temp.ipopt_wrapper import minimize_ipopt
+from .ipopt_temp.jacobian import *
```

## direpack/cross_validation/__init__.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Jul 22 12:17:17 2018
-
-@author: Sven Serneels, Ponalytics
-"""
-
-__name__ = "cross_validation"
-__author__ = "Sven Serneels"
-__license__ = "MIT"
-__version__ = "0.7.0"
-__date__ = "2020-04-03"
-
-
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Jul 22 12:17:17 2018
+
+@author: Sven Serneels, Ponalytics
+"""
+
+__name__ = "cross_validation"
+__author__ = "Sven Serneels"
+__license__ = "MIT"
+__version__ = "0.7.0"
+__date__ = "2020-04-03"
+
+
```

## direpack/cross_validation/_cv_support_functions.py

 * *Ordering differences only*

```diff
@@ -1,58 +1,58 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Jan 27 11:42:23 2019
-
-Ancillary tools for plotting sprm results. 
-    
-    # Deleted: ABLine2D class, was broken in Py 3.7
-    cv_score_table (function): tranform sklearn GridSearchCV 
-        results into Data Frame
-
-@author: Sven Serneels
-"""
-import numpy as np
-import pandas as ps
-from sklearn.metrics import mean_squared_error
-from scipy.stats import norm
-from ..sprm._m_support_functions import Fair, Huber, Hampel
-        
-def cv_score_table(res_sprm_cv):
-        
-    """
-    Internal function reorganizing sklearn GridSearchCV results to pandas table. 
-    The function adds the cv score table to the object as cv_score_table_
-    """
-        
-    n_settings = len(res_sprm_cv.cv_results_['params'])
-    etas = [res_sprm_cv.cv_results_['params'][i]['eta'] for i in range(0,n_settings)]
-    components = [res_sprm_cv.cv_results_['params'][i]['n_components'] for i in range(0,n_settings)]
-    cv_score_table_ = ps.DataFrame({'etas':etas, 'n_components':components, 'score':res_sprm_cv.cv_results_['mean_test_score']})
-    return(cv_score_table_)
-    
-def robust_loss(y,ypred,lfun=mean_squared_error,fun=Hampel,probct=norm.ppf(0.975),hampelb=norm.ppf(.99),hampelr=norm.ppf(.999)):
-    
-    """
-    Weighted loss function to be used in sklearn cross-validation
-    Inputs: 
-        y: array or matrix, original predictand
-        ypred, array or matrix, predicted values
-        lfun, function: an sklearn loss metric that accepts caseweights, 
-            e.g. sklearn.metrics.mean_squared_error
-        fun: function, weight function, 
-            e.g. Fair, Huber or Hampel from sprm.sprm._m_support_functions
-        probct, hampelb, hampelr: float, cutoffs for weight functions
-    Output:
-        loss, float
-    """            
-    
-    if len(ypred.shape) > 1:
-        ypred = np.array(ypred).reshape(-1)
-    ypred = ypred.astype('float64')
-    if len(y.shape) > 1:
-        y = np.array(y).reshape(-1)
-    y = y.astype('float64')
-    r = y - ypred
-    w = fun(r,probct,hampelb,hampelr)
-    return(lfun(y,ypred,sample_weight=w))
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Jan 27 11:42:23 2019
+
+Ancillary tools for plotting sprm results. 
+    
+    # Deleted: ABLine2D class, was broken in Py 3.7
+    cv_score_table (function): tranform sklearn GridSearchCV 
+        results into Data Frame
+
+@author: Sven Serneels
+"""
+import numpy as np
+import pandas as ps
+from sklearn.metrics import mean_squared_error
+from scipy.stats import norm
+from ..sprm._m_support_functions import Fair, Huber, Hampel
+        
+def cv_score_table(res_sprm_cv):
+        
+    """
+    Internal function reorganizing sklearn GridSearchCV results to pandas table. 
+    The function adds the cv score table to the object as cv_score_table_
+    """
+        
+    n_settings = len(res_sprm_cv.cv_results_['params'])
+    etas = [res_sprm_cv.cv_results_['params'][i]['eta'] for i in range(0,n_settings)]
+    components = [res_sprm_cv.cv_results_['params'][i]['n_components'] for i in range(0,n_settings)]
+    cv_score_table_ = ps.DataFrame({'etas':etas, 'n_components':components, 'score':res_sprm_cv.cv_results_['mean_test_score']})
+    return(cv_score_table_)
+    
+def robust_loss(y,ypred,lfun=mean_squared_error,fun=Hampel,probct=norm.ppf(0.975),hampelb=norm.ppf(.99),hampelr=norm.ppf(.999)):
+    
+    """
+    Weighted loss function to be used in sklearn cross-validation
+    Inputs: 
+        y: array or matrix, original predictand
+        ypred, array or matrix, predicted values
+        lfun, function: an sklearn loss metric that accepts caseweights, 
+            e.g. sklearn.metrics.mean_squared_error
+        fun: function, weight function, 
+            e.g. Fair, Huber or Hampel from sprm.sprm._m_support_functions
+        probct, hampelb, hampelr: float, cutoffs for weight functions
+    Output:
+        loss, float
+    """            
+    
+    if len(ypred.shape) > 1:
+        ypred = np.array(ypred).reshape(-1)
+    ypred = ypred.astype('float64')
+    if len(y.shape) > 1:
+        y = np.array(y).reshape(-1)
+    y = y.astype('float64')
+    r = y - ypred
+    w = fun(r,probct,hampelb,hampelr)
+    return(lfun(y,ypred,sample_weight=w))
```

## direpack/dicomo/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Wed Jul 9 14:20:17 2019
-
-@author: Sven Serneels, Ponalytics
-"""
-
-__name__ = "dicomo"
-__author__ = "Sven Serneels"
-__license__ = "MIT"
-__version__ = "1.0.4"
-__date__ = "2022-10-08"
-
-
-
-
-
-
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Wed Jul 9 14:20:17 2019
+
+@author: Sven Serneels, Ponalytics
+"""
+
+__name__ = "dicomo"
+__author__ = "Sven Serneels"
+__license__ = "MIT"
+__version__ = "1.0.4"
+__date__ = "2022-10-08"
+
+
+
+
+
+
```

## direpack/dicomo/_dicomo_utils.py

 * *Ordering differences only*

```diff
@@ -1,464 +1,464 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Dec 16 13:44:19 2018
-
-@author: sven
-"""
-
-import scipy.stats as sps
-import scipy.spatial as spp
-import numpy as np
-import copy
-from ..utils.utils import MyException
-
-def trim_mean(x,trimming,axis=0):
-    """
-    computes the trimmed mean of array x according to axis.
-    Input :
-        x : input data as numpy array
-        trimming, float : trimming percentage to be used
-        axis, int or None : Axis along which the trimmed means are computed
-        
-    Output:
-        The trimmed mean of x according to axis.
-    
-    """
-    
-    if trimming == 0: 
-        return(np.mean(x,axis=axis))
-    else:
-        return(sps.trim_mean(x,trimming,axis=axis))
-
-def trimvar(x,trimming):
-    
-    """
-    computes the trimmed variance of array x .
-    Input :
-        x : input data as numpy array
-        trimming, float : trimming percentage to be used
-     
-    Output:
-        The trimmed variance of x.
-    
-    """
-        # division by n
-    return(sps.trim_mean(np.square(x - sps.trim_mean(x,trimming)),trimming))
-        
-def identity(x): 
-    return(x)
-        
-def trim_mom(x,y,locest,order,trimming,option,fscorr=True):
-    """
-    computes trimmed comoment   between x and y. order represents the order of 
-    the comoment.
-    input :
-        x : Input data as matrix 
-        y : Input data as matrix or 1d vector
-        order, int : order of the comoment
-        trimming, float : trimming percentage to be used.
-        option, int : option to select the type of co-moment (order 3: option 1 = com(x,x,y))
-        fscor, bool: if True, a finite sample correction is applied to the comoment. 
-    
-    output : 
-        the trimmed comoment between x and y 
-    """
-        # division by n
-        
-    if order == 0:
-        como = 0
-    elif order == 1: 
-        como = locest(x,trimming)
-    else:
-        if order > 2: 
-            iter_stop_2 = option 
-            iter_stop_1 = order - option
-        else: 
-            iter_stop_1 = 1
-            iter_stop_2 = 1
-        
-        if locest == np.median:
-            trimming = 0
-            factor = 1
-            if (x==y).all():
-                wrapper = abs
-                power = 1/order
-                if power == 0.5:
-                    factor = 1.4826
-            else:
-                wrapper = identity
-                power = 1
-        else:
-            n = len(x)
-            wrapper = identity
-            power = 1
-            if fscorr:
-                ntrim = round(n * (1-trimming)) 
-                factor = ntrim
-                factor /= np.product(ntrim - np.arange(max(1,order-2),order))
-            else:
-                factor = 1
-    
-        xc = wrapper(x - locest(x,trimming))
-        yc = wrapper(y - locest(y,trimming))
-    
-        factor1 = np.power(xc,iter_stop_1)
-        factor2 = np.power(yc,iter_stop_2)
-    
-        como = locest(np.power(np.multiply(factor1,factor2),power),trimming)*factor
-#        como = sps.trim_mean(np.multiply(x - sps.trim_mean(x,trimming),y - sps.trim_mean(y,trimming)),trimming)*ntrim/(ntrim-1)
-    if len(como.shape)>1: 
-        como = como[0,0]
-    else:
-        if type(como) is np.ndarray:
-            como = como[0]
-            
-        
-    return(como)
-
-def double_center_flex(a, center='mean', **kwargs):
-    """
-    Double centered function adapted to accommodate for location types different
-    from mean.
-    Input : 
-        a : input data as matrix
-        center, str : which location estimate to use for centering. either 'mean or 'median'
-        kwargs :
-            trimming, float : trimming percentage to be used.
-            biascorr, bool : if True, bias correction is applied during double centering.
-    Output : 
-        The double centered version of the matrix a.
-
-    """
-    
-    # print(kwargs)
-    
-    if 'trimming' not in kwargs:
-        trimming = 0
-    else:
-        trimming = kwargs.get('trimming')
-        # print('trimming is: ' + str(trimming))
-        
-    if 'biascorr' not in kwargs:
-        biascorr = False
-    else:
-        biascorr = kwargs.get('biascorr')
-    
-    out = copy.deepcopy(a)
-    
-    dim = np.size(a, 0)
-    n1 = dim
-
-    # mu = np.sum(a) / (dim * dim)
-    if center=='mean':
-        mu = trim_mean(a.reshape((dim**2,1)),trimming)
-        if biascorr:
-            n1 = np.round(dim*(1-trimming))
-            # print(n1)
-            mu *= (n1**2) / ((n1-1) * (n1-2))
-        mu_cols = trim_mean(a, trimming, axis=0).reshape((1,dim))
-        mu_rows = trim_mean(a, trimming, axis=1).reshape((dim,1))
-        if biascorr:
-            mu_cols *= n1/(n1 - 2)
-            mu_rows *= n1/(n1 - 2)
-        mu_cols = np.ones((dim, 1)).dot(mu_cols)
-        mu_rows = mu_rows.dot(np.ones((1, dim)))
-    elif center=='median':
-        mu = np.median(a.reshape((dim**2,1)))
-        mu_cols = np.median(a,axis=0).reshape((1,dim))
-        mu_rows = np.median(a,axis=1).reshape((dim,1))
-        mu_cols = np.ones((dim, 1)).dot(mu_cols)
-        mu_rows = mu_rows.dot(np.ones((1, dim)))
-    else:
-        raise(ValueError('Center should be mean or median'))
-        
-
-    # Do one operation at a time, to improve broadcasting memory usage.
-    out -= mu_rows
-    out -= mu_cols
-    out += mu
-    
-    if biascorr:
-        out[np.eye(dim, dtype=bool)] = 0
-
-    return out,n1
-
-
-def distance_matrix_centered(x,**kwargs):
-    """
-    Computes the trimmed double centered distance matrix of x.
-    Input : 
-        x : input data as matrix.
-        kwargs :
-            trimming, float : trimming percentage to be used.
-            biascorr, bool : if True, bias correction is applied during double centering.
-            center, str : which location estimate to use for centering. either 'mean or 'median'
-            dmetric, str : which distance metric to use. Default is euclidean distance.
-    Output :
-        the trimmed double centered distance matrix of x            
-    """
-    
-    if 'trimming' not in kwargs:
-        trimming = 0
-    else:
-        trimming = kwargs.get('trimming')
-            
-    if 'biascorr' not in kwargs:
-        biascorr = False
-    else:
-        biascorr = kwargs.get('biascorr')
-        
-    if 'center' not in kwargs:
-        center = 'mean'
-    else:
-        center = kwargs.get('center')
-        
-    if 'dmetric' not in kwargs:
-        dmetric = 'euclidean'
-    else:
-        dmetric = kwargs.get('dmetric')
-        
-    dx = spp.distance.squareform(spp.distance.pdist(x,metric=dmetric))
-    dmx, n1 = double_center_flex(dx,biascorr=biascorr,
-                                 trimming=trimming,center=center)
-    
-    return dmx,n1
-
-
-def distance_moment(dmx,dmy,**kwargs):
-    """
-    Computes the trimmed distance comoment between x and y based on their distance matrices.
-    Input : 
-        dmx : distance matrix of x 
-        dmy : distance matrix of y 
-        
-        kwargs :
-            trimming, float : trimming percentage to be used.
-            biascorr, bool : if True, bias correction is applied during double centering.
-            center, str : which location estimate to use for centering. either 'mean or 'median'
-            dmetric, str : which distance metric to use. Default is euclidean distance.
-            order, int : order  of the comoment to be computed, default is 2 for covariance.
-            option, int : option to be used during the computation. 
-    Output :
-        The trimmed  distance comoment  between x and y
-    
-    """
-    
-    if 'trimming' not in kwargs:
-        trimming = 0
-    else:
-        trimming = kwargs.get('trimming')
-            
-    if 'biascorr' not in kwargs:
-        biascorr = False
-    else:
-        biascorr = kwargs.get('biascorr')
-        
-    if 'center' not in kwargs:
-        center = 'mean'
-    else:
-        center = kwargs.get('center')
-        
-    if 'order' not in kwargs:
-        order = 2
-    else:
-        order = kwargs.get('order')
-        
-    if order > 2: 
-        if 'option' not in kwargs:
-            option = 1
-        else:
-            option = kwargs.get('option')
-            
-        iter_stop_2 = option 
-        iter_stop_1 = order - option
-    else:
-        option = 0
-        iter_stop_1 = 1
-        iter_stop_2 = 1
-        
-    nx = dmx.shape[0]
-    ny = dmy.shape[0]
-    if nx!=ny:
-        raise(ValueError)
-        
-        
-    if biascorr: 
-        
-        if trimming == 0:
-            n1 = nx
-        elif 'n1' not in kwargs:
-            raise(MyException('n1 needs to be provided when correcting for bias'))
-        else:
-            n1 = kwargs.get('n1')
-        
-        corr4bias = n1**2/(n1*(n1-3))
-        
-    else:
-        corr4bias = 1
-        
-    if order>2:
-        i = 1
-        while i < iter_stop_1:  
-            dmx *= dmx
-            i += 1
-        i = 1
-        while i < iter_stop_2:  
-            dmy *= dmy
-            i += 1
-        
-        
-    if center=='mean':
-        moment = trim_mean((dmx*dmy).reshape((nx**2,1)),trimming)
-        moment *= corr4bias
-        moment = moment[0]
-        moment = (-1)**order*abs(moment)**(1/order)
-    elif center=='median':
-        moment = np.median(dmx*dmy)
-        
-    return(moment)
-    
-    
-def difference_divergence(X,Y,**kwargs):
-    
-    
-    """
-    This function computes the (U)Martingale Difference Divergence of Y given X.
-    
-    input :
-    
-        X : A  matrix or data frame, where rows represent samples, and columns represent variables.
-        Y : The response variable or matrix.
-        biascorr, bool : if True, uses U centering to produce an unbiased estimator of MDD
-        
-    output:
-        returns the squared martingale difference divergence of Y given X.
-       
-    """
-    
-    if 'trimming' not in kwargs:
-        trimming = 0
-    else:
-        trimming = kwargs.get('trimming')
-        
-    if 'biascorr' not in kwargs:
-        biascorr = False
-    else:
-        biascorr = kwargs.get('biascorr')
-    if 'center' not in kwargs:
-        center = 'mean'
-    else:
-        center = kwargs.get('center')
-        
-    if 'dmetric' not in kwargs:
-        dmetric = 'euclidean'
-    else:
-        dmetric = kwargs.get('dmetric')
-    
-   
-    
-    
-    
-    A, Adim = distance_matrix_centered(X,biascorr=biascorr,trimming=trimming,center=center)  
-    dy=  spp.distance.squareform(spp.distance.pdist(Y.reshape(-1, 1),metric=dmetric)**2)
-    B,Bdim = double_center_flex(0.5*dy,biascorr=biascorr,trimming=trimming,center=center)
-    if biascorr:
-        return(U_inner(A,B,trimming))
-    else:
-        return(D_inner(A,B,trimming))
-       
-        
-def U_inner(X,Y,trimming=0):
-    
-    """
-        Computes the inner product in the space of U centered matrices, between matrices X and Y.
-        The matrices have to be  square matrices.
-    
-    """
-        
-    nx = X.shape[0]
-    ny = Y.shape[0]
-    
-    if nx != ny:
-        raise(MyException('Please feed x and y data of equal length'))
-        
-    #((1/(nx*(nx-3))) *(np.sum(arr)))
-    arr= np.multiply(X,Y)
-    arr=arr.flatten()
-    lowercut = int(trimming * (nx**2))
-    uppercut = (nx**2) - lowercut
-    atmp = np.partition(arr, (lowercut, uppercut - 1), axis=0)
-    sl = [slice(None)] * atmp.ndim
-    sl[0] = slice(lowercut, uppercut)
-    res= atmp[tuple(sl)]
-    n = np.sqrt(len(res))
-    return((1/(n*(n-3)))*np.sum(atmp[tuple(sl)], axis=0))
-
-
-def D_inner(X,Y,trimming=0):
-    
-    """
-    Computes the inner product in the space of D centered matrices, between Double centered matrices X and Y.
-    The matrices have to be square matrices.
-    
-    """
-        
-    nx = X.shape[0]
-    ny = Y.shape[0]
-    
-    if nx != ny:
-        raise(MyException('Please feed x and y data of equal length'))
-        
-    #arr= (1/(nx*nx))*np.multiply(X,Y)
-    arr= np.multiply(X,Y)
-    arr=arr.flatten()
-    lowercut = int(trimming * (nx**2))
-    uppercut = (nx**2) - lowercut
-    atmp = np.partition(arr, (lowercut, uppercut - 1), axis=0)
-    sl = [slice(None)] * atmp.ndim
-    sl[0] = slice(lowercut, uppercut)
-    res= atmp[tuple(sl)]
-    n = np.sqrt(len(res))
-    return((1/(n*n))*np.sum(res, axis=0))
-
-def MDDM(X,Y): 
-    
-    """Computes the MDDM(Y|X)
-    for more details, see the article by Chung Eun Lee & Xiaofeng Shao;
-    Martingale Difference Divergence Matrix and Its
-    Application to Dimension Reduction for Stationary
-    Multivariate Time Series;                                                 
-    Journal of the American Statistical Association; 2018;521;
-    216--229
-    
-    
-    Input: 
-    X  ---  ndarray of shape (n,p)
-    Y  --- ndarray of shape(n,q)
-    
-    Output: 
-    MDDM(Y|X)
-    """
-
-    n,q = Y.shape
-    n,p = X.shape
-    MDDM = np.zeros((q,q))
-    Y_mean = np.mean(Y,axis=0).reshape(1,-1)
-    Y_center = Y - np.matmul(np.ones((n,1)),Y_mean)
-    
-    for i in range(n):
-        if(p==1):
-            X_dist = np.abs(X[i]-X)
-            
-        else: 
-            X_diff= (( X.T - np.vstack(X[i,:])).T)**2
-            X_sum = np.sum(X_diff,axis=1)
-            X_dist = np.sqrt(X_sum).reshape(-1,n)
-            
-        
-        MDDM = MDDM + np.matmul(Y_center[i,:].reshape(q,-1), np.matmul(X_dist,Y_center))
-        
-    
-    MDDM = (-MDDM)/(n**2)
-    
-    return(MDDM)
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Dec 16 13:44:19 2018
+
+@author: sven
+"""
+
+import scipy.stats as sps
+import scipy.spatial as spp
+import numpy as np
+import copy
+from ..utils.utils import MyException
+
+def trim_mean(x,trimming,axis=0):
+    """
+    computes the trimmed mean of array x according to axis.
+    Input :
+        x : input data as numpy array
+        trimming, float : trimming percentage to be used
+        axis, int or None : Axis along which the trimmed means are computed
+        
+    Output:
+        The trimmed mean of x according to axis.
+    
+    """
+    
+    if trimming == 0: 
+        return(np.mean(x,axis=axis))
+    else:
+        return(sps.trim_mean(x,trimming,axis=axis))
+
+def trimvar(x,trimming):
+    
+    """
+    computes the trimmed variance of array x .
+    Input :
+        x : input data as numpy array
+        trimming, float : trimming percentage to be used
+     
+    Output:
+        The trimmed variance of x.
+    
+    """
+        # division by n
+    return(sps.trim_mean(np.square(x - sps.trim_mean(x,trimming)),trimming))
+        
+def identity(x): 
+    return(x)
+        
+def trim_mom(x,y,locest,order,trimming,option,fscorr=True):
+    """
+    computes trimmed comoment   between x and y. order represents the order of 
+    the comoment.
+    input :
+        x : Input data as matrix 
+        y : Input data as matrix or 1d vector
+        order, int : order of the comoment
+        trimming, float : trimming percentage to be used.
+        option, int : option to select the type of co-moment (order 3: option 1 = com(x,x,y))
+        fscor, bool: if True, a finite sample correction is applied to the comoment. 
+    
+    output : 
+        the trimmed comoment between x and y 
+    """
+        # division by n
+        
+    if order == 0:
+        como = 0
+    elif order == 1: 
+        como = locest(x,trimming)
+    else:
+        if order > 2: 
+            iter_stop_2 = option 
+            iter_stop_1 = order - option
+        else: 
+            iter_stop_1 = 1
+            iter_stop_2 = 1
+        
+        if locest == np.median:
+            trimming = 0
+            factor = 1
+            if (x==y).all():
+                wrapper = abs
+                power = 1/order
+                if power == 0.5:
+                    factor = 1.4826
+            else:
+                wrapper = identity
+                power = 1
+        else:
+            n = len(x)
+            wrapper = identity
+            power = 1
+            if fscorr:
+                ntrim = round(n * (1-trimming)) 
+                factor = ntrim
+                factor /= np.product(ntrim - np.arange(max(1,order-2),order))
+            else:
+                factor = 1
+    
+        xc = wrapper(x - locest(x,trimming))
+        yc = wrapper(y - locest(y,trimming))
+    
+        factor1 = np.power(xc,iter_stop_1)
+        factor2 = np.power(yc,iter_stop_2)
+    
+        como = locest(np.power(np.multiply(factor1,factor2),power),trimming)*factor
+#        como = sps.trim_mean(np.multiply(x - sps.trim_mean(x,trimming),y - sps.trim_mean(y,trimming)),trimming)*ntrim/(ntrim-1)
+    if len(como.shape)>1: 
+        como = como[0,0]
+    else:
+        if type(como) is np.ndarray:
+            como = como[0]
+            
+        
+    return(como)
+
+def double_center_flex(a, center='mean', **kwargs):
+    """
+    Double centered function adapted to accommodate for location types different
+    from mean.
+    Input : 
+        a : input data as matrix
+        center, str : which location estimate to use for centering. either 'mean or 'median'
+        kwargs :
+            trimming, float : trimming percentage to be used.
+            biascorr, bool : if True, bias correction is applied during double centering.
+    Output : 
+        The double centered version of the matrix a.
+
+    """
+    
+    # print(kwargs)
+    
+    if 'trimming' not in kwargs:
+        trimming = 0
+    else:
+        trimming = kwargs.get('trimming')
+        # print('trimming is: ' + str(trimming))
+        
+    if 'biascorr' not in kwargs:
+        biascorr = False
+    else:
+        biascorr = kwargs.get('biascorr')
+    
+    out = copy.deepcopy(a)
+    
+    dim = np.size(a, 0)
+    n1 = dim
+
+    # mu = np.sum(a) / (dim * dim)
+    if center=='mean':
+        mu = trim_mean(a.reshape((dim**2,1)),trimming)
+        if biascorr:
+            n1 = np.round(dim*(1-trimming))
+            # print(n1)
+            mu *= (n1**2) / ((n1-1) * (n1-2))
+        mu_cols = trim_mean(a, trimming, axis=0).reshape((1,dim))
+        mu_rows = trim_mean(a, trimming, axis=1).reshape((dim,1))
+        if biascorr:
+            mu_cols *= n1/(n1 - 2)
+            mu_rows *= n1/(n1 - 2)
+        mu_cols = np.ones((dim, 1)).dot(mu_cols)
+        mu_rows = mu_rows.dot(np.ones((1, dim)))
+    elif center=='median':
+        mu = np.median(a.reshape((dim**2,1)))
+        mu_cols = np.median(a,axis=0).reshape((1,dim))
+        mu_rows = np.median(a,axis=1).reshape((dim,1))
+        mu_cols = np.ones((dim, 1)).dot(mu_cols)
+        mu_rows = mu_rows.dot(np.ones((1, dim)))
+    else:
+        raise(ValueError('Center should be mean or median'))
+        
+
+    # Do one operation at a time, to improve broadcasting memory usage.
+    out -= mu_rows
+    out -= mu_cols
+    out += mu
+    
+    if biascorr:
+        out[np.eye(dim, dtype=bool)] = 0
+
+    return out,n1
+
+
+def distance_matrix_centered(x,**kwargs):
+    """
+    Computes the trimmed double centered distance matrix of x.
+    Input : 
+        x : input data as matrix.
+        kwargs :
+            trimming, float : trimming percentage to be used.
+            biascorr, bool : if True, bias correction is applied during double centering.
+            center, str : which location estimate to use for centering. either 'mean or 'median'
+            dmetric, str : which distance metric to use. Default is euclidean distance.
+    Output :
+        the trimmed double centered distance matrix of x            
+    """
+    
+    if 'trimming' not in kwargs:
+        trimming = 0
+    else:
+        trimming = kwargs.get('trimming')
+            
+    if 'biascorr' not in kwargs:
+        biascorr = False
+    else:
+        biascorr = kwargs.get('biascorr')
+        
+    if 'center' not in kwargs:
+        center = 'mean'
+    else:
+        center = kwargs.get('center')
+        
+    if 'dmetric' not in kwargs:
+        dmetric = 'euclidean'
+    else:
+        dmetric = kwargs.get('dmetric')
+        
+    dx = spp.distance.squareform(spp.distance.pdist(x,metric=dmetric))
+    dmx, n1 = double_center_flex(dx,biascorr=biascorr,
+                                 trimming=trimming,center=center)
+    
+    return dmx,n1
+
+
+def distance_moment(dmx,dmy,**kwargs):
+    """
+    Computes the trimmed distance comoment between x and y based on their distance matrices.
+    Input : 
+        dmx : distance matrix of x 
+        dmy : distance matrix of y 
+        
+        kwargs :
+            trimming, float : trimming percentage to be used.
+            biascorr, bool : if True, bias correction is applied during double centering.
+            center, str : which location estimate to use for centering. either 'mean or 'median'
+            dmetric, str : which distance metric to use. Default is euclidean distance.
+            order, int : order  of the comoment to be computed, default is 2 for covariance.
+            option, int : option to be used during the computation. 
+    Output :
+        The trimmed  distance comoment  between x and y
+    
+    """
+    
+    if 'trimming' not in kwargs:
+        trimming = 0
+    else:
+        trimming = kwargs.get('trimming')
+            
+    if 'biascorr' not in kwargs:
+        biascorr = False
+    else:
+        biascorr = kwargs.get('biascorr')
+        
+    if 'center' not in kwargs:
+        center = 'mean'
+    else:
+        center = kwargs.get('center')
+        
+    if 'order' not in kwargs:
+        order = 2
+    else:
+        order = kwargs.get('order')
+        
+    if order > 2: 
+        if 'option' not in kwargs:
+            option = 1
+        else:
+            option = kwargs.get('option')
+            
+        iter_stop_2 = option 
+        iter_stop_1 = order - option
+    else:
+        option = 0
+        iter_stop_1 = 1
+        iter_stop_2 = 1
+        
+    nx = dmx.shape[0]
+    ny = dmy.shape[0]
+    if nx!=ny:
+        raise(ValueError)
+        
+        
+    if biascorr: 
+        
+        if trimming == 0:
+            n1 = nx
+        elif 'n1' not in kwargs:
+            raise(MyException('n1 needs to be provided when correcting for bias'))
+        else:
+            n1 = kwargs.get('n1')
+        
+        corr4bias = n1**2/(n1*(n1-3))
+        
+    else:
+        corr4bias = 1
+        
+    if order>2:
+        i = 1
+        while i < iter_stop_1:  
+            dmx *= dmx
+            i += 1
+        i = 1
+        while i < iter_stop_2:  
+            dmy *= dmy
+            i += 1
+        
+        
+    if center=='mean':
+        moment = trim_mean((dmx*dmy).reshape((nx**2,1)),trimming)
+        moment *= corr4bias
+        moment = moment[0]
+        moment = (-1)**order*abs(moment)**(1/order)
+    elif center=='median':
+        moment = np.median(dmx*dmy)
+        
+    return(moment)
+    
+    
+def difference_divergence(X,Y,**kwargs):
+    
+    
+    """
+    This function computes the (U)Martingale Difference Divergence of Y given X.
+    
+    input :
+    
+        X : A  matrix or data frame, where rows represent samples, and columns represent variables.
+        Y : The response variable or matrix.
+        biascorr, bool : if True, uses U centering to produce an unbiased estimator of MDD
+        
+    output:
+        returns the squared martingale difference divergence of Y given X.
+       
+    """
+    
+    if 'trimming' not in kwargs:
+        trimming = 0
+    else:
+        trimming = kwargs.get('trimming')
+        
+    if 'biascorr' not in kwargs:
+        biascorr = False
+    else:
+        biascorr = kwargs.get('biascorr')
+    if 'center' not in kwargs:
+        center = 'mean'
+    else:
+        center = kwargs.get('center')
+        
+    if 'dmetric' not in kwargs:
+        dmetric = 'euclidean'
+    else:
+        dmetric = kwargs.get('dmetric')
+    
+   
+    
+    
+    
+    A, Adim = distance_matrix_centered(X,biascorr=biascorr,trimming=trimming,center=center)  
+    dy=  spp.distance.squareform(spp.distance.pdist(Y.reshape(-1, 1),metric=dmetric)**2)
+    B,Bdim = double_center_flex(0.5*dy,biascorr=biascorr,trimming=trimming,center=center)
+    if biascorr:
+        return(U_inner(A,B,trimming))
+    else:
+        return(D_inner(A,B,trimming))
+       
+        
+def U_inner(X,Y,trimming=0):
+    
+    """
+        Computes the inner product in the space of U centered matrices, between matrices X and Y.
+        The matrices have to be  square matrices.
+    
+    """
+        
+    nx = X.shape[0]
+    ny = Y.shape[0]
+    
+    if nx != ny:
+        raise(MyException('Please feed x and y data of equal length'))
+        
+    #((1/(nx*(nx-3))) *(np.sum(arr)))
+    arr= np.multiply(X,Y)
+    arr=arr.flatten()
+    lowercut = int(trimming * (nx**2))
+    uppercut = (nx**2) - lowercut
+    atmp = np.partition(arr, (lowercut, uppercut - 1), axis=0)
+    sl = [slice(None)] * atmp.ndim
+    sl[0] = slice(lowercut, uppercut)
+    res= atmp[tuple(sl)]
+    n = np.sqrt(len(res))
+    return((1/(n*(n-3)))*np.sum(atmp[tuple(sl)], axis=0))
+
+
+def D_inner(X,Y,trimming=0):
+    
+    """
+    Computes the inner product in the space of D centered matrices, between Double centered matrices X and Y.
+    The matrices have to be square matrices.
+    
+    """
+        
+    nx = X.shape[0]
+    ny = Y.shape[0]
+    
+    if nx != ny:
+        raise(MyException('Please feed x and y data of equal length'))
+        
+    #arr= (1/(nx*nx))*np.multiply(X,Y)
+    arr= np.multiply(X,Y)
+    arr=arr.flatten()
+    lowercut = int(trimming * (nx**2))
+    uppercut = (nx**2) - lowercut
+    atmp = np.partition(arr, (lowercut, uppercut - 1), axis=0)
+    sl = [slice(None)] * atmp.ndim
+    sl[0] = slice(lowercut, uppercut)
+    res= atmp[tuple(sl)]
+    n = np.sqrt(len(res))
+    return((1/(n*n))*np.sum(res, axis=0))
+
+def MDDM(X,Y): 
+    
+    """Computes the MDDM(Y|X)
+    for more details, see the article by Chung Eun Lee & Xiaofeng Shao;
+    Martingale Difference Divergence Matrix and Its
+    Application to Dimension Reduction for Stationary
+    Multivariate Time Series;                                                 
+    Journal of the American Statistical Association; 2018;521;
+    216--229
+    
+    
+    Input: 
+    X  ---  ndarray of shape (n,p)
+    Y  --- ndarray of shape(n,q)
+    
+    Output: 
+    MDDM(Y|X)
+    """
+
+    n,q = Y.shape
+    n,p = X.shape
+    MDDM = np.zeros((q,q))
+    Y_mean = np.mean(Y,axis=0).reshape(1,-1)
+    Y_center = Y - np.matmul(np.ones((n,1)),Y_mean)
+    
+    for i in range(n):
+        if(p==1):
+            X_dist = np.abs(X[i]-X)
+            
+        else: 
+            X_diff= (( X.T - np.vstack(X[i,:])).T)**2
+            X_sum = np.sum(X_diff,axis=1)
+            X_dist = np.sqrt(X_sum).reshape(-1,n)
+            
+        
+        MDDM = MDDM + np.matmul(Y_center[i,:].reshape(q,-1), np.matmul(X_dist,Y_center))
+        
+    
+    MDDM = (-MDDM)/(n**2)
+    
+    return(MDDM)
```

## direpack/dicomo/dicomo.py

 * *Ordering differences only*

```diff
@@ -1,452 +1,452 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Created on Sun Dec  2 13:14:51 2018
-
-# @author: sven
-
-import numpy as np
-import scipy.stats as sps
-import scipy.spatial as spp
-from sklearn.base import BaseEstimator
-from sklearn.utils.metaestimators import _BaseComposition
-from statsmodels import robust as srs
-from ._dicomo_utils import *
-import dcor as dc
-# Optional: if Ballcov required, import Ball 
-# import Ball
-
-class MyException(Exception):
-        pass
-
-class dicomo(_BaseComposition,BaseEstimator):
-    
-    """
-    The `dicomo` class implements (co)-moment statistics, covering both clasical product-moment 
-    statistics, as well as more recently developed energy statistics. 
-    The `dicomo` class also serves as a plug-in into `capi` and  `ppdire`. 
-    It has been written consistently with `ppdire` such that it provides a wide range of 
-    projection indices based on (co-)moments. Ancillary functions for (co-)moment 
-    estimation are in `_dicomo_utils.py`.
-    
-    Parameters
-    ------------ 
-
-        est :  str
-             mode of estimation. The set of options are `'arithmetic'` (product-moment)  or `'distance'` (energy statistics)
-
-        mode : str
-                 type of moment. Options include :
-                * `'mom'`: moment 
-                * `'var'`: variance 
-                * `'std'`: standard deviation 
-                * `'skew'`: skewness 
-                * `'kurt'`: kurtosis
-                * `'com'`: co-moment 
-                * `'M3'`: shortcut for third order co-moment
-                * `'cov'`: covariance 
-                * `'cos'`: co-skewness
-                * `'cok'`: co-kurtosis 
-                * `'corr'`: correlation, 
-                * `'continuum'`: continuum association 
-                * `'mdd'`: martingale difference divergence (requires `est = 'distance'`)
-                * `'mdc'`: martingale difference correlation (requires `est = 'distance'`)
-                * `'ballcov'`: ball covariance (requires installing `Ball` and uncommenting the `import` statement)
-
-        center : str 
-                 internal centring used in calculation. Options are `mean` or `median`.  
-
-    Attributes
-    ------------
-    Attributes always provided: 
-
-        - `moment_`: The resulting (co-)moment Depending on the options picked, intermediate results are stored as well, such as `x_moment_`, `y_moment_` or `co_moment_`
-    
-    """
-    
-
-    def __init__(self,est='arithmetic',mode='mom',center='mean'):
-        self.mode = mode
-        self.est=est
-        self.center=center
-        self.liest = ['arithmetic','distance','sign','entropy']
-        self.limo = ['mom','var','std','skew','kurt','com','cov','cok','cos','corr','continuum','M3', 'mdd','ballcov']
-        self.licenter = ['mean','median']
-        if not(self.mode in self.limo):
-            raise(MyException("Only models allowed are: 'mom','var','skew','kurt','com','cov','cos','cok','corr','continuum','M3','mdd','mdc','ballcov'"))
-        if not(self.est in self.liest):
-            raise(MyException('Only estimator classes allowed are: "arithmetic", "distance", "sign", "entropy"'))
-        if not(self.center in self.licenter):
-            raise(MyException('Only centring classes allowed are: "mean", "median"'))
-        if (self.est=='arithmetic' and self.mode in ['mdd','mdc']):
-            raise(MyException('MDD only defined when est="distance"'))
-        if (self.mode=='ballcov'): 
-            print("To use the Ballcov functionality, uncomment the import statement and comment this line")
-        
-        
-        
-    def fit(self,x,**kwargs):
-        """
-        Fit a dicomo model
-
-        Parameters
-        ------------
-            X : numpy array or pandas DataFrame
-                input data
-
-
-        Remarks:
-        The `fit` function takes several optional input arguments. These are options that 
-        apply to individual settings: 
-            `biascorr`, Bool, when `True`, correct for bias. For classical product-moment statistics, this 
-                is the small sample correction. For energy statistics, this leads to the estimates 
-                that are unbiased in high dimension
-                (but not preferred in low dimension). 
-            `alpha`, float, parameter for continuum association. Has no effect for other options.  
-            `option`, int, determines which higher order co-moment to calculate, 
-                e.g. for co-skewness, `option=1` calculates CoS(x,x,y)
-            `order`, int, which order (co-)moment to calculate. Can be overruled by `mode`, 
-                e.g. if `mode='var'`, `order` is set to 2. 
-            `calcmode`, str, to use the efficient or naive algorithm to calculate distance statistics. Defaults to `fast` when available.
-        """
-        
-        
-        if 'trimming' not in kwargs:
-            trimming = 0
-        else:
-            trimming = kwargs.get('trimming')
-            
-        if 'biascorr' not in kwargs:
-            biascorr = False
-        else:
-            biascorr = kwargs.get('biascorr')
-            
-        if 'alpha' not in kwargs:
-            alpha = 1
-        else:
-            alpha = kwargs.get('alpha')
-            
-        if 'dmetric' not in kwargs:
-            dmetric = 'euclidean'
-        else:
-            dmetric = kwargs.get('dmetric')
-            
-        if 'calcmode' not in kwargs:
-            calcmode = 'fast'
-        else:
-            calcmode = kwargs.get('calcmode')
-            
-        if 'order' not in kwargs:
-            order = 2
-        else:
-            order = kwargs.get('order')
-            
-        if self.mode == 'var':
-            mode = 'mom' 
-            order=2
-        elif self.mode == 'cov':
-            mode = 'com'
-            order=2
-        elif self.mode == 'std':
-            self.center = 'mean'
-            mode = 'mom'
-            order=2
-            num_power = 0.5
-        elif self.mode == 'skew':
-            self.center = 'mean'
-            mode = 'mom'
-            order = 3
-            num_power = 1.5
-        elif self.mode == 'kurt':
-            self.center = 'mean'
-            mode = 'mom'
-            order = 4
-            num_power = 2
-            if 'Fisher' not in kwargs:
-                Fisher = True
-            else:
-                Fisher = kwargs.get('Fisher')
-        elif self.mode == 'cos':
-            self.center = 'mean'
-            mode = 'com'
-            order = 3
-            if 'standardized' not in kwargs:
-                standardized = True
-            else:
-                standardized = kwargs.get('standardized')
-        elif self.mode == 'M3':
-            self.center = 'mean'
-            mode = 'com'
-            order = 3
-            if 'standardized' not in kwargs:
-                standardized = False
-            else:
-                standardized = kwargs.get('standardized')
-        elif self.mode == 'cok':
-            self.center = 'mean'
-            mode = 'com'
-            order = 4
-            if 'standardized' not in kwargs:
-                standardized = True
-            else:
-                standardized = kwargs.get('standardized')
-            if 'Fisher' not in kwargs:
-                Fisher = True
-            else:
-                Fisher = kwargs.get('Fisher')
-        else:
-            mode = self.mode
-            
-        if order > 2: 
-            if 'option' not in kwargs:
-                option = 1
-            else:
-                option = kwargs.get('option')
-        else:
-            option = 0
-            
-        n = len(x)
-        ntrim = round(n * (1-trimming)) 
-        
-        if len(x.shape)==1:
-            x = np.array(x).reshape((n,1))
-        
-        if mode=='corr':
-            alpha = 1
-        
-        if n==0:
-            raise(MyException('Please feed data with length > 0'))
-            
-        if self.center == 'median':
-            locest = np.median
-        else:
-            locest = trim_mean
-        
-        # Classical variance, covariance and continuum as well as robust alternatives
-        if self.est=='arithmetic':
-            
-            # Variance
-            if mode!='com':
-#                if self.center=='mean':
-#                    xvar = trimvar(x,trimming)*ntrim/(ntrim-1)
-#                elif self.center=='median':
-#                    xvar = srs.mad(x)**2
-                xmom = trim_mom(x,x,locest,order,trimming,option,biascorr) 
-                self.x_moment_ = xmom
-                
-                if self.mode in ('std','skew','kurt'):
-                    x2mom = trim_mom(x,x,locest,2,trimming,option,False)
-                    xmom /= (x2mom**num_power) 
-                    if biascorr:
-                        if self.mode == 'skew':
-                            xmom *= (ntrim-1)**2
-                            xmom /= np.sqrt(ntrim**2 - ntrim)
-                        elif self.mode == 'kurt':
-                            xmom = xmom*ntrim - xmom/ntrim
-                            xmom -= 3*(ntrim-1)**2.0 / ((ntrim-2)*(ntrim-3))
-                            if not Fisher:
-                                xmom += 3
-                
-                if mode=='mom':
-                    self.moment_ = xmom
-            
-            # Covariance or continuum
-            if mode!='mom':
-                
-                if 'y' not in kwargs:
-                    raise(MyException('Please supply second data vector'))
-                else:
-                    y = kwargs.get('y')
-                
-                n1 = len(y)
-                if n1==0:
-                    raise(MyException('Please feed data with length > 0'))
-                if n1!=n:
-                    raise(MyException('Please feed x and y data of equal length'))
-                if len(y.shape)==1:
-                    y = np.array(y).reshape((n,1))
-                
-                como = trim_mom(x,y,locest,order,trimming,option,biascorr)
-                
-                self.co_moment_ = como
-                
-                if (biascorr and (order > 2)):
-                    como *= ntrim
-                
-                if mode=='com':
-                    self.moment_ = como
-                
-                    
-                if self.mode in ('cok','cos'):
-                    x2sd = np.sqrt(trim_mom(x,x,locest,2,trimming,option,biascorr))
-                    y2sd = np.sqrt(trim_mom(y,y,locest,2,trimming,option,biascorr))
-                        
-                    if ((self.mode == 'cok') and biascorr): # biascorr is only exact if standardized
-                        como -= como/(ntrim**2)
-                        como -= 3*(ntrim-1)**2.0 / ((ntrim-2)*(ntrim-3))
-
-                
-                if mode in ['corr','continuum']:
-                    
-#                    if self.center=='mean':
-#                        yvar = trimvar(y,trimming)*ntrim/(ntrim-1)
-#                    
-#                    elif self.center=='median':
-#                        yvar = srs.mad(y)
-                    ymom = trim_mom(y,y,locest,order,trimming,option,biascorr)
-                    
-                    self.y_moment_ = ymom
-                    
-        
-        # Distance based metrics
-        elif self.est=='distance':
-            
-            if 'dmetric' not in kwargs:
-                dmetric = 'euclidean'
-            else:
-                dmetric = kwargs.get('dmetric')
-                
-            if (mode in ['ballcov','mdd','mdc']):
-        
-                if 'y' not in kwargs:
-                        raise(MyException('Please supply second data vector'))
-                else:
-                    y = kwargs.get('y')
-                    n1 = len(y)
-                    if n1==0:
-                        raise(MyException('Please feed data with length > 0'))
-                    if n1!=n:
-                        raise(MyException('Please feed x and y data of equal length')) 
-                    if (mode in ['mdd','mdc']):
-                        como=np.sqrt(difference_divergence(x,y,center=self.center,trimming=trimming,biascorr=biascorr))
-                        self.co_moment_ = como
-                        if mode=='mdd':
-                            self.moment_ = como 
-                        else:
-                            xmom=difference_divergence(x,x,center=self.center,trimming=trimming,biascorr=biascorr)
-                            ymom=difference_divergence(y,y,center=self.center,trimming=trimming,biascorr=biascorr)
-                            self.x_moment_ = xmom
-                            self.y_moment_ = ymom
-                            mode = 'corr'
-
-                    else:
-                        dmy, n2 = distance_matrix_centered(y,biascorr=biascorr,
-                                                   trimming=trimming,
-                                                   center=self.center,
-                                                   dmetric=dmetric) 
-                        bcov_res=Ball.bcov_test(x,y,num_permutations=0)[0]
-                        self.moment_ = bcov_res
-                        
-                
-                
-            
-            elif (calcmode=='fast' and self.center =='mean' and trimming == 0 and order==2):
-                if mode != 'com':
-                    if biascorr:
-                        xmom = np.sqrt(dc.u_distance_covariance_sqr(x,x))
-                    else:
-                        xmom = np.sqrt(dc.distance_covariance_sqr(x,x))
-                    self.moment_ = xmom
-                if mode !='mom':
-                    if 'y' not in kwargs:
-                        raise(MyException('Please supply second data vector'))
-                    else:
-                        y = kwargs.get('y')
-                        n1 = len(y)
-                    
-                    if biascorr:
-                        como = np.sqrt(dc.u_distance_covariance_sqr(x,y))
-                    else:
-                        como = np.sqrt(dc.distance_covariance_sqr(x,y))
-                    if mode=='com':
-                        self.co_moment_ = como
-                        self.moment_ = como
-                    elif mode in ['corr','continuum','cos','cok']:
-                        if biascorr:
-                            ymom = np.sqrt(dc.u_distance_covariance_sqr(y,y))
-                        else:
-                            ymom = np.sqrt(dc.distance_covariance_sqr(y,y))
-                        self.y_moment_ = ymom
-                        
-            
-                
-            else:
-                dmx, n1 = distance_matrix_centered(x,biascorr=biascorr,
-                                                   trimming=trimming,
-                                                   center=self.center,
-                                                   dmetric=dmetric)
-                
-                # Variance
-                if mode!='com':
-                    
-                    xmom = distance_moment(dmx,dmx,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=order,option=option)
-                        
-                    self.x_moment_ = xmom
-                    
-                    if self.mode in ('std','skew','kurt'):
-                        x2mom =distance_moment(dmx,dmx,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=2,option=option)
-                        xmom /= x2mom**num_power
-                        
-                    if mode=='mom':
-                        self.moment_ = xmom
-                        
-                if mode!='mom':
-                    
-                    if 'y' not in kwargs:
-                        raise(MyException('Please supply second data vector'))
-                    else:
-                        y = kwargs.get('y')
-                        n1 = len(y)
-                    if n1==0:
-                        raise(MyException('Please feed data with length > 0'))
-                    if n1!=n:
-                        raise(MyException('Please feed x and y data of equal length'))
-                    
-                    dmy, n2 = distance_matrix_centered(y,biascorr=biascorr,
-                                                   trimming=trimming,
-                                                   center=self.center,
-                                                   dmetric=dmetric)
-                    
-                    como = distance_moment(dmx,dmy,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=order,option=option)
-                    
-                    self.co_moment_ = como
-                    
-                    if mode=='com':
-                        self.moment_ = como
-                        
-                    if self.mode in ('cok','cos'):
-                        x2sd = distance_moment(dmx,dmx,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=2,option=option)
-                        x2sd = np.sqrt(x2sd)
-                        y2sd = distance_moment(dmy,dmy,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=2,option=option)
-                        y2sd = np.sqrt(y2sd)
-                    
-                    if mode in ['corr','continuum','cok','cos']:
-                        
-                        ymom = distance_moment(dmy,dmy,n1=n,biascorr=biascorr,center=self.center,trimming=trimming,order=order,option=option)
-                        self.y_moment_ = ymom
-                    
-        if mode == 'corr': 
-            como /= (np.sqrt(xmom)*np.sqrt(ymom))
-            self.moment_=como
-        elif mode == 'continuum':
-            como *= como * (np.sqrt(xmom)**(alpha -1))
-            self.moment_=como
-        
-        if (self.mode in ('cok','cos') and standardized):
-            iter_stop_2 = option 
-            iter_stop_1 = order - option
-            como /= np.power(x2sd,iter_stop_1)
-            como /= np.power(y2sd,iter_stop_2)
-            if ((self.mode == 'cok') and not Fisher): # Not very meaningful for co-moment
-                como += 3
-            self.moment_=como
-        
-                        
-        
-        if type(self.moment_)==np.ndarray:
-            self.moment_ = self.moment_[0]
-        return(self.moment_)
-        
-                    
-                    
-                
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+# Created on Sun Dec  2 13:14:51 2018
+
+# @author: sven
+
+import numpy as np
+import scipy.stats as sps
+import scipy.spatial as spp
+from sklearn.base import BaseEstimator
+from sklearn.utils.metaestimators import _BaseComposition
+from statsmodels import robust as srs
+from ._dicomo_utils import *
+import dcor as dc
+# Optional: if Ballcov required, import Ball 
+# import Ball
+
+class MyException(Exception):
+        pass
+
+class dicomo(_BaseComposition,BaseEstimator):
+    
+    """
+    The `dicomo` class implements (co)-moment statistics, covering both clasical product-moment 
+    statistics, as well as more recently developed energy statistics. 
+    The `dicomo` class also serves as a plug-in into `capi` and  `ppdire`. 
+    It has been written consistently with `ppdire` such that it provides a wide range of 
+    projection indices based on (co-)moments. Ancillary functions for (co-)moment 
+    estimation are in `_dicomo_utils.py`.
+    
+    Parameters
+    ------------ 
+
+        est :  str
+             mode of estimation. The set of options are `'arithmetic'` (product-moment)  or `'distance'` (energy statistics)
+
+        mode : str
+                 type of moment. Options include :
+                * `'mom'`: moment 
+                * `'var'`: variance 
+                * `'std'`: standard deviation 
+                * `'skew'`: skewness 
+                * `'kurt'`: kurtosis
+                * `'com'`: co-moment 
+                * `'M3'`: shortcut for third order co-moment
+                * `'cov'`: covariance 
+                * `'cos'`: co-skewness
+                * `'cok'`: co-kurtosis 
+                * `'corr'`: correlation, 
+                * `'continuum'`: continuum association 
+                * `'mdd'`: martingale difference divergence (requires `est = 'distance'`)
+                * `'mdc'`: martingale difference correlation (requires `est = 'distance'`)
+                * `'ballcov'`: ball covariance (requires installing `Ball` and uncommenting the `import` statement)
+
+        center : str 
+                 internal centring used in calculation. Options are `mean` or `median`.  
+
+    Attributes
+    ------------
+    Attributes always provided: 
+
+        - `moment_`: The resulting (co-)moment Depending on the options picked, intermediate results are stored as well, such as `x_moment_`, `y_moment_` or `co_moment_`
+    
+    """
+    
+
+    def __init__(self,est='arithmetic',mode='mom',center='mean'):
+        self.mode = mode
+        self.est=est
+        self.center=center
+        self.liest = ['arithmetic','distance','sign','entropy']
+        self.limo = ['mom','var','std','skew','kurt','com','cov','cok','cos','corr','continuum','M3', 'mdd','ballcov']
+        self.licenter = ['mean','median']
+        if not(self.mode in self.limo):
+            raise(MyException("Only models allowed are: 'mom','var','skew','kurt','com','cov','cos','cok','corr','continuum','M3','mdd','mdc','ballcov'"))
+        if not(self.est in self.liest):
+            raise(MyException('Only estimator classes allowed are: "arithmetic", "distance", "sign", "entropy"'))
+        if not(self.center in self.licenter):
+            raise(MyException('Only centring classes allowed are: "mean", "median"'))
+        if (self.est=='arithmetic' and self.mode in ['mdd','mdc']):
+            raise(MyException('MDD only defined when est="distance"'))
+        if (self.mode=='ballcov'): 
+            print("To use the Ballcov functionality, uncomment the import statement and comment this line")
+        
+        
+        
+    def fit(self,x,**kwargs):
+        """
+        Fit a dicomo model
+
+        Parameters
+        ------------
+            X : numpy array or pandas DataFrame
+                input data
+
+
+        Remarks:
+        The `fit` function takes several optional input arguments. These are options that 
+        apply to individual settings: 
+            `biascorr`, Bool, when `True`, correct for bias. For classical product-moment statistics, this 
+                is the small sample correction. For energy statistics, this leads to the estimates 
+                that are unbiased in high dimension
+                (but not preferred in low dimension). 
+            `alpha`, float, parameter for continuum association. Has no effect for other options.  
+            `option`, int, determines which higher order co-moment to calculate, 
+                e.g. for co-skewness, `option=1` calculates CoS(x,x,y)
+            `order`, int, which order (co-)moment to calculate. Can be overruled by `mode`, 
+                e.g. if `mode='var'`, `order` is set to 2. 
+            `calcmode`, str, to use the efficient or naive algorithm to calculate distance statistics. Defaults to `fast` when available.
+        """
+        
+        
+        if 'trimming' not in kwargs:
+            trimming = 0
+        else:
+            trimming = kwargs.get('trimming')
+            
+        if 'biascorr' not in kwargs:
+            biascorr = False
+        else:
+            biascorr = kwargs.get('biascorr')
+            
+        if 'alpha' not in kwargs:
+            alpha = 1
+        else:
+            alpha = kwargs.get('alpha')
+            
+        if 'dmetric' not in kwargs:
+            dmetric = 'euclidean'
+        else:
+            dmetric = kwargs.get('dmetric')
+            
+        if 'calcmode' not in kwargs:
+            calcmode = 'fast'
+        else:
+            calcmode = kwargs.get('calcmode')
+            
+        if 'order' not in kwargs:
+            order = 2
+        else:
+            order = kwargs.get('order')
+            
+        if self.mode == 'var':
+            mode = 'mom' 
+            order=2
+        elif self.mode == 'cov':
+            mode = 'com'
+            order=2
+        elif self.mode == 'std':
+            self.center = 'mean'
+            mode = 'mom'
+            order=2
+            num_power = 0.5
+        elif self.mode == 'skew':
+            self.center = 'mean'
+            mode = 'mom'
+            order = 3
+            num_power = 1.5
+        elif self.mode == 'kurt':
+            self.center = 'mean'
+            mode = 'mom'
+            order = 4
+            num_power = 2
+            if 'Fisher' not in kwargs:
+                Fisher = True
+            else:
+                Fisher = kwargs.get('Fisher')
+        elif self.mode == 'cos':
+            self.center = 'mean'
+            mode = 'com'
+            order = 3
+            if 'standardized' not in kwargs:
+                standardized = True
+            else:
+                standardized = kwargs.get('standardized')
+        elif self.mode == 'M3':
+            self.center = 'mean'
+            mode = 'com'
+            order = 3
+            if 'standardized' not in kwargs:
+                standardized = False
+            else:
+                standardized = kwargs.get('standardized')
+        elif self.mode == 'cok':
+            self.center = 'mean'
+            mode = 'com'
+            order = 4
+            if 'standardized' not in kwargs:
+                standardized = True
+            else:
+                standardized = kwargs.get('standardized')
+            if 'Fisher' not in kwargs:
+                Fisher = True
+            else:
+                Fisher = kwargs.get('Fisher')
+        else:
+            mode = self.mode
+            
+        if order > 2: 
+            if 'option' not in kwargs:
+                option = 1
+            else:
+                option = kwargs.get('option')
+        else:
+            option = 0
+            
+        n = len(x)
+        ntrim = round(n * (1-trimming)) 
+        
+        if len(x.shape)==1:
+            x = np.array(x).reshape((n,1))
+        
+        if mode=='corr':
+            alpha = 1
+        
+        if n==0:
+            raise(MyException('Please feed data with length > 0'))
+            
+        if self.center == 'median':
+            locest = np.median
+        else:
+            locest = trim_mean
+        
+        # Classical variance, covariance and continuum as well as robust alternatives
+        if self.est=='arithmetic':
+            
+            # Variance
+            if mode!='com':
+#                if self.center=='mean':
+#                    xvar = trimvar(x,trimming)*ntrim/(ntrim-1)
+#                elif self.center=='median':
+#                    xvar = srs.mad(x)**2
+                xmom = trim_mom(x,x,locest,order,trimming,option,biascorr) 
+                self.x_moment_ = xmom
+                
+                if self.mode in ('std','skew','kurt'):
+                    x2mom = trim_mom(x,x,locest,2,trimming,option,False)
+                    xmom /= (x2mom**num_power) 
+                    if biascorr:
+                        if self.mode == 'skew':
+                            xmom *= (ntrim-1)**2
+                            xmom /= np.sqrt(ntrim**2 - ntrim)
+                        elif self.mode == 'kurt':
+                            xmom = xmom*ntrim - xmom/ntrim
+                            xmom -= 3*(ntrim-1)**2.0 / ((ntrim-2)*(ntrim-3))
+                            if not Fisher:
+                                xmom += 3
+                
+                if mode=='mom':
+                    self.moment_ = xmom
+            
+            # Covariance or continuum
+            if mode!='mom':
+                
+                if 'y' not in kwargs:
+                    raise(MyException('Please supply second data vector'))
+                else:
+                    y = kwargs.get('y')
+                
+                n1 = len(y)
+                if n1==0:
+                    raise(MyException('Please feed data with length > 0'))
+                if n1!=n:
+                    raise(MyException('Please feed x and y data of equal length'))
+                if len(y.shape)==1:
+                    y = np.array(y).reshape((n,1))
+                
+                como = trim_mom(x,y,locest,order,trimming,option,biascorr)
+                
+                self.co_moment_ = como
+                
+                if (biascorr and (order > 2)):
+                    como *= ntrim
+                
+                if mode=='com':
+                    self.moment_ = como
+                
+                    
+                if self.mode in ('cok','cos'):
+                    x2sd = np.sqrt(trim_mom(x,x,locest,2,trimming,option,biascorr))
+                    y2sd = np.sqrt(trim_mom(y,y,locest,2,trimming,option,biascorr))
+                        
+                    if ((self.mode == 'cok') and biascorr): # biascorr is only exact if standardized
+                        como -= como/(ntrim**2)
+                        como -= 3*(ntrim-1)**2.0 / ((ntrim-2)*(ntrim-3))
+
+                
+                if mode in ['corr','continuum']:
+                    
+#                    if self.center=='mean':
+#                        yvar = trimvar(y,trimming)*ntrim/(ntrim-1)
+#                    
+#                    elif self.center=='median':
+#                        yvar = srs.mad(y)
+                    ymom = trim_mom(y,y,locest,order,trimming,option,biascorr)
+                    
+                    self.y_moment_ = ymom
+                    
+        
+        # Distance based metrics
+        elif self.est=='distance':
+            
+            if 'dmetric' not in kwargs:
+                dmetric = 'euclidean'
+            else:
+                dmetric = kwargs.get('dmetric')
+                
+            if (mode in ['ballcov','mdd','mdc']):
+        
+                if 'y' not in kwargs:
+                        raise(MyException('Please supply second data vector'))
+                else:
+                    y = kwargs.get('y')
+                    n1 = len(y)
+                    if n1==0:
+                        raise(MyException('Please feed data with length > 0'))
+                    if n1!=n:
+                        raise(MyException('Please feed x and y data of equal length')) 
+                    if (mode in ['mdd','mdc']):
+                        como=np.sqrt(difference_divergence(x,y,center=self.center,trimming=trimming,biascorr=biascorr))
+                        self.co_moment_ = como
+                        if mode=='mdd':
+                            self.moment_ = como 
+                        else:
+                            xmom=difference_divergence(x,x,center=self.center,trimming=trimming,biascorr=biascorr)
+                            ymom=difference_divergence(y,y,center=self.center,trimming=trimming,biascorr=biascorr)
+                            self.x_moment_ = xmom
+                            self.y_moment_ = ymom
+                            mode = 'corr'
+
+                    else:
+                        dmy, n2 = distance_matrix_centered(y,biascorr=biascorr,
+                                                   trimming=trimming,
+                                                   center=self.center,
+                                                   dmetric=dmetric) 
+                        bcov_res=Ball.bcov_test(x,y,num_permutations=0)[0]
+                        self.moment_ = bcov_res
+                        
+                
+                
+            
+            elif (calcmode=='fast' and self.center =='mean' and trimming == 0 and order==2):
+                if mode != 'com':
+                    if biascorr:
+                        xmom = np.sqrt(dc.u_distance_covariance_sqr(x,x))
+                    else:
+                        xmom = np.sqrt(dc.distance_covariance_sqr(x,x))
+                    self.moment_ = xmom
+                if mode !='mom':
+                    if 'y' not in kwargs:
+                        raise(MyException('Please supply second data vector'))
+                    else:
+                        y = kwargs.get('y')
+                        n1 = len(y)
+                    
+                    if biascorr:
+                        como = np.sqrt(dc.u_distance_covariance_sqr(x,y))
+                    else:
+                        como = np.sqrt(dc.distance_covariance_sqr(x,y))
+                    if mode=='com':
+                        self.co_moment_ = como
+                        self.moment_ = como
+                    elif mode in ['corr','continuum','cos','cok']:
+                        if biascorr:
+                            ymom = np.sqrt(dc.u_distance_covariance_sqr(y,y))
+                        else:
+                            ymom = np.sqrt(dc.distance_covariance_sqr(y,y))
+                        self.y_moment_ = ymom
+                        
+            
+                
+            else:
+                dmx, n1 = distance_matrix_centered(x,biascorr=biascorr,
+                                                   trimming=trimming,
+                                                   center=self.center,
+                                                   dmetric=dmetric)
+                
+                # Variance
+                if mode!='com':
+                    
+                    xmom = distance_moment(dmx,dmx,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=order,option=option)
+                        
+                    self.x_moment_ = xmom
+                    
+                    if self.mode in ('std','skew','kurt'):
+                        x2mom =distance_moment(dmx,dmx,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=2,option=option)
+                        xmom /= x2mom**num_power
+                        
+                    if mode=='mom':
+                        self.moment_ = xmom
+                        
+                if mode!='mom':
+                    
+                    if 'y' not in kwargs:
+                        raise(MyException('Please supply second data vector'))
+                    else:
+                        y = kwargs.get('y')
+                        n1 = len(y)
+                    if n1==0:
+                        raise(MyException('Please feed data with length > 0'))
+                    if n1!=n:
+                        raise(MyException('Please feed x and y data of equal length'))
+                    
+                    dmy, n2 = distance_matrix_centered(y,biascorr=biascorr,
+                                                   trimming=trimming,
+                                                   center=self.center,
+                                                   dmetric=dmetric)
+                    
+                    como = distance_moment(dmx,dmy,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=order,option=option)
+                    
+                    self.co_moment_ = como
+                    
+                    if mode=='com':
+                        self.moment_ = como
+                        
+                    if self.mode in ('cok','cos'):
+                        x2sd = distance_moment(dmx,dmx,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=2,option=option)
+                        x2sd = np.sqrt(x2sd)
+                        y2sd = distance_moment(dmy,dmy,n1=n1,biascorr=biascorr,center=self.center,trimming=trimming,order=2,option=option)
+                        y2sd = np.sqrt(y2sd)
+                    
+                    if mode in ['corr','continuum','cok','cos']:
+                        
+                        ymom = distance_moment(dmy,dmy,n1=n,biascorr=biascorr,center=self.center,trimming=trimming,order=order,option=option)
+                        self.y_moment_ = ymom
+                    
+        if mode == 'corr': 
+            como /= (np.sqrt(xmom)*np.sqrt(ymom))
+            self.moment_=como
+        elif mode == 'continuum':
+            como *= como * (np.sqrt(xmom)**(alpha -1))
+            self.moment_=como
+        
+        if (self.mode in ('cok','cos') and standardized):
+            iter_stop_2 = option 
+            iter_stop_1 = order - option
+            como /= np.power(x2sd,iter_stop_1)
+            como /= np.power(y2sd,iter_stop_2)
+            if ((self.mode == 'cok') and not Fisher): # Not very meaningful for co-moment
+                como += 3
+            self.moment_=como
+        
+                        
+        
+        if type(self.moment_)==np.ndarray:
+            self.moment_ = self.moment_[0]
+        return(self.moment_)
+        
+                    
+                    
+
```

## direpack/ipopt_temp/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Apr 12 2020
-
-This folder is temporary. It copies a fix to ipopt: 
-    https://github.com/matthias-k/optpy/blob/master/optpy/jacobian.py
-    
-Folder will stay in direpack until the latter has been released.
-
-"""
-
-__name__ = "opt_temp"
-__author__ = "Sven Serneels"
-__license__ = "MIT"
-__version__ = "0.0.2"
-__date__ = "2021-04-15"
-
-
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Apr 12 2020
+
+This folder is temporary. It copies a fix to ipopt: 
+    https://github.com/matthias-k/optpy/blob/master/optpy/jacobian.py
+    
+Folder will stay in direpack until the latter has been released.
+
+"""
+
+__name__ = "opt_temp"
+__author__ = "Sven Serneels"
+__license__ = "MIT"
+__version__ = "0.0.2"
+__date__ = "2021-04-15"
+
+
```

## direpack/ipopt_temp/ipopt_wrapper.py

 * *Ordering differences only*

```diff
@@ -1,243 +1,243 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""
-cyipopt: Python wrapper for the Ipopt optimization package, written in Cython.
-
-Copyright (C) 2012-2015 Amit Aides
-Copyright (C) 2015-2018 Matthias Kümmerer
-
-Author: Matthias Kümmerer <matthias.kuemmerer@bethgelab.org>
-(original Author: Amit Aides <amitibo@tx.technion.ac.il>)
-URL: https://github.com/matthias-k/cyipopt
-License: EPL 1.0
-
-This section is copied from ipopt until the fix in jacobians.py gets included into 
-that package.
-
-"""
-
-from __future__ import absolute_import, unicode_literals
-import sys
-
-from builtins import bytes  # from the future package
-import numpy as np
-try:
-    import scipy
-except ImportError:  # scipy is not installed
-    SCIPY_INSTALLED = False
-else:
-    SCIPY_INSTALLED = True
-    del scipy
-    from scipy.optimize import approx_fprime
-    try:
-        from scipy.optimize import OptimizeResult
-    except ImportError:
-        # in scipy 0.14 Result was renamed to OptimzeResult
-        from scipy.optimize import Result
-        OptimizeResult = Result
-
-import cyipopt
-from .jacobian import FunctionWithApproxJacobianCentral,FunctionWithApproxJacobian
-
-
-class IpoptProblemWrapper(object):
-    def __init__(self, fun, args=(), kwargs=None, jac=None, hess=None, hessp=None,
-                 constraints=(), eps=1e-8):
-        if not SCIPY_INSTALLED:
-            raise ImportError('Install SciPy to use the `IpoptProblemWrapper` class.')
-        self.fun_with_jac = None
-        self.last_x = None
-        if hess is not None or hessp is not None:
-            raise NotImplementedError('Using hessian matrixes is not yet implemented!')
-        if jac is None:
-            #fun = FunctionWithApproxJacobian(fun, epsilon=eps, verbose=False)
-            jac = lambda x0, *args, **kwargs: approx_fprime(x0, fun, eps, *args, **kwargs)
-        elif jac is True:
-            self.fun_with_jac = fun
-        elif not callable(jac):
-            raise NotImplementedError('jac has to be bool or a function')
-        self.fun = fun
-        self.jac = jac
-        self.args = args
-        self.kwargs = kwargs or {}
-        self._constraint_funs = []
-        self._constraint_jacs = []
-        self._constraint_args = []
-        if isinstance(constraints, dict):
-            constraints = (constraints, )
-        for con in constraints:
-            con_fun = con['fun']
-            con_jac = con.get('jac', None)
-            if con_jac is None:
-                con_fun = FunctionWithApproxJacobian(con_fun, epsilon=eps, verbose=False)
-                con_jac = con_fun.jac
-            con_args = con.get('args', [])
-            self._constraint_funs.append(con_fun)
-            self._constraint_jacs.append(con_jac)
-            self._constraint_args.append(con_args)
-        # Set up evaluation counts
-        self.nfev = 0
-        self.njev = 0
-        self.nit = 0
-
-    def evaluate_fun_with_grad(self, x):
-        if self.last_x is None or not np.all(self.last_x == x):
-            self.last_x = x
-            self.nfev += 1
-            self.last_value = self.fun(x, *self.args, **self.kwargs)
-        return self.last_value
-
-    def objective(self, x):
-        if self.fun_with_jac:
-            return self.evaluate_fun_with_grad(x)[0]
-
-        self.nfev += 1
-        return self.fun(x, *self.args, **self.kwargs)
-
-    def gradient(self, x, **kwargs):
-        if self.fun_with_jac:
-            return self.evaluate_fun_with_grad(x)[1]
-
-        self.njev += 1
-        return self.jac(x, *self.args, **self.kwargs)  # .T
-
-    def constraints(self, x):
-        con_values = []
-        for fun, args in zip(self._constraint_funs, self._constraint_args):
-            con_values.append(fun(x, *args))
-        return np.hstack(con_values)
-
-    def jacobian(self, x):
-        con_values = []
-        for fun, args in zip(self._constraint_jacs, self._constraint_args):
-            con_values.append(fun(x, *args))
-        return np.vstack(con_values)
-
-    def intermediate(
-            self,
-            alg_mod,
-            iter_count,
-            obj_value,
-            inf_pr,
-            inf_du,
-            mu,
-            d_norm,
-            regularization_size,
-            alpha_du,
-            alpha_pr,
-            ls_trials
-            ):
-
-        self.nit = iter_count
-
-
-def get_bounds(bounds):
-    if bounds is None:
-        return None, None
-    else:
-        lb = [b[0] for b in bounds]
-        ub = [b[1] for b in bounds]
-        return lb, ub
-
-
-def get_constraint_bounds(constraints, x0, INF=1e19):
-    if isinstance(constraints, dict):
-        constraints = (constraints, )
-    cl = []
-    cu = []
-    if isinstance(constraints, dict):
-        constraints = (constraints, )
-    for con in constraints:
-        m = len(np.atleast_1d(con['fun'](x0, *con.get('args', []))))
-        cl.extend(np.zeros(m))
-        if con['type'] == 'eq':
-            cu.extend(np.zeros(m))
-        elif con['type'] == 'ineq':
-            cu.extend(INF*np.ones(m))
-        else:
-            raise ValueError(con['type'])
-    cl = np.array(cl)
-    cu = np.array(cu)
-
-    return cl, cu
-
-
-def replace_option(options, oldname, newname):
-    if oldname in options:
-        if newname not in options:
-            options[newname] = options.pop(oldname)
-
-def convert_to_bytes(options):
-    if sys.version_info >= (3, 0):
-        for key in list(options.keys()):
-            try:
-                if bytes(key, 'utf-8') != key:
-                    options[bytes(key, 'utf-8')] = options[key]
-                    options.pop(key)
-            except TypeError:
-                pass
-
-def minimize_ipopt(fun, x0, args=(), kwargs=None, method=None, jac=None, hess=None, hessp=None,
-                   bounds=None, constraints=(), tol=None, callback=None, options=None):
-    """
-    Minimize a function using ipopt. The call signature is exactly like for
-    `scipy.optimize.mimize`. In options, all options are directly passed to
-    ipopt. Check [http://www.coin-or.org/Ipopt/documentation/node39.html] for
-    details.
-    The options `disp` and `maxiter` are automatically mapped to their
-    ipopt-equivalents `print_level` and `max_iter`.
-    """
-    if not SCIPY_INSTALLED:
-        raise ImportError('Install SciPy to use the `minimize_ipopt` function.')
-
-    _x0 = np.atleast_1d(x0)
-    problem = IpoptProblemWrapper(fun, args=args, kwargs=kwargs, jac=jac, hess=hess,
-                                  hessp=hessp, constraints=constraints)
-    lb, ub = get_bounds(bounds)
-
-    cl, cu = get_constraint_bounds(constraints, x0)
-
-    if options is None:
-        options = {}
-
-    nlp = cyipopt.problem(n = len(_x0),
-                          m = len(cl),
-                          problem_obj=problem,
-                          lb=lb,
-                          ub=ub,
-                          cl=cl,
-                          cu=cu)
-
-    # python3 compatibility
-    convert_to_bytes(options)
-
-    # Rename some default scipy options
-    replace_option(options, b'disp', b'print_level')
-    replace_option(options, b'maxiter', b'max_iter')
-    if b'print_level' not in options:
-        options[b'print_level'] = 0
-    if b'tol' not in options:
-        options[b'tol'] = tol or 1e-8
-    if b'mu_strategy' not in options:
-        options[b'mu_strategy'] = b'adaptive'
-    if b'hessian_approximation' not in options:
-        if hess is None and hessp is None:
-            options[b'hessian_approximation'] = b'limited-memory'
-    for option, value in options.items():
-        try:
-            nlp.addOption(option, value)
-        except TypeError as e:
-            raise TypeError('Invalid option for IPOPT: {0}: {1} (Original message: "{2}")'.format(option, value, e))
-
-    x, info = nlp.solve(_x0)
-
-    if np.asarray(x0).shape == ():
-        x = x[0]
-
-    return OptimizeResult(x=x, success=info['status'] == 0, status=info['status'],
-                          message=info['status_msg'],
-                          fun=info['obj_val'],
-                          info=info,
-                          nfev=problem.nfev,
-                          njev=problem.njev,
-                          nit=problem.nit)
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+"""
+cyipopt: Python wrapper for the Ipopt optimization package, written in Cython.
+
+Copyright (C) 2012-2015 Amit Aides
+Copyright (C) 2015-2018 Matthias Kümmerer
+
+Author: Matthias Kümmerer <matthias.kuemmerer@bethgelab.org>
+(original Author: Amit Aides <amitibo@tx.technion.ac.il>)
+URL: https://github.com/matthias-k/cyipopt
+License: EPL 1.0
+
+This section is copied from ipopt until the fix in jacobians.py gets included into 
+that package.
+
+"""
+
+from __future__ import absolute_import, unicode_literals
+import sys
+
+from builtins import bytes  # from the future package
+import numpy as np
+try:
+    import scipy
+except ImportError:  # scipy is not installed
+    SCIPY_INSTALLED = False
+else:
+    SCIPY_INSTALLED = True
+    del scipy
+    from scipy.optimize import approx_fprime
+    try:
+        from scipy.optimize import OptimizeResult
+    except ImportError:
+        # in scipy 0.14 Result was renamed to OptimzeResult
+        from scipy.optimize import Result
+        OptimizeResult = Result
+
+import cyipopt
+from .jacobian import FunctionWithApproxJacobianCentral,FunctionWithApproxJacobian
+
+
+class IpoptProblemWrapper(object):
+    def __init__(self, fun, args=(), kwargs=None, jac=None, hess=None, hessp=None,
+                 constraints=(), eps=1e-8):
+        if not SCIPY_INSTALLED:
+            raise ImportError('Install SciPy to use the `IpoptProblemWrapper` class.')
+        self.fun_with_jac = None
+        self.last_x = None
+        if hess is not None or hessp is not None:
+            raise NotImplementedError('Using hessian matrixes is not yet implemented!')
+        if jac is None:
+            #fun = FunctionWithApproxJacobian(fun, epsilon=eps, verbose=False)
+            jac = lambda x0, *args, **kwargs: approx_fprime(x0, fun, eps, *args, **kwargs)
+        elif jac is True:
+            self.fun_with_jac = fun
+        elif not callable(jac):
+            raise NotImplementedError('jac has to be bool or a function')
+        self.fun = fun
+        self.jac = jac
+        self.args = args
+        self.kwargs = kwargs or {}
+        self._constraint_funs = []
+        self._constraint_jacs = []
+        self._constraint_args = []
+        if isinstance(constraints, dict):
+            constraints = (constraints, )
+        for con in constraints:
+            con_fun = con['fun']
+            con_jac = con.get('jac', None)
+            if con_jac is None:
+                con_fun = FunctionWithApproxJacobian(con_fun, epsilon=eps, verbose=False)
+                con_jac = con_fun.jac
+            con_args = con.get('args', [])
+            self._constraint_funs.append(con_fun)
+            self._constraint_jacs.append(con_jac)
+            self._constraint_args.append(con_args)
+        # Set up evaluation counts
+        self.nfev = 0
+        self.njev = 0
+        self.nit = 0
+
+    def evaluate_fun_with_grad(self, x):
+        if self.last_x is None or not np.all(self.last_x == x):
+            self.last_x = x
+            self.nfev += 1
+            self.last_value = self.fun(x, *self.args, **self.kwargs)
+        return self.last_value
+
+    def objective(self, x):
+        if self.fun_with_jac:
+            return self.evaluate_fun_with_grad(x)[0]
+
+        self.nfev += 1
+        return self.fun(x, *self.args, **self.kwargs)
+
+    def gradient(self, x, **kwargs):
+        if self.fun_with_jac:
+            return self.evaluate_fun_with_grad(x)[1]
+
+        self.njev += 1
+        return self.jac(x, *self.args, **self.kwargs)  # .T
+
+    def constraints(self, x):
+        con_values = []
+        for fun, args in zip(self._constraint_funs, self._constraint_args):
+            con_values.append(fun(x, *args))
+        return np.hstack(con_values)
+
+    def jacobian(self, x):
+        con_values = []
+        for fun, args in zip(self._constraint_jacs, self._constraint_args):
+            con_values.append(fun(x, *args))
+        return np.vstack(con_values)
+
+    def intermediate(
+            self,
+            alg_mod,
+            iter_count,
+            obj_value,
+            inf_pr,
+            inf_du,
+            mu,
+            d_norm,
+            regularization_size,
+            alpha_du,
+            alpha_pr,
+            ls_trials
+            ):
+
+        self.nit = iter_count
+
+
+def get_bounds(bounds):
+    if bounds is None:
+        return None, None
+    else:
+        lb = [b[0] for b in bounds]
+        ub = [b[1] for b in bounds]
+        return lb, ub
+
+
+def get_constraint_bounds(constraints, x0, INF=1e19):
+    if isinstance(constraints, dict):
+        constraints = (constraints, )
+    cl = []
+    cu = []
+    if isinstance(constraints, dict):
+        constraints = (constraints, )
+    for con in constraints:
+        m = len(np.atleast_1d(con['fun'](x0, *con.get('args', []))))
+        cl.extend(np.zeros(m))
+        if con['type'] == 'eq':
+            cu.extend(np.zeros(m))
+        elif con['type'] == 'ineq':
+            cu.extend(INF*np.ones(m))
+        else:
+            raise ValueError(con['type'])
+    cl = np.array(cl)
+    cu = np.array(cu)
+
+    return cl, cu
+
+
+def replace_option(options, oldname, newname):
+    if oldname in options:
+        if newname not in options:
+            options[newname] = options.pop(oldname)
+
+def convert_to_bytes(options):
+    if sys.version_info >= (3, 0):
+        for key in list(options.keys()):
+            try:
+                if bytes(key, 'utf-8') != key:
+                    options[bytes(key, 'utf-8')] = options[key]
+                    options.pop(key)
+            except TypeError:
+                pass
+
+def minimize_ipopt(fun, x0, args=(), kwargs=None, method=None, jac=None, hess=None, hessp=None,
+                   bounds=None, constraints=(), tol=None, callback=None, options=None):
+    """
+    Minimize a function using ipopt. The call signature is exactly like for
+    `scipy.optimize.mimize`. In options, all options are directly passed to
+    ipopt. Check [http://www.coin-or.org/Ipopt/documentation/node39.html] for
+    details.
+    The options `disp` and `maxiter` are automatically mapped to their
+    ipopt-equivalents `print_level` and `max_iter`.
+    """
+    if not SCIPY_INSTALLED:
+        raise ImportError('Install SciPy to use the `minimize_ipopt` function.')
+
+    _x0 = np.atleast_1d(x0)
+    problem = IpoptProblemWrapper(fun, args=args, kwargs=kwargs, jac=jac, hess=hess,
+                                  hessp=hessp, constraints=constraints)
+    lb, ub = get_bounds(bounds)
+
+    cl, cu = get_constraint_bounds(constraints, x0)
+
+    if options is None:
+        options = {}
+
+    nlp = cyipopt.problem(n = len(_x0),
+                          m = len(cl),
+                          problem_obj=problem,
+                          lb=lb,
+                          ub=ub,
+                          cl=cl,
+                          cu=cu)
+
+    # python3 compatibility
+    convert_to_bytes(options)
+
+    # Rename some default scipy options
+    replace_option(options, b'disp', b'print_level')
+    replace_option(options, b'maxiter', b'max_iter')
+    if b'print_level' not in options:
+        options[b'print_level'] = 0
+    if b'tol' not in options:
+        options[b'tol'] = tol or 1e-8
+    if b'mu_strategy' not in options:
+        options[b'mu_strategy'] = b'adaptive'
+    if b'hessian_approximation' not in options:
+        if hess is None and hessp is None:
+            options[b'hessian_approximation'] = b'limited-memory'
+    for option, value in options.items():
+        try:
+            nlp.addOption(option, value)
+        except TypeError as e:
+            raise TypeError('Invalid option for IPOPT: {0}: {1} (Original message: "{2}")'.format(option, value, e))
+
+    x, info = nlp.solve(_x0)
+
+    if np.asarray(x0).shape == ():
+        x = x[0]
+
+    return OptimizeResult(x=x, success=info['status'] == 0, status=info['status'],
+                          message=info['status_msg'],
+                          fun=info['obj_val'],
+                          info=info,
+                          nfev=problem.nfev,
+                          njev=problem.njev,
+                          nit=problem.nit)
```

## direpack/ipopt_temp/jacobian.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-"""
-Author: Matthias Kuemmerer, 2014
-"""
-from __future__ import print_function, division, unicode_literals, absolute_import
-
-import sys
-import numpy as np
-
-
-class FunctionWithApproxJacobian(object):
-    def __init__(self, func, epsilon, verbose=True):
-        self._func = func
-        self.epsilon = epsilon
-        self.value_cache = {}
-        self.verbose = verbose
-
-    def __call__(self, x, *args, **kwargs):
-        key = tuple(x)
-        if not key in self.value_cache:
-            self.log('.')
-            value = self._func(x, *args, **kwargs)
-            if np.any(np.isnan(value)):
-                print("Warning! nan function value encountered at {0}".format(x))
-            self.value_cache[key] = value
-        return self.value_cache[key]
-
-    def func(self, x, *args, **kwargs):
-        if self.verbose:
-            print(x)
-        return self(x, *args, **kwargs)
-
-    def log(self, msg):
-        if self.verbose:
-            sys.stdout.write(msg)
-            sys.stdout.flush()
-
-    def jac(self, x, *args, **kwargs):
-        self.log('G[')
-        x0 = np.asfarray(x)
-        #print x0
-        dxs = np.zeros((len(x0), len(x0) + 1))
-        for i in range(len(x0)):
-            dxs[i, i + 1] = self.epsilon
-        results = [self(*(x0 + dxs[:, i], ) + args, **kwargs) for i in range(len(x0) + 1)]
-        jac = np.zeros([len(x0), len(np.atleast_1d(results[0]))])
-        for i in range(len(x0)):
-            jac[i] = (results[i + 1] - results[0]) / self.epsilon
-        self.log(']')
-        return jac.transpose()
-
-
-class FunctionWithApproxJacobianCentral(FunctionWithApproxJacobian):
-    def jac(self, x, *args, **kwargs):
-        self.log('G[')
-        x0 = np.asfarray(x)
-        #print x0
-        dxs = np.zeros((len(x0), 2*len(x0)))
-        for i in range(len(x0)):
-            dxs[i, i] = -self.epsilon
-            dxs[i, len(x0)+i] = self.epsilon
-        results = [self(*(x0 + dxs[:, i], ) + args, **kwargs) for i in range(2*len(x0))]
-        jac = np.zeros([len(x0), len(np.atleast_1d(results[0]))])
-        for i in range(len(x0)):
-            jac[i] = (results[len(x0)+i] - results[i]) / (2*self.epsilon)
-        self.log(']')
+"""
+Author: Matthias Kuemmerer, 2014
+"""
+from __future__ import print_function, division, unicode_literals, absolute_import
+
+import sys
+import numpy as np
+
+
+class FunctionWithApproxJacobian(object):
+    def __init__(self, func, epsilon, verbose=True):
+        self._func = func
+        self.epsilon = epsilon
+        self.value_cache = {}
+        self.verbose = verbose
+
+    def __call__(self, x, *args, **kwargs):
+        key = tuple(x)
+        if not key in self.value_cache:
+            self.log('.')
+            value = self._func(x, *args, **kwargs)
+            if np.any(np.isnan(value)):
+                print("Warning! nan function value encountered at {0}".format(x))
+            self.value_cache[key] = value
+        return self.value_cache[key]
+
+    def func(self, x, *args, **kwargs):
+        if self.verbose:
+            print(x)
+        return self(x, *args, **kwargs)
+
+    def log(self, msg):
+        if self.verbose:
+            sys.stdout.write(msg)
+            sys.stdout.flush()
+
+    def jac(self, x, *args, **kwargs):
+        self.log('G[')
+        x0 = np.asfarray(x)
+        #print x0
+        dxs = np.zeros((len(x0), len(x0) + 1))
+        for i in range(len(x0)):
+            dxs[i, i + 1] = self.epsilon
+        results = [self(*(x0 + dxs[:, i], ) + args, **kwargs) for i in range(len(x0) + 1)]
+        jac = np.zeros([len(x0), len(np.atleast_1d(results[0]))])
+        for i in range(len(x0)):
+            jac[i] = (results[i + 1] - results[0]) / self.epsilon
+        self.log(']')
+        return jac.transpose()
+
+
+class FunctionWithApproxJacobianCentral(FunctionWithApproxJacobian):
+    def jac(self, x, *args, **kwargs):
+        self.log('G[')
+        x0 = np.asfarray(x)
+        #print x0
+        dxs = np.zeros((len(x0), 2*len(x0)))
+        for i in range(len(x0)):
+            dxs[i, i] = -self.epsilon
+            dxs[i, len(x0)+i] = self.epsilon
+        results = [self(*(x0 + dxs[:, i], ) + args, **kwargs) for i in range(2*len(x0))]
+        jac = np.zeros([len(x0), len(np.atleast_1d(results[0]))])
+        for i in range(len(x0)):
+            jac[i] = (results[len(x0)+i] - results[i]) / (2*self.epsilon)
+        self.log(']')
         return jac.transpose()
```

## direpack/plot/__init__.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Jul 22 12:17:17 2018
-
-@author: Sven Serneels, Ponalytics
-"""
-
-__name__ = "plot"
-__author__ = "Sven Serneels"
-__license__ = "MIT"
-__version__ = "0.9.0"
-__date__ = "2020-04-18"
-
-
-
-
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Jul 22 12:17:17 2018
+
+@author: Sven Serneels, Ponalytics
+"""
+
+__name__ = "plot"
+__author__ = "Sven Serneels"
+__license__ = "MIT"
+__version__ = "0.9.0"
+__date__ = "2020-04-18"
+
+
+
+
```

## direpack/plot/sprm_plot.py

 * *Ordering differences only*

```diff
@@ -1,380 +1,380 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Jan 27 11:41:35 2019
-
-SPRM Plotting options (based on Matplotlib)
----------------------
-
-In class sprm_plot
-    Y vs Y predicted plot with outlier flagging
-    Projection to score space with outlier flagging 
-    Barchart of regression coefficients with range selection 
-    Barchart of caseweights 
-        All of these plots work both with or without new test datya beoing s
-        supplied and have the optoion to only plot the test data.
-        
-In class sprm_plot_cv
-    3D contour for SPRM Grid Search CV results 
-        Requires an sklearn GridSearchCV object that trained an SPRM model 
-        Provides a 3d contour plot across the (eta, n_components) space
-
-Version 0.2: Ancillary functions: have been moved to ._plot_internals
-
-@author: Sven Serneels, Ponalytics. 
-"""
-
-from __future__ import absolute_import, division, print_function
-from __future__ import unicode_literals
-
-from ..sprm.sprm import sprm
-from ..utils.utils import MyException, convert_X_input, convert_y_input
-from ..cross_validation._cv_support_functions import cv_score_table
-
-import matplotlib.pyplot as pp 
-import numpy as np
-from sklearn.model_selection import GridSearchCV
-
-class sprm_plot(sprm):
-    
-    def __init__(self,res_sprm,colors,markers=['o','d','v'],*args):
-        """
-        Initialize with 
-        res_sprm, an sprm class object
-        
-        Only mandatory input is colors, a list of colors for 
-            [0] borders of pane 
-            [1] plot background
-            [2] marker fill
-            [3] diagonal line 
-            [4] marker contour, if different from fill
-            [5] marker color for new cases, if applicable
-            [6] marker color for harsh calibration outliers
-            [7] marker color for harsh prediction outliers
-        
-        Optional input markers, a list:
-            [0] marker for regular cases
-            [1] marker for moderate outliers (caseweight in (0,1))
-            [2] marker for harsh outliers (caseweight = 0)
-        
-        """
-        if not(isinstance(res_sprm,sprm)):
-            raise(MyException("Object supplied to sprmplot needs to be an sprm object"))
-        self.res_sprm = res_sprm
-        self.colors = colors
-        self.markers = markers
-
-    def plot_yyp(self,ytruev=[],Xn=[],label=[],names=[],namesv=[],title=[],legend_pos='lower right',onlyval=False):
-        """
-        plot_yyp will plot y vs y predicted for SPRM opbjects
-        Optional inputs: 
-            ytruev: array (new_cases,) of predictands
-            Xn: array (new_cases,variables) of predictors 
-            If these arguments are supplied, SPRM predictions for ytrue will be 
-                made from Xn through res_sprm.predict()
-            label: string: name of variable to be plotted. Will show in legend.
-            names: list or tuple of strings, casenames from training set
-            namesv: list or tuple of strings, casenames from test set
-            title: String containing plot title
-            legend_pos: string containing legend position
-            onlyval: boolean: only plot validation cases
-        """
-        
-        if len(label)==0:
-            label = 'none'
-        fig = pp.figure()
-        fig.set_facecolor(self.colors[0])
-        pp.rcParams['axes.facecolor'] = self.colors[1]
-        ax1 = fig.add_subplot(111)
-        if (not(onlyval)):
-            ytruec = self.res_sprm.y
-            if len(ytruec.shape) >1:
-                ytruec = np.array(ytruec).reshape(-1).astype('float64')
-            ypredc = np.array(self.res_sprm.fitted_).T.reshape(-1)
-            labelcr = label[0] + ' Training' + ' Regular'
-            labelcm = label[0] + ' Training' + ' Moderate'
-            labelch = label[0] + ' Training' + ' Harsh'
-            reg_cases = np.where(self.res_sprm.caseweights_ == 1)
-            mod_outliers = np.where((self.res_sprm.caseweights_ > 0) & (self.res_sprm.caseweights_ < 1))
-            harsh_outliers = np.where(self.res_sprm.caseweights_ == 0)
-            ax1.scatter(ytruec[reg_cases], ypredc[reg_cases], c=self.colors[2],label=labelcr, 
-                    zorder=1,edgecolors=self.colors[4],marker=self.markers[0])
-            if len(mod_outliers[0]>0):
-                ax1.scatter(ytruec[mod_outliers], ypredc[mod_outliers], c=self.colors[2],label=labelcm, 
-                    zorder=1,edgecolors=self.colors[4],marker=self.markers[1])
-            if len(harsh_outliers[0]>0):
-                ax1.scatter(ytruec[harsh_outliers], ypredc[harsh_outliers], c=self.colors[6],label=labelch, 
-                    zorder=1,edgecolors=self.colors[4],marker=self.markers[2])
-        else:
-            if (len(Xn)==0):
-                ValueError('In onlyval=True mode, new cases Xn need to be provided')
-        if not(len(Xn)==0):
-            if len(ytruev.shape) >1:
-                ytruev = np.array(ytruev).reshape(-1).astype('float64')
-            ypredv = self.res_sprm.predict(Xn)
-            ypredv = np.array(ypredv).reshape(-1).astype('float64')
-            wv = self.res_sprm.weightnewx(Xn)
-            labelvr = label[0] + ' Test' + ' Regular'
-            labelvm = label[0] + ' Test' + ' Moderate'
-            labelvh = label[0] + ' Test' + ' Harsh'
-            reg_cases = np.where(wv == 1)[0]
-            mod_outliers = np.where((wv > 0) & (wv < 1))[0]
-            harsh_outliers = np.where(wv == 0)[0]
-            if len(reg_cases>0):
-                ax1.scatter(ytruev[reg_cases],ypredv[reg_cases],c=self.colors[5],label=labelvr,
-                        zorder=1,edgecolors=self.colors[4],marker=self.markers[0])
-            if len(mod_outliers>0):
-                ax1.scatter(ytruev[mod_outliers],ypredv[mod_outliers],c=self.colors[5],label=labelvm,
-                        zorder=1,edgecolors=self.colors[4],marker=self.markers[1])
-            if len(harsh_outliers>0):
-                ax1.scatter(ytruev[harsh_outliers],ypredv[harsh_outliers],c=self.colors[7],label=labelvh,
-                        zorder=1,edgecolors=self.colors[4],marker=self.markers[2])
-        x_abline = np.array(ax1.get_xbound())
-        ax1.add_line(pp.Line2D(x_abline,x_abline,color=self.colors[3]))
-        if len(label)==0:
-            ax1.legend_.remove()
-        else:
-            pp.legend(loc=legend_pos)
-        if len(names)>0:
-            if not(onlyval):
-                for i in range(0,len(names)-1):
-                    ax1.annotate(names[i], (ytruec[i],ypredc[i]))
-        if len(namesv)>0:
-            for i in range(0,len(namesv)-1):
-                ax1.annotate(namesv[i], (ytruev[i],ypredv[i]))
-        if len(title)>0:
-            pp.title(title)
-        pp.show()
-        
-    def plot_coeffs(self,entity="coef_",truncation=0,columns=[],title=[]):
-        """
-        plot_coeffs will plot estimated model parameters, with option to 
-        truncate (useful if highly multivariate) 
-        Optional Inputs: 
-            entity, str, exact name of attribute from sprm object to be plotted
-            truncation, float in [0,1), Percentage of smallest and largest 
-                coefficients to be plotted
-            columns, array, tuple or list of str, variable names
-            title, str, plot title
-        """
-        fig = pp.figure()
-        fig.set_facecolor(self.colors[0])
-        pp.rcParams['axes.facecolor'] = self.colors[1]
-        ax1 = fig.add_subplot(111)
-        p = len(self.res_sprm.non_zero_scale_vars_)
-        if len(columns) > 0:
-            x_plot = columns[self.res_sprm.non_zero_scale_vars_]
-            x_labels = columns[self.res_sprm.non_zero_scale_vars_]
-        else:
-            x_plot = np.arange(0,p)
-       
-        b_orig = np.array(getattr(self.res_sprm,entity)).reshape(-1)
-        if truncation > 0:
-            ind_sort = np.argsort(np.argsort(b_orig))
-            b_sort = np.sort(b_orig)
-            left = np.ceil(p*(truncation/2)).astype(int)
-            right = (p-np.ceil(p*(truncation/2)).astype(int))
-            b_plot = b_sort[np.union1d(np.arange(0,left),
-                                       np.arange(right,p))]
-            x_labels = np.union1d(x_plot[ind_sort[0:left]],
-                                x_plot[ind_sort[right:p]])
-            x_plot = np.arange(0,len(x_labels))
-            ax1.bar(x_plot,b_plot,color=self.colors[2],edgecolor=self.colors[4],tick_label=x_labels)
-        else:
-            b_plot = b_orig 
-            ax1.bar(x_plot,b_plot,color=self.colors[2],edgecolor=self.colors[4])  
-        if ((truncation > 0) | len(columns) > 0): 
-            pp.xticks(x_plot, x_labels, rotation='vertical')
-            pp.margins(.4)
-        if len(title)>0:
-            pp.title(title)
-        pp.show()
-        
-        
-    def plot_projections(self,Xn=[],label=[],components = [0,1],names=[],namesv=[],title=[],legend_pos='lower right',onlyval=False):
-        
-        """
-        plot_projections will plot the score space  
-        Optional inputs: 
-            Xn: array (new_cases,variables) of predictors 
-            If supplied, SPRM projections for new cases will be 
-                made from Xn through res_sprm.transform()
-            label: string: name of variable to be plotted. Will show in legend.
-            names: list or tuple of strings, casenames from training set
-            namesv: list or tuple of strings, casenames from test set
-            title: String containing plot title
-            legend_pos: string containing legend position
-            onlyval: boolean: only plot validation cases
-        """
-        
-        if len(label)==0:
-            label = 'none'
-        fig = pp.figure()
-        fig.set_facecolor(self.colors[0])
-        pp.rcParams['axes.facecolor'] = self.colors[1]
-        ax1 = fig.add_subplot(111)
-        if (not(onlyval)):
-            Tc = np.array(self.res_sprm.x_scores_)
-            labelcr = label[0] + ' Training' + ' Regular'
-            labelcm = label[0] + ' Training' + ' Moderate'
-            labelch = label[0] + ' Training' + ' Harsh'
-            reg_cases = np.where(self.res_sprm.caseweights_ == 1)
-            mod_outliers = np.where((self.res_sprm.caseweights_ > 0) & (self.res_sprm.caseweights_ < 1))
-            harsh_outliers = np.where(self.res_sprm.caseweights_ == 0)
-            ax1.scatter(Tc[reg_cases,components[0]], Tc[reg_cases,components[1]], c=self.colors[2],label=labelcr, 
-                    zorder=1,edgecolors=self.colors[4],marker=self.markers[0])
-            if len(mod_outliers[0])>0:
-                ax1.scatter(Tc[mod_outliers,components[0]], Tc[mod_outliers,components[1]], c=self.colors[2],label=labelcm, 
-                    zorder=1,edgecolors=self.colors[4],marker=self.markers[1])
-            if len(harsh_outliers[0])>0:
-                ax1.scatter(Tc[harsh_outliers,components[0]], Tc[harsh_outliers,components[1]], c=self.colors[6],label=labelch, 
-                    zorder=1,edgecolors=self.colors[4],marker=self.markers[2])
-        else:
-            if (len(Xn)==0):
-                ValueError('In onlyval=True mode, new cases Xn need to be provided')
-        if not(len(Xn)==0):
-            Tv = np.array(self.res_sprm.transform(Xn))
-            labelvr = label[0] + ' Test' + ' Regular'
-            labelvm = label[0] + ' Test' + ' Moderate'
-            labelvh = label[0] + ' Test' + ' Harsh'
-            wv = self.res_sprm.weightnewx(Xn)
-            reg_cases = np.where(wv == 1)
-            mod_outliers = np.where((wv > 0) & (wv < 1))
-            harsh_outliers = np.where(wv == 0)
-            if len(reg_cases[0]>0):
-                ax1.scatter(Tv[reg_cases,components[0]], Tv[reg_cases,components[1]],c=self.colors[5],label=labelvr,
-                        zorder=1,edgecolors=self.colors[4],marker=self.markers[0])
-            if len(mod_outliers[0])>0:
-                ax1.scatter(Tv[mod_outliers,components[0]], Tv[mod_outliers,components[1]],c=self.colors[5],label=labelvm,
-                        zorder=1,edgecolors=self.colors[4],marker=self.markers[1])
-            if len(harsh_outliers[0])>0:
-                ax1.scatter(Tv[harsh_outliers,components[0]], Tv[harsh_outliers,components[1]],c=self.colors[7],label=labelvh,
-                        zorder=1,edgecolors=self.colors[4],marker=self.markers[2])
-        if len(label)==0:
-            ax1.legend_.remove()
-        else:
-            pp.legend(loc=legend_pos)
-        if len(names)>0:
-            if not(onlyval):
-                for i in range(0,len(names)-1):
-                    ax1.annotate(names[i], (Tc[i,components[0]], Tc[i,components[1]]))
-        if len(namesv)>0:
-            for i in range(0,len(namesv)-1):
-                ax1.annotate(namesv[i], (Tv[i,components[0]], Tv[i,components[1]]))
-        if len(title)>0:
-            pp.title(title)
-        pp.show()
-        
-    def plot_caseweights(self,Xn=[],label=[],names=[],namesv=[],title=[],legend_pos='lower right',onlyval=False,mode='overall'):
-        
-        """
-        plot_caseweights will plot caseweights
-        Optional inputs: 
-            Xn: array (new_cases,variables) of predictors 
-            If supplied, SPRM projections for new cases will be 
-                made from Xn through res_sprm.weightnewx()
-            label: string: name of variable to be plotted. Will show in legend.
-            names: list or tuple of strings, casenames from training set
-            namesv: list or tuple of strings, casenames from test set
-            title: String containing plot title
-            legend_pos: string containing legend position
-            onlyval: boolean: only plot validation cases
-            mode: str, which weights to plot for cases from training set, 
-                - 'overall': combined caseweights
-                - 'x': predictor block 
-                - 'y': predictand block
-                Since for validation cases y is unknown, 'x' caseweights are 
-                plotted by default there. 
-        """
-        
-        if len(label)==0:
-            label = 'none'
-        fig = pp.figure()
-        fig.set_facecolor(self.colors[0])
-        pp.rcParams['axes.facecolor'] = self.colors[1]
-        ax1 = fig.add_subplot(111)
-        if (not(onlyval)):
-            if mode=='overall':
-                wc = self.res_sprm.caseweights_
-            elif mode == 'x':
-                wc = self.res_sprm.x_caseweights_
-            elif mode == 'y':
-                wc = self.res_sprm.y_caseweights_
-            else:
-                ValueError('Options for mode are overall, x or y')
-            labelc = label[0] + ' Training' 
-        else:
-            wc = []
-            labelc=[]
-            if (len(Xn)==0):
-                ValueError('In onlyval=True mode, new cases Xn need to be provided')
-        if not(len(Xn)==0):
-            wv = self.res_sprm.weightnewx(Xn)
-            labelv = label[0] + ' Test'
-        else:
-            wv=[]
-            labelv = []
-        name_indices = np.array(range(1,len(wc)+len(wv)+1)) 
-        if (len(wc)>0):
-            ax1.bar(name_indices,np.concatenate((wc,np.repeat(np.nan,len(wv)))),color=self.colors[2],label=labelc)
-        if (len(wv)>0):
-            ax1.bar(name_indices,np.concatenate((np.repeat(np.nan,len(wc)),wv)),color=self.colors[5],label=labelv)
-        if len(label)==0:
-            ax1.legend_.remove()
-        else:
-            pp.legend(loc=legend_pos)
-        if len(names)>0:
-            if not(onlyval):
-                for i in range(0,len(names)-1):
-                    ax1.annotate(names[i], (name_indices[i+1],wc[i]))
-        if len(namesv)>0:
-            for i in range(0,len(namesv)-1):
-                ax1.annotate(namesv[i], (name_indices[len(wc)+i],wv[i]))
-        if len(title)>0:
-            pp.title(title)
-        pp.show()
-        
-
-
-class sprm_plot_cv(GridSearchCV,sprm):
-    
-    def __init__(self,res_sprm_cv,colors,*args):
-        
-        """
-        Initialize with 
-        res_sprm_cv, an GridSearchCV cross-validated sprm object 
-        
-        Only mandatory input is colors, a list of colors for 
-            [0] borders of pane 
-            [1] plot background
-            [2] marker fill
-            [3] diagonal line 
-            [4] marker contour, if different from fill
-            [5] marker color for new cases, if applicable
-            [6] marker color for harsh calibration outliers
-            [7] marker color for harsh prediction outliers
-        
-        """
-        
-        self.res_sprm_cv = res_sprm_cv
-        self.colors = colors
-        
-    def eta_ncomp_contour(self,title='SPRM Cross-Validation Contour Plot'):
-        
-        """
-        Function to draw contour plot from cross-valation results. 
-        Optional Input: 
-        title, str. Plot title. 
-        
-        """
-        
-        if not(hasattr(self,'cv_score_table_')):
-            cv_score_table_ = cv_score_table(self.res_sprm_cv) 
-            setattr(self,'cv_score_table_',cv_score_table_)
-        fig = pp.figure()
-        fig.set_facecolor(self.colors[0])
-        pp.rcParams['axes.facecolor'] = self.colors[1]
-        ax1 = fig.add_subplot(111)
-        ax1.tricontour(self.cv_score_table_.values[:,0],self.cv_score_table_.values[:,1],self.cv_score_table_.values[:,2])
-        pp.title(title)
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Jan 27 11:41:35 2019
+
+SPRM Plotting options (based on Matplotlib)
+---------------------
+
+In class sprm_plot
+    Y vs Y predicted plot with outlier flagging
+    Projection to score space with outlier flagging 
+    Barchart of regression coefficients with range selection 
+    Barchart of caseweights 
+        All of these plots work both with or without new test datya beoing s
+        supplied and have the optoion to only plot the test data.
+        
+In class sprm_plot_cv
+    3D contour for SPRM Grid Search CV results 
+        Requires an sklearn GridSearchCV object that trained an SPRM model 
+        Provides a 3d contour plot across the (eta, n_components) space
+
+Version 0.2: Ancillary functions: have been moved to ._plot_internals
+
+@author: Sven Serneels, Ponalytics. 
+"""
+
+from __future__ import absolute_import, division, print_function
+from __future__ import unicode_literals
+
+from ..sprm.sprm import sprm
+from ..utils.utils import MyException, convert_X_input, convert_y_input
+from ..cross_validation._cv_support_functions import cv_score_table
+
+import matplotlib.pyplot as pp 
+import numpy as np
+from sklearn.model_selection import GridSearchCV
+
+class sprm_plot(sprm):
+    
+    def __init__(self,res_sprm,colors,markers=['o','d','v'],*args):
+        """
+        Initialize with 
+        res_sprm, an sprm class object
+        
+        Only mandatory input is colors, a list of colors for 
+            [0] borders of pane 
+            [1] plot background
+            [2] marker fill
+            [3] diagonal line 
+            [4] marker contour, if different from fill
+            [5] marker color for new cases, if applicable
+            [6] marker color for harsh calibration outliers
+            [7] marker color for harsh prediction outliers
+        
+        Optional input markers, a list:
+            [0] marker for regular cases
+            [1] marker for moderate outliers (caseweight in (0,1))
+            [2] marker for harsh outliers (caseweight = 0)
+        
+        """
+        if not(isinstance(res_sprm,sprm)):
+            raise(MyException("Object supplied to sprmplot needs to be an sprm object"))
+        self.res_sprm = res_sprm
+        self.colors = colors
+        self.markers = markers
+
+    def plot_yyp(self,ytruev=[],Xn=[],label=[],names=[],namesv=[],title=[],legend_pos='lower right',onlyval=False):
+        """
+        plot_yyp will plot y vs y predicted for SPRM opbjects
+        Optional inputs: 
+            ytruev: array (new_cases,) of predictands
+            Xn: array (new_cases,variables) of predictors 
+            If these arguments are supplied, SPRM predictions for ytrue will be 
+                made from Xn through res_sprm.predict()
+            label: string: name of variable to be plotted. Will show in legend.
+            names: list or tuple of strings, casenames from training set
+            namesv: list or tuple of strings, casenames from test set
+            title: String containing plot title
+            legend_pos: string containing legend position
+            onlyval: boolean: only plot validation cases
+        """
+        
+        if len(label)==0:
+            label = 'none'
+        fig = pp.figure()
+        fig.set_facecolor(self.colors[0])
+        pp.rcParams['axes.facecolor'] = self.colors[1]
+        ax1 = fig.add_subplot(111)
+        if (not(onlyval)):
+            ytruec = self.res_sprm.y
+            if len(ytruec.shape) >1:
+                ytruec = np.array(ytruec).reshape(-1).astype('float64')
+            ypredc = np.array(self.res_sprm.fitted_).T.reshape(-1)
+            labelcr = label[0] + ' Training' + ' Regular'
+            labelcm = label[0] + ' Training' + ' Moderate'
+            labelch = label[0] + ' Training' + ' Harsh'
+            reg_cases = np.where(self.res_sprm.caseweights_ == 1)
+            mod_outliers = np.where((self.res_sprm.caseweights_ > 0) & (self.res_sprm.caseweights_ < 1))
+            harsh_outliers = np.where(self.res_sprm.caseweights_ == 0)
+            ax1.scatter(ytruec[reg_cases], ypredc[reg_cases], c=self.colors[2],label=labelcr, 
+                    zorder=1,edgecolors=self.colors[4],marker=self.markers[0])
+            if len(mod_outliers[0]>0):
+                ax1.scatter(ytruec[mod_outliers], ypredc[mod_outliers], c=self.colors[2],label=labelcm, 
+                    zorder=1,edgecolors=self.colors[4],marker=self.markers[1])
+            if len(harsh_outliers[0]>0):
+                ax1.scatter(ytruec[harsh_outliers], ypredc[harsh_outliers], c=self.colors[6],label=labelch, 
+                    zorder=1,edgecolors=self.colors[4],marker=self.markers[2])
+        else:
+            if (len(Xn)==0):
+                ValueError('In onlyval=True mode, new cases Xn need to be provided')
+        if not(len(Xn)==0):
+            if len(ytruev.shape) >1:
+                ytruev = np.array(ytruev).reshape(-1).astype('float64')
+            ypredv = self.res_sprm.predict(Xn)
+            ypredv = np.array(ypredv).reshape(-1).astype('float64')
+            wv = self.res_sprm.weightnewx(Xn)
+            labelvr = label[0] + ' Test' + ' Regular'
+            labelvm = label[0] + ' Test' + ' Moderate'
+            labelvh = label[0] + ' Test' + ' Harsh'
+            reg_cases = np.where(wv == 1)[0]
+            mod_outliers = np.where((wv > 0) & (wv < 1))[0]
+            harsh_outliers = np.where(wv == 0)[0]
+            if len(reg_cases>0):
+                ax1.scatter(ytruev[reg_cases],ypredv[reg_cases],c=self.colors[5],label=labelvr,
+                        zorder=1,edgecolors=self.colors[4],marker=self.markers[0])
+            if len(mod_outliers>0):
+                ax1.scatter(ytruev[mod_outliers],ypredv[mod_outliers],c=self.colors[5],label=labelvm,
+                        zorder=1,edgecolors=self.colors[4],marker=self.markers[1])
+            if len(harsh_outliers>0):
+                ax1.scatter(ytruev[harsh_outliers],ypredv[harsh_outliers],c=self.colors[7],label=labelvh,
+                        zorder=1,edgecolors=self.colors[4],marker=self.markers[2])
+        x_abline = np.array(ax1.get_xbound())
+        ax1.add_line(pp.Line2D(x_abline,x_abline,color=self.colors[3]))
+        if len(label)==0:
+            ax1.legend_.remove()
+        else:
+            pp.legend(loc=legend_pos)
+        if len(names)>0:
+            if not(onlyval):
+                for i in range(0,len(names)-1):
+                    ax1.annotate(names[i], (ytruec[i],ypredc[i]))
+        if len(namesv)>0:
+            for i in range(0,len(namesv)-1):
+                ax1.annotate(namesv[i], (ytruev[i],ypredv[i]))
+        if len(title)>0:
+            pp.title(title)
+        pp.show()
+        
+    def plot_coeffs(self,entity="coef_",truncation=0,columns=[],title=[]):
+        """
+        plot_coeffs will plot estimated model parameters, with option to 
+        truncate (useful if highly multivariate) 
+        Optional Inputs: 
+            entity, str, exact name of attribute from sprm object to be plotted
+            truncation, float in [0,1), Percentage of smallest and largest 
+                coefficients to be plotted
+            columns, array, tuple or list of str, variable names
+            title, str, plot title
+        """
+        fig = pp.figure()
+        fig.set_facecolor(self.colors[0])
+        pp.rcParams['axes.facecolor'] = self.colors[1]
+        ax1 = fig.add_subplot(111)
+        p = len(self.res_sprm.non_zero_scale_vars_)
+        if len(columns) > 0:
+            x_plot = columns[self.res_sprm.non_zero_scale_vars_]
+            x_labels = columns[self.res_sprm.non_zero_scale_vars_]
+        else:
+            x_plot = np.arange(0,p)
+       
+        b_orig = np.array(getattr(self.res_sprm,entity)).reshape(-1)
+        if truncation > 0:
+            ind_sort = np.argsort(np.argsort(b_orig))
+            b_sort = np.sort(b_orig)
+            left = np.ceil(p*(truncation/2)).astype(int)
+            right = (p-np.ceil(p*(truncation/2)).astype(int))
+            b_plot = b_sort[np.union1d(np.arange(0,left),
+                                       np.arange(right,p))]
+            x_labels = np.union1d(x_plot[ind_sort[0:left]],
+                                x_plot[ind_sort[right:p]])
+            x_plot = np.arange(0,len(x_labels))
+            ax1.bar(x_plot,b_plot,color=self.colors[2],edgecolor=self.colors[4],tick_label=x_labels)
+        else:
+            b_plot = b_orig 
+            ax1.bar(x_plot,b_plot,color=self.colors[2],edgecolor=self.colors[4])  
+        if ((truncation > 0) | len(columns) > 0): 
+            pp.xticks(x_plot, x_labels, rotation='vertical')
+            pp.margins(.4)
+        if len(title)>0:
+            pp.title(title)
+        pp.show()
+        
+        
+    def plot_projections(self,Xn=[],label=[],components = [0,1],names=[],namesv=[],title=[],legend_pos='lower right',onlyval=False):
+        
+        """
+        plot_projections will plot the score space  
+        Optional inputs: 
+            Xn: array (new_cases,variables) of predictors 
+            If supplied, SPRM projections for new cases will be 
+                made from Xn through res_sprm.transform()
+            label: string: name of variable to be plotted. Will show in legend.
+            names: list or tuple of strings, casenames from training set
+            namesv: list or tuple of strings, casenames from test set
+            title: String containing plot title
+            legend_pos: string containing legend position
+            onlyval: boolean: only plot validation cases
+        """
+        
+        if len(label)==0:
+            label = 'none'
+        fig = pp.figure()
+        fig.set_facecolor(self.colors[0])
+        pp.rcParams['axes.facecolor'] = self.colors[1]
+        ax1 = fig.add_subplot(111)
+        if (not(onlyval)):
+            Tc = np.array(self.res_sprm.x_scores_)
+            labelcr = label[0] + ' Training' + ' Regular'
+            labelcm = label[0] + ' Training' + ' Moderate'
+            labelch = label[0] + ' Training' + ' Harsh'
+            reg_cases = np.where(self.res_sprm.caseweights_ == 1)
+            mod_outliers = np.where((self.res_sprm.caseweights_ > 0) & (self.res_sprm.caseweights_ < 1))
+            harsh_outliers = np.where(self.res_sprm.caseweights_ == 0)
+            ax1.scatter(Tc[reg_cases,components[0]], Tc[reg_cases,components[1]], c=self.colors[2],label=labelcr, 
+                    zorder=1,edgecolors=self.colors[4],marker=self.markers[0])
+            if len(mod_outliers[0])>0:
+                ax1.scatter(Tc[mod_outliers,components[0]], Tc[mod_outliers,components[1]], c=self.colors[2],label=labelcm, 
+                    zorder=1,edgecolors=self.colors[4],marker=self.markers[1])
+            if len(harsh_outliers[0])>0:
+                ax1.scatter(Tc[harsh_outliers,components[0]], Tc[harsh_outliers,components[1]], c=self.colors[6],label=labelch, 
+                    zorder=1,edgecolors=self.colors[4],marker=self.markers[2])
+        else:
+            if (len(Xn)==0):
+                ValueError('In onlyval=True mode, new cases Xn need to be provided')
+        if not(len(Xn)==0):
+            Tv = np.array(self.res_sprm.transform(Xn))
+            labelvr = label[0] + ' Test' + ' Regular'
+            labelvm = label[0] + ' Test' + ' Moderate'
+            labelvh = label[0] + ' Test' + ' Harsh'
+            wv = self.res_sprm.weightnewx(Xn)
+            reg_cases = np.where(wv == 1)
+            mod_outliers = np.where((wv > 0) & (wv < 1))
+            harsh_outliers = np.where(wv == 0)
+            if len(reg_cases[0]>0):
+                ax1.scatter(Tv[reg_cases,components[0]], Tv[reg_cases,components[1]],c=self.colors[5],label=labelvr,
+                        zorder=1,edgecolors=self.colors[4],marker=self.markers[0])
+            if len(mod_outliers[0])>0:
+                ax1.scatter(Tv[mod_outliers,components[0]], Tv[mod_outliers,components[1]],c=self.colors[5],label=labelvm,
+                        zorder=1,edgecolors=self.colors[4],marker=self.markers[1])
+            if len(harsh_outliers[0])>0:
+                ax1.scatter(Tv[harsh_outliers,components[0]], Tv[harsh_outliers,components[1]],c=self.colors[7],label=labelvh,
+                        zorder=1,edgecolors=self.colors[4],marker=self.markers[2])
+        if len(label)==0:
+            ax1.legend_.remove()
+        else:
+            pp.legend(loc=legend_pos)
+        if len(names)>0:
+            if not(onlyval):
+                for i in range(0,len(names)-1):
+                    ax1.annotate(names[i], (Tc[i,components[0]], Tc[i,components[1]]))
+        if len(namesv)>0:
+            for i in range(0,len(namesv)-1):
+                ax1.annotate(namesv[i], (Tv[i,components[0]], Tv[i,components[1]]))
+        if len(title)>0:
+            pp.title(title)
+        pp.show()
+        
+    def plot_caseweights(self,Xn=[],label=[],names=[],namesv=[],title=[],legend_pos='lower right',onlyval=False,mode='overall'):
+        
+        """
+        plot_caseweights will plot caseweights
+        Optional inputs: 
+            Xn: array (new_cases,variables) of predictors 
+            If supplied, SPRM projections for new cases will be 
+                made from Xn through res_sprm.weightnewx()
+            label: string: name of variable to be plotted. Will show in legend.
+            names: list or tuple of strings, casenames from training set
+            namesv: list or tuple of strings, casenames from test set
+            title: String containing plot title
+            legend_pos: string containing legend position
+            onlyval: boolean: only plot validation cases
+            mode: str, which weights to plot for cases from training set, 
+                - 'overall': combined caseweights
+                - 'x': predictor block 
+                - 'y': predictand block
+                Since for validation cases y is unknown, 'x' caseweights are 
+                plotted by default there. 
+        """
+        
+        if len(label)==0:
+            label = 'none'
+        fig = pp.figure()
+        fig.set_facecolor(self.colors[0])
+        pp.rcParams['axes.facecolor'] = self.colors[1]
+        ax1 = fig.add_subplot(111)
+        if (not(onlyval)):
+            if mode=='overall':
+                wc = self.res_sprm.caseweights_
+            elif mode == 'x':
+                wc = self.res_sprm.x_caseweights_
+            elif mode == 'y':
+                wc = self.res_sprm.y_caseweights_
+            else:
+                ValueError('Options for mode are overall, x or y')
+            labelc = label[0] + ' Training' 
+        else:
+            wc = []
+            labelc=[]
+            if (len(Xn)==0):
+                ValueError('In onlyval=True mode, new cases Xn need to be provided')
+        if not(len(Xn)==0):
+            wv = self.res_sprm.weightnewx(Xn)
+            labelv = label[0] + ' Test'
+        else:
+            wv=[]
+            labelv = []
+        name_indices = np.array(range(1,len(wc)+len(wv)+1)) 
+        if (len(wc)>0):
+            ax1.bar(name_indices,np.concatenate((wc,np.repeat(np.nan,len(wv)))),color=self.colors[2],label=labelc)
+        if (len(wv)>0):
+            ax1.bar(name_indices,np.concatenate((np.repeat(np.nan,len(wc)),wv)),color=self.colors[5],label=labelv)
+        if len(label)==0:
+            ax1.legend_.remove()
+        else:
+            pp.legend(loc=legend_pos)
+        if len(names)>0:
+            if not(onlyval):
+                for i in range(0,len(names)-1):
+                    ax1.annotate(names[i], (name_indices[i+1],wc[i]))
+        if len(namesv)>0:
+            for i in range(0,len(namesv)-1):
+                ax1.annotate(namesv[i], (name_indices[len(wc)+i],wv[i]))
+        if len(title)>0:
+            pp.title(title)
+        pp.show()
+        
+
+
+class sprm_plot_cv(GridSearchCV,sprm):
+    
+    def __init__(self,res_sprm_cv,colors,*args):
+        
+        """
+        Initialize with 
+        res_sprm_cv, an GridSearchCV cross-validated sprm object 
+        
+        Only mandatory input is colors, a list of colors for 
+            [0] borders of pane 
+            [1] plot background
+            [2] marker fill
+            [3] diagonal line 
+            [4] marker contour, if different from fill
+            [5] marker color for new cases, if applicable
+            [6] marker color for harsh calibration outliers
+            [7] marker color for harsh prediction outliers
+        
+        """
+        
+        self.res_sprm_cv = res_sprm_cv
+        self.colors = colors
+        
+    def eta_ncomp_contour(self,title='SPRM Cross-Validation Contour Plot'):
+        
+        """
+        Function to draw contour plot from cross-valation results. 
+        Optional Input: 
+        title, str. Plot title. 
+        
+        """
+        
+        if not(hasattr(self,'cv_score_table_')):
+            cv_score_table_ = cv_score_table(self.res_sprm_cv) 
+            setattr(self,'cv_score_table_',cv_score_table_)
+        fig = pp.figure()
+        fig.set_facecolor(self.colors[0])
+        pp.rcParams['axes.facecolor'] = self.colors[1]
+        ax1 = fig.add_subplot(111)
+        ax1.tricontour(self.cv_score_table_.values[:,0],self.cv_score_table_.values[:,1],self.cv_score_table_.values[:,2])
+        pp.title(title)
         pp.show()
```

## direpack/ppdire/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Wed Jul 9 14:20:17 2019
-
-@author: Sven Serneels, Ponalytics
-"""
-
-__name__ = "ppdire"
-__author__ = "Sven Serneels"
-__license__ = "MIT"
-__version__ = "0.2.12"
-__date__ = "2022-10-22"
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Wed Jul 9 14:20:17 2019
+
+@author: Sven Serneels, Ponalytics
+"""
+
+__name__ = "ppdire"
+__author__ = "Sven Serneels"
+__license__ = "MIT"
+__version__ = "0.2.12"
+__date__ = "2022-10-22"
```

## direpack/ppdire/_ppdire_utils.py

 * *Ordering differences only*

```diff
@@ -1,188 +1,188 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Thu Jan 2 2020
-
-@author: Sven Serneels, Ponalytics. 
-"""
-
-import numpy as np
-import pandas as ps
-
-
-def pp_objective(x,est,X,opt_args):
-    
-    """
-    Optimization objective for ppdire 
-    
-    """
-    
-    n = len(x)
-    x = np.array(x).reshape((n,1))
-    return(-est.fit(np.matmul(X,x),**opt_args))
-
-def gridplane(X,most,pi_arguments={},**kwargs):
-
-    """
-    Function for grid search in a plane in two dimensions
-    
-    Required: X, np.array(n,2), data, 
-              most, class object, projection index. Designed for 
-                  dicomo or capi classes.
-    Optional: pi_arguments, dict: arguments to pass on to projection index, 
-                    plus a few local arguments such as optrange and square_pi
-                    (see ppdire for explanation)
-        
-              y, np.array(n,1), second block of data 
-              biascorr, to apply bias correction at normal distribution 
-              alphamat, np.array: matrix of alpha angles to be scanned. 
-        
-        
-    Values: 
-        wi, np.array(p,1): optimal direction 
-        maximo, float: optimal value of projection index
-        
-    Note: this function is written exclusively to be called from within the ppdire class
-    
-    """
-    
-            
-    if (('biascorr' not in kwargs) and ('biascorr' not in pi_arguments)):
-        biascorr = False
-    else:
-        biascorr = kwargs.get('biascorr')
-    
-    if len(pi_arguments) == 0:
-        
-        pi_arguments = {
-                        'alpha': 0,
-                        'ndir': 1000,
-                        'trimming': 0,
-                        'biascorr': biascorr, 
-                        'dmetric' : 'euclidean',
-                        'alphamat': None,
-                        'optrange': (-1,1),
-                        'square_pi': False
-                        }
-        
-    if ('y' in kwargs):
-        y = kwargs.pop('y')
-        pi_arguments['y'] = y
-        
-    optrange = pi_arguments['optrange']
-    optmax = optrange[1]
-    
-    alphamat = kwargs.pop('alphamat',pi_arguments['alphamat'])
-    if (alphamat != None):
-        optrange = np.sign(optrange)
-        stop0s = np.arcsin(optrange[0])
-        stop1s = np.arcsin(optrange[1])
-        stop1c = np.arccos(optrange[0])
-        stop0c = np.arccos(optrange[1])
-        anglestart = max(stop0c,stop0s)
-        anglestop = max(stop1c,stop1s)
-        nangle = np.linspace(anglestart,anglestop,pi_arguments['ndir'],endpoint=False)            
-        alphamat = np.array([np.cos(nangle), np.sin(nangle)])
-        if optmax != 1:
-            alphamat *= optmax
-    
-    tj = np.matmul(X,alphamat)
-    if pi_arguments['square_pi']:
-        meas = [most.fit(tj[:,i],**pi_arguments)**2 
-        for i in np.arange(0,pi_arguments['ndir'])]
-    else:
-        meas = [most.fit(tj[:,i],**pi_arguments) 
-        for i in np.arange(0,pi_arguments['ndir'])]
-        
-    maximo = np.max(meas)
-    indmax = np.where(meas == maximo)[0]
-    if len(indmax)>0:
-        indmax = indmax[0]
-    wi = np.array(alphamat[:,indmax]).reshape((2,1))
-    
-    return(wi,maximo)
-    
-    
-
-def gridplane_2(X,most,q,div,pi_arguments={},**kwargs):
-
-    """
-    Function for refining a grid search in a plane in two dimensions
-    
-    Required: X, np.array(n,2), data
-              most, class object, projection index. Designed for 
-                  dicomo or capi classes.
-              q, np.array(1,1), last obtained suboptimal direction component
-              div, float, number of subsegments to divide angle into
-    
-    Optional: pi_arguments, dict: arguments to pass on to projection index, 
-                    plus a few local arguments such as optrange and square_pi
-                    (see ppdire for explanation)
-        
-              y, np.array(n,1), second block of data 
-              biascorr, to apply bias correction at normal distribution 
-              alphamat, np.array: matrix of alpha angles to be scanned. 
-        
-    pi_arguments is a dict of arguments passed on to the projection index
-        
-    Values: 
-        wi, np.array(p,1): optimal direction 
-        maximo, float: optimal value of projection index
-        
-    Note: this function is written to be called from within the ppdire class
-    
-    """
-            
-    if (('biascorr' not in kwargs) and ('biascorr' not in pi_arguments)):
-        biascorr = False
-    else:
-        biascorr = kwargs.get('biascorr')
-        
-    if len(pi_arguments) == 0:
-        
-        pi_arguments = {
-                        'alpha': 0,
-                        'ndir': 1000,
-                        'trimming': 0,
-                        'biascorr': biascorr, 
-                        'dmetric' : 'euclidean',
-                        'alphamat': None,
-                        'optrange': (-1,1),
-                        'square_pi': False
-                        }
-
-        
-    if 'y' in kwargs:
-        y = kwargs.pop('y')
-        pi_arguments['y'] = y
-
-    optrange = pi_arguments['optrange']
-    optmax = optrange[1]
-   
-    alphamat = kwargs.pop('alphamat',pi_arguments['alphamat'])
-    if (alphamat != None).any():
-        anglestart = min(pi_arguments['_stop0c'],pi_arguments['_stop0s'])
-        anglestop = min(pi_arguments['_stop1c'],pi_arguments['_stop1s'])
-        nangle = np.linspace(anglestart,anglestop,pi_arguments['ndir'],endpoint=True)
-        alphamat = np.array([np.cos(nangle), np.sin(nangle)])
-        if optmax != 1:
-            alphamat *= optmax
-    alpha1 = alphamat
-    divisor = np.sqrt(1 + 2*np.multiply(alphamat[0,:].reshape(1,-1),alphamat[1,:].reshape(1,-1))*q[0])
-    alpha1 = np.divide(alphamat,np.repeat(divisor,2,0))
-    tj = np.dot(X,alpha1)
-    
-    if pi_arguments['square_pi']:
-        meas = [most.fit(tj[:,i],**pi_arguments)**2 
-        for i in np.arange(0,pi_arguments['ndir'])]
-    else:
-        meas = [most.fit(tj[:,i],**pi_arguments) 
-        for i in np.arange(0,pi_arguments['ndir'])]
-
-    maximo = np.max(meas)
-    indmax = np.where(meas == maximo)[0]
-    if len(indmax)>0:
-        indmax = indmax[0]
-    wi = np.array(alpha1[:,indmax]).reshape((2,1))
-    
-    return(wi,maximo)
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Thu Jan 2 2020
+
+@author: Sven Serneels, Ponalytics. 
+"""
+
+import numpy as np
+import pandas as ps
+
+
+def pp_objective(x,est,X,opt_args):
+    
+    """
+    Optimization objective for ppdire 
+    
+    """
+    
+    n = len(x)
+    x = np.array(x).reshape((n,1))
+    return(-est.fit(np.matmul(X,x),**opt_args))
+
+def gridplane(X,most,pi_arguments={},**kwargs):
+
+    """
+    Function for grid search in a plane in two dimensions
+    
+    Required: X, np.array(n,2), data, 
+              most, class object, projection index. Designed for 
+                  dicomo or capi classes.
+    Optional: pi_arguments, dict: arguments to pass on to projection index, 
+                    plus a few local arguments such as optrange and square_pi
+                    (see ppdire for explanation)
+        
+              y, np.array(n,1), second block of data 
+              biascorr, to apply bias correction at normal distribution 
+              alphamat, np.array: matrix of alpha angles to be scanned. 
+        
+        
+    Values: 
+        wi, np.array(p,1): optimal direction 
+        maximo, float: optimal value of projection index
+        
+    Note: this function is written exclusively to be called from within the ppdire class
+    
+    """
+    
+            
+    if (('biascorr' not in kwargs) and ('biascorr' not in pi_arguments)):
+        biascorr = False
+    else:
+        biascorr = kwargs.get('biascorr')
+    
+    if len(pi_arguments) == 0:
+        
+        pi_arguments = {
+                        'alpha': 0,
+                        'ndir': 1000,
+                        'trimming': 0,
+                        'biascorr': biascorr, 
+                        'dmetric' : 'euclidean',
+                        'alphamat': None,
+                        'optrange': (-1,1),
+                        'square_pi': False
+                        }
+        
+    if ('y' in kwargs):
+        y = kwargs.pop('y')
+        pi_arguments['y'] = y
+        
+    optrange = pi_arguments['optrange']
+    optmax = optrange[1]
+    
+    alphamat = kwargs.pop('alphamat',pi_arguments['alphamat'])
+    if (alphamat != None):
+        optrange = np.sign(optrange)
+        stop0s = np.arcsin(optrange[0])
+        stop1s = np.arcsin(optrange[1])
+        stop1c = np.arccos(optrange[0])
+        stop0c = np.arccos(optrange[1])
+        anglestart = max(stop0c,stop0s)
+        anglestop = max(stop1c,stop1s)
+        nangle = np.linspace(anglestart,anglestop,pi_arguments['ndir'],endpoint=False)            
+        alphamat = np.array([np.cos(nangle), np.sin(nangle)])
+        if optmax != 1:
+            alphamat *= optmax
+    
+    tj = np.matmul(X,alphamat)
+    if pi_arguments['square_pi']:
+        meas = [most.fit(tj[:,i],**pi_arguments)**2 
+        for i in np.arange(0,pi_arguments['ndir'])]
+    else:
+        meas = [most.fit(tj[:,i],**pi_arguments) 
+        for i in np.arange(0,pi_arguments['ndir'])]
+        
+    maximo = np.max(meas)
+    indmax = np.where(meas == maximo)[0]
+    if len(indmax)>0:
+        indmax = indmax[0]
+    wi = np.array(alphamat[:,indmax]).reshape((2,1))
+    
+    return(wi,maximo)
+    
+    
+
+def gridplane_2(X,most,q,div,pi_arguments={},**kwargs):
+
+    """
+    Function for refining a grid search in a plane in two dimensions
+    
+    Required: X, np.array(n,2), data
+              most, class object, projection index. Designed for 
+                  dicomo or capi classes.
+              q, np.array(1,1), last obtained suboptimal direction component
+              div, float, number of subsegments to divide angle into
+    
+    Optional: pi_arguments, dict: arguments to pass on to projection index, 
+                    plus a few local arguments such as optrange and square_pi
+                    (see ppdire for explanation)
+        
+              y, np.array(n,1), second block of data 
+              biascorr, to apply bias correction at normal distribution 
+              alphamat, np.array: matrix of alpha angles to be scanned. 
+        
+    pi_arguments is a dict of arguments passed on to the projection index
+        
+    Values: 
+        wi, np.array(p,1): optimal direction 
+        maximo, float: optimal value of projection index
+        
+    Note: this function is written to be called from within the ppdire class
+    
+    """
+            
+    if (('biascorr' not in kwargs) and ('biascorr' not in pi_arguments)):
+        biascorr = False
+    else:
+        biascorr = kwargs.get('biascorr')
+        
+    if len(pi_arguments) == 0:
+        
+        pi_arguments = {
+                        'alpha': 0,
+                        'ndir': 1000,
+                        'trimming': 0,
+                        'biascorr': biascorr, 
+                        'dmetric' : 'euclidean',
+                        'alphamat': None,
+                        'optrange': (-1,1),
+                        'square_pi': False
+                        }
+
+        
+    if 'y' in kwargs:
+        y = kwargs.pop('y')
+        pi_arguments['y'] = y
+
+    optrange = pi_arguments['optrange']
+    optmax = optrange[1]
+   
+    alphamat = kwargs.pop('alphamat',pi_arguments['alphamat'])
+    if (alphamat != None).any():
+        anglestart = min(pi_arguments['_stop0c'],pi_arguments['_stop0s'])
+        anglestop = min(pi_arguments['_stop1c'],pi_arguments['_stop1s'])
+        nangle = np.linspace(anglestart,anglestop,pi_arguments['ndir'],endpoint=True)
+        alphamat = np.array([np.cos(nangle), np.sin(nangle)])
+        if optmax != 1:
+            alphamat *= optmax
+    alpha1 = alphamat
+    divisor = np.sqrt(1 + 2*np.multiply(alphamat[0,:].reshape(1,-1),alphamat[1,:].reshape(1,-1))*q[0])
+    alpha1 = np.divide(alphamat,np.repeat(divisor,2,0))
+    tj = np.dot(X,alpha1)
+    
+    if pi_arguments['square_pi']:
+        meas = [most.fit(tj[:,i],**pi_arguments)**2 
+        for i in np.arange(0,pi_arguments['ndir'])]
+    else:
+        meas = [most.fit(tj[:,i],**pi_arguments) 
+        for i in np.arange(0,pi_arguments['ndir'])]
+
+    maximo = np.max(meas)
+    indmax = np.where(meas == maximo)[0]
+    if len(indmax)>0:
+        indmax = indmax[0]
+    wi = np.array(alpha1[:,indmax]).reshape((2,1))
+    
+    return(wi,maximo)
```

## direpack/ppdire/capi.py

 * *Ordering differences only*

```diff
@@ -1,195 +1,195 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun May 12 10:03:05 2019
-
-@author: Sven Serneels, Ponalytics.
-"""
-
-from sklearn.base import BaseEstimator, defaultdict
-from sklearn.utils.metaestimators import _BaseComposition
-from collections import defaultdict
-import inspect
-from ..dicomo.dicomo import dicomo
-from ..dicomo._dicomo_utils import *
-
-
-class capi(_BaseComposition, BaseEstimator):
-
-    """
-    CAPI Co-moment analysis projection index
-    
-    The CAPI projection index to estimate generalized betas was first introduced
-    in: 
-    
-    S. Serneels, Projection pursuit based generalized betas accounting for 
-    higher order co-moment effects in financial market analysis,  in: 
-    JSM Proceedings, Business and Economic Statistics Section. 
-    Alexandria, VA: American Statistical Association, 2019, 3009-3035.
-    
-    Class arguments 
-    
-    max_degree, int: maxmimal degree of co-moments to be used. In [2,3,4]. 
-    
-    projection_index, class object: class used to calculate co-moments. 
-        Written to work with dicomo class yet other plugins could be written.
-    
-    pi_arguments, dict: dict of arguments to pass on to projection_index
-    
-    weights, list of float: weights to used in linear combination of co-moments. 
-    
-    centring, bool
-    
-    scaling, bool whether to calculate CAPI based on scaled higher co-moments 
-        (co-skewness, co-kurtosis) or raw higher co-moments
-    
-    options, either a list of co-moment options to be included, or 'all' (e.g.
-    option=i calculates M3,i and M4,i etc.)
-    
-    After intializing the object, call object.fit(x,y,**kwargs) to evaluate. 
-    CAPI takes no direct kwargs, yet passes all kwargs on to the fit method of
-    the projection index. 
-    
-    """
-
-    def __init__(
-        self,
-        max_degree=2,
-        projection_index=dicomo,
-        pi_arguments={},
-        weights=[1, 1, 1, -1, -1, -1],
-        centring=False,
-        scaling=True,
-        options="all",
-    ):
-        self.max_degree = max_degree
-        self.projection_index = projection_index
-        self.pi_arguments = pi_arguments
-        self.weights = weights
-        self.most = self.projection_index(**self.pi_arguments)
-        self.scaling = scaling
-        self.options = options
-        self.capi_index_ = None
-        if self.max_degree > 4:
-            raise (ValueError("Maximal degree is 4."))
-
-    def fit(self, x, y, **kwargs):
-
-        if self.scaling:
-            order_kwargs = ["cov", "cos", "cok"]
-        else:
-            order_kwargs = ["com", "com", "com"]
-
-        if self.max_degree < 2:
-            raise (ValueError("capi not meaningful for max_degree < 2"))
-        if self.options == "all":
-            options = np.arange(1, 4)
-        else:
-            options = np.array(self.options, ndmin=1)
-        moments = np.zeros(6)
-        fit_arguments = {"order": 0, "y": y}
-        fit_arguments = {**kwargs, **fit_arguments}
-        init_moment_calc = 2
-        k = 0
-        for i in range(init_moment_calc, self.max_degree + 1):
-            fit_arguments["order"] = i
-            self.most.set_params(mode=order_kwargs[i - 2])
-            l = min(i - 1, len(options))
-            for j in options[np.arange(0, l)]:
-                fit_arguments["option"] = j
-                moments[i - 3 + j + k] = self.most.fit(x, **fit_arguments)
-            if i == 3:
-                k += 1
-        capi_index_ = np.dot(self.weights, moments)
-        self.capi_index_ = capi_index_
-        self.moments_ = moments
-        return capi_index_
-
-    @classmethod
-    def _get_param_names(cls):
-        """Get parameter names for the estimator"""
-        # fetch the constructor or the original constructor before
-        # deprecation wrapping if any
-        init = getattr(cls.__init__, "deprecated_original", cls.__init__)
-        if init is object.__init__:
-            # No explicit constructor to introspect
-            return []
-
-        # introspect the constructor arguments to find the model parameters
-        # to represent
-        init_signature = inspect.signature(init)
-        # Consider the constructor parameters excluding 'self'
-        parameters = [
-            p
-            for p in init_signature.parameters.values()
-            if p.name != "self" and p.kind != p.VAR_KEYWORD
-        ]
-        for p in parameters:
-            if p.kind == p.VAR_POSITIONAL:
-                raise RuntimeError(
-                    "scikit-learn estimators should always "
-                    "specify their parameters in the signature"
-                    " of their __init__ (no varargs)."
-                    " %s with constructor %s doesn't "
-                    " follow this convention." % (cls, init_signature)
-                )
-        # Extract and sort argument names excluding 'self'
-        return sorted([p.name for p in parameters])
-
-    def get_params(self, deep=False):
-        """Get parameters for this estimator.
-        Parameters
-        ----------
-        deep : boolean, optional
-            If True, will return the parameters for this estimator and
-            contained subobjects that are estimators.
-        Returns
-        -------
-        params : mapping of string to any
-            Parameter names mapped to their values.
-        ------
-        Copied from ScikitLlearn instead of imported to avoid 'deep=True'
-        """
-        out = dict()
-        for key in self._get_param_names():
-            value = getattr(self, key, None)
-            if deep and hasattr(value, "get_params"):
-                deep_items = value.get_params().items()
-                out.update((key + "__" + k, val) for k, val in deep_items)
-            out[key] = value
-        return out
-
-    def set_params(self, **params):
-        """Set the parameters of this estimator.
-        Copied from ScikitLearn, adapted to avoid calling 'deep=True'
-        Returns
-        -------
-        self
-        ------
-        Copied from ScikitLlearn instead of imported to avoid 'deep=True'
-        """
-        if not params:
-            # Simple optimization to gain speed (inspect is slow)
-            return self
-        valid_params = self.get_params()
-
-        nested_params = defaultdict(dict)  # grouped by prefix
-        for key, value in params.items():
-            key, delim, sub_key = key.partition("__")
-            if key not in valid_params:
-                raise ValueError(
-                    "Invalid parameter %s for estimator %s. "
-                    "Check the list of available parameters "
-                    "with `estimator.get_params().keys()`." % (key, self)
-                )
-
-            if delim:
-                nested_params[key][sub_key] = value
-            else:
-                setattr(self, key, value)
-                valid_params[key] = value
-
-        for key, sub_params in nested_params.items():
-            valid_params[key].set_params(**sub_params)
-
-        return self
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun May 12 10:03:05 2019
+
+@author: Sven Serneels, Ponalytics.
+"""
+
+from sklearn.base import BaseEstimator, defaultdict
+from sklearn.utils.metaestimators import _BaseComposition
+from collections import defaultdict
+import inspect
+from ..dicomo.dicomo import dicomo
+from ..dicomo._dicomo_utils import *
+
+
+class capi(_BaseComposition, BaseEstimator):
+
+    """
+    CAPI Co-moment analysis projection index
+    
+    The CAPI projection index to estimate generalized betas was first introduced
+    in: 
+    
+    S. Serneels, Projection pursuit based generalized betas accounting for 
+    higher order co-moment effects in financial market analysis,  in: 
+    JSM Proceedings, Business and Economic Statistics Section. 
+    Alexandria, VA: American Statistical Association, 2019, 3009-3035.
+    
+    Class arguments 
+    
+    max_degree, int: maxmimal degree of co-moments to be used. In [2,3,4]. 
+    
+    projection_index, class object: class used to calculate co-moments. 
+        Written to work with dicomo class yet other plugins could be written.
+    
+    pi_arguments, dict: dict of arguments to pass on to projection_index
+    
+    weights, list of float: weights to used in linear combination of co-moments. 
+    
+    centring, bool
+    
+    scaling, bool whether to calculate CAPI based on scaled higher co-moments 
+        (co-skewness, co-kurtosis) or raw higher co-moments
+    
+    options, either a list of co-moment options to be included, or 'all' (e.g.
+    option=i calculates M3,i and M4,i etc.)
+    
+    After intializing the object, call object.fit(x,y,**kwargs) to evaluate. 
+    CAPI takes no direct kwargs, yet passes all kwargs on to the fit method of
+    the projection index. 
+    
+    """
+
+    def __init__(
+        self,
+        max_degree=2,
+        projection_index=dicomo,
+        pi_arguments={},
+        weights=[1, 1, 1, -1, -1, -1],
+        centring=False,
+        scaling=True,
+        options="all",
+    ):
+        self.max_degree = max_degree
+        self.projection_index = projection_index
+        self.pi_arguments = pi_arguments
+        self.weights = weights
+        self.most = self.projection_index(**self.pi_arguments)
+        self.scaling = scaling
+        self.options = options
+        self.capi_index_ = None
+        if self.max_degree > 4:
+            raise (ValueError("Maximal degree is 4."))
+
+    def fit(self, x, y, **kwargs):
+
+        if self.scaling:
+            order_kwargs = ["cov", "cos", "cok"]
+        else:
+            order_kwargs = ["com", "com", "com"]
+
+        if self.max_degree < 2:
+            raise (ValueError("capi not meaningful for max_degree < 2"))
+        if self.options == "all":
+            options = np.arange(1, 4)
+        else:
+            options = np.array(self.options, ndmin=1)
+        moments = np.zeros(6)
+        fit_arguments = {"order": 0, "y": y}
+        fit_arguments = {**kwargs, **fit_arguments}
+        init_moment_calc = 2
+        k = 0
+        for i in range(init_moment_calc, self.max_degree + 1):
+            fit_arguments["order"] = i
+            self.most.set_params(mode=order_kwargs[i - 2])
+            l = min(i - 1, len(options))
+            for j in options[np.arange(0, l)]:
+                fit_arguments["option"] = j
+                moments[i - 3 + j + k] = self.most.fit(x, **fit_arguments)
+            if i == 3:
+                k += 1
+        capi_index_ = np.dot(self.weights, moments)
+        self.capi_index_ = capi_index_
+        self.moments_ = moments
+        return capi_index_
+
+    @classmethod
+    def _get_param_names(cls):
+        """Get parameter names for the estimator"""
+        # fetch the constructor or the original constructor before
+        # deprecation wrapping if any
+        init = getattr(cls.__init__, "deprecated_original", cls.__init__)
+        if init is object.__init__:
+            # No explicit constructor to introspect
+            return []
+
+        # introspect the constructor arguments to find the model parameters
+        # to represent
+        init_signature = inspect.signature(init)
+        # Consider the constructor parameters excluding 'self'
+        parameters = [
+            p
+            for p in init_signature.parameters.values()
+            if p.name != "self" and p.kind != p.VAR_KEYWORD
+        ]
+        for p in parameters:
+            if p.kind == p.VAR_POSITIONAL:
+                raise RuntimeError(
+                    "scikit-learn estimators should always "
+                    "specify their parameters in the signature"
+                    " of their __init__ (no varargs)."
+                    " %s with constructor %s doesn't "
+                    " follow this convention." % (cls, init_signature)
+                )
+        # Extract and sort argument names excluding 'self'
+        return sorted([p.name for p in parameters])
+
+    def get_params(self, deep=False):
+        """Get parameters for this estimator.
+        Parameters
+        ----------
+        deep : boolean, optional
+            If True, will return the parameters for this estimator and
+            contained subobjects that are estimators.
+        Returns
+        -------
+        params : mapping of string to any
+            Parameter names mapped to their values.
+        ------
+        Copied from ScikitLlearn instead of imported to avoid 'deep=True'
+        """
+        out = dict()
+        for key in self._get_param_names():
+            value = getattr(self, key, None)
+            if deep and hasattr(value, "get_params"):
+                deep_items = value.get_params().items()
+                out.update((key + "__" + k, val) for k, val in deep_items)
+            out[key] = value
+        return out
+
+    def set_params(self, **params):
+        """Set the parameters of this estimator.
+        Copied from ScikitLearn, adapted to avoid calling 'deep=True'
+        Returns
+        -------
+        self
+        ------
+        Copied from ScikitLlearn instead of imported to avoid 'deep=True'
+        """
+        if not params:
+            # Simple optimization to gain speed (inspect is slow)
+            return self
+        valid_params = self.get_params()
+
+        nested_params = defaultdict(dict)  # grouped by prefix
+        for key, value in params.items():
+            key, delim, sub_key = key.partition("__")
+            if key not in valid_params:
+                raise ValueError(
+                    "Invalid parameter %s for estimator %s. "
+                    "Check the list of available parameters "
+                    "with `estimator.get_params().keys()`." % (key, self)
+                )
+
+            if delim:
+                nested_params[key][sub_key] = value
+            else:
+                setattr(self, key, value)
+                valid_params[key] = value
+
+        for key, sub_params in nested_params.items():
+            valid_params[key].set_params(**sub_params)
+
+        return self
```

## direpack/ppdire/ppdire.py

 * *Ordering differences only*

```diff
@@ -1,892 +1,892 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Created on Sun Dec 30 12:02:12 2018
-
-# ppdire - Projection pursuit dimension reduction
-
-# @author: Sven Serneels (Ponalytics)
-
-
-
-#from .dicomo import dicomo
-import numpy as np
-from statsmodels.regression.quantile_regression import QuantReg
-import statsmodels.robust as srs
-import scipy.stats as sps
-from scipy.linalg import pinv
-from scipy.optimize import minimize, differential_evolution, NonlinearConstraint
-import copy
-from sklearn.utils.metaestimators import _BaseComposition
-from sklearn.base import RegressorMixin,BaseEstimator,TransformerMixin, defaultdict
-from sklearn.utils.extmath import svd_flip
-from ..sprm.rm import rm 
-from ..preprocessing.robcent import VersatileScaler
-import warnings
-from ..dicomo.dicomo import dicomo 
-from ..dicomo._dicomo_utils import * 
-from .capi import capi
-from ._ppdire_utils import *
-from ..preprocessing._preproc_utilities import scale_data
-from ..utils.utils import MyException, convert_X_input, convert_y_input
-import inspect
-from ..ipopt_temp.ipopt_wrapper import minimize_ipopt
-from ..ipopt_temp.jacobian import (
-    FunctionWithApproxJacobianCentral,
-    FunctionWithApproxJacobian,
-)
-
-class ppdire(_BaseComposition,BaseEstimator,TransformerMixin,RegressorMixin):
-    
-    """
-    PPDIRE Projection Pursuit Dimension Reduction
-    
-    The class allows for calculation of the projection pursuit optimization
-    either through `scipy.optimize` or through the grid algorithm, native to this
-    package. The class provides a very flexible way to access optimization of 
-    projection indices that can lead to either classical or robust dimension
-    reduction. Optimization through scipy.optimize is much more efficient, yet 
-    it will only provide correct results for classical projection indices. The
-    native grid algorithm should be used when the projection index involves 
-    order statistics of any kind, such as ranks, trimming, winsorizing, or 
-    empirical quantiles. The grid optimization algorithm for projection pursuit 
-    implemented here, was outlined in: 
-        
-        Filzmoser, P., Serneels, S., Croux, C. and Van Espen, P.J., 
-        Robust multivariate methods: The projection pursuit approach,
-        in: From Data and Information Analysis to Knowledge Engineering,
-        Spiliopoulou, M., Kruse, R., Borgelt, C., Nuernberger, A. and Gaul, W., eds., 
-        Springer Verlag, Berlin, Germany,
-        2006, pages 270--277.
-        
-    Parameters
-    ------------ 
-
-        projection_index : function or class. 
-                            dicomo and capi supplied in this package can both be used, 
-                            but user defined projection indices can be processed 
-                                    
-            
-        pi_arguments : dict 
-                        arguments to be passed on to projection index 
-
-        n_components : int 
-                        number of components to estimate
-
-        trimming : float
-                     trimming percentage to be entered as pct/100 
-
-        alpha : float.
-                 Continuum coefficient. Only relevant if ppdire is used to estimate 
-                 (classical or robust) continuum regression 
-
-        optimizer : str.
-                    Presently: either 'grid' (native optimizer), any of the 
-                    options in scipy-optimize (e.g. 'SLSQP'), 
-                    'IPOPT', or 'differential_evolution'
-
-        optimizer_options : dict 
-                            with options to pass on to the optimizer 
-                            If optimizer == 'grid',
-                            ndir: int: Number of directions to calculate per iteration.
-                            maxiter: int. Maximal number of iterations.
-
-        optimizer_constraints : dict or list of dicts, 
-                                further constraints to be passed on to the 
-                                optimizer function.
-
-        regopt : str. 
-                regression option for regression step y~T. Can be set to 
-                'OLS' (default), 'robust' (will run sprm.rm) or 
-                'quantile' (statsmodels.regression.quantreg). 
-
-        center : str, 
-                how to center the data. options accepted are options from sprm.preprocessing 
-
-        center_data : bool 
-
-        scale_data : bool. 
-                    Note: if set to False, convergence to correct optimum  is 
-                    not a given. Will throw a warning. 
-
-        whiten_data : bool. 
-                    Typically used for ICA (kurtosis as PI)
-
-        square_pi : bool. 
-                    Whether to square the projection index upon evaluation.
-
-        compression : bool. 
-                        Use internal data compresion step for flat data. 
-
-        copy : bool. 
-                Whether to make a deep copy of the input data or not. 
-
-        verbose : bool. 
-                    Set to True prints the iteration number. 
-
-        return_scaling_object : bool.
-                                If True, the rescaling object will be returned. 
-
-    Attributes
-    ------------ 
-    Attributes always provided 
-        -  `x_weights_`: X block PPDIRE weighting vectors (usually denoted W)
-        -  `x_loadings_`: X block PPDIRE loading vectors (usually denoted P)
-        -  `x_scores_`: X block PPDIRE score vectors (usually denoted T)
-        -  `x_ev_`: X block explained variance per component
-        -  `x_Rweights_`: X block SIMPLS style weighting vectors (usually denoted R)
-        -  `x_loc_`: X block location estimate 
-        -  `x_sca_`: X block scale estimate
-        -  `crit_values_`: vector of evaluated values for the optimization objective. 
-        -  `Maxobjf_`: vector containing the optimized objective per component. 
-
-    Attributes created when more than one block of data is provided: 
-        -  `C_`: vector of inner relationship between response and latent variables block
-        -  `coef_`: vector of regression coefficients, if second data block provided 
-        -  `intercept_`: intercept
-        -  `coef_scaled_`: vector of scaled regression coefficients (when scaling option used)
-        -  `intercept_scaled_`: scaled intercept
-        -  `residuals_`: vector of regression residuals
-        -  `y_ev_`: y block explained variance 
-        -  `fitted_`: fitted response
-        -  `y_loc_`: y location estimate
-        -  `y_sca_`: y scale estimate
-
-    Attributes created only when corresponding input flags are `True`:
-        -   `whitening_`: whitened data matrix (usually denoted K)
-        -   `mixing_`: mixing matrix estimate
-        -   `scaling_object_`: scaling object from `VersatileScaler`
-
-    
-    """
-
-    def __init__(self,
-                 projection_index, 
-                 pi_arguments = {}, 
-                 n_components = 1, 
-                 trimming = 0,
-                 alpha = 1,
-                 optimizer = 'SLSQP',
-                 optimizer_options = {'maxiter': 100000}, 
-                 optimizer_constraints = None,
-                 regopt = 'OLS',
-                 center = 'mean',
-                 center_data=True,
-                 scale_data=True,
-                 whiten_data=False,
-                 square_pi = False,
-                 compression = False,
-                 copy=True,
-                 verbose=True, 
-                 return_scaling_object=True):
-        # Called arguments
-        self.projection_index = projection_index
-        self.pi_arguments = pi_arguments
-        self.n_components = n_components
-        self.trimming = trimming
-        self.alpha = alpha
-        self.optimizer = optimizer
-        self.optimizer_options = optimizer_options
-        self.optimizer_constraints = optimizer_constraints
-        self.regopt = regopt
-        self.center = center
-        self.center_data = center_data
-        self.scale_data = scale_data
-        self.whiten_data = whiten_data
-        self.square_pi = square_pi
-        self.compression = compression
-        self.copy = copy
-        self.verbose = verbose
-        self.return_scaling_object = return_scaling_object
-        
-        # Other global parameters 
-        self.constraint = 'norm'
-        self.optrange = (-1,1)
-        self.licenter = ['mean','median']
-        if not(self.center in self.licenter):
-            raise(ValueError('Only location estimator classes allowed are: "mean", "median"'))
-    
-
-    def fit(self,X,*args,**kwargs):
-        
-        """
-            Fit a projection pursuit dimension reduction model. 
-
-            Parameters
-            ------------ 
-                
-                X : numpy array 
-                    Input data.
-        
-        """
-
-        # Collect optional fit arguments
-        biascorr = kwargs.pop('biascorr',False)
-            
-        if 'h' not in kwargs:
-            h = self.n_components
-        else:
-            h = kwargs.pop('h')
-            self.n_components = h
-            
-        if 'dmetric' not in kwargs:
-            dmetric = 'euclidean'
-        else:
-            dmetric = kwargs.get('dmetric')
-            
-        if 'mixing' not in kwargs:
-            mixing = False
-        else:
-            mixing = kwargs.get('mixing')
-            
-        if 'y' not in kwargs:
-            na = len(args)
-            if na > 0: #Use of *args makes it sklearn consistent
-                flag = 'two-block'
-                y = args[0]
-            else:
-                flag = 'one-block'
-                y = 0 # to allow calls with 'y=y' in spit of no real y argument present
-        else:
-            flag = 'two-block'
-            y = kwargs.get('y')
-                            
-            if 'quantile' not in kwargs:
-                quantile = .5
-            else:
-                quantile = kwargs.get('quantile')
-                
-            if self.regopt == 'robust':
-            
-                if 'fun' not in kwargs:
-                    fun = 'Hampel'
-                else:
-                    fun = kwargs.get('fun')
-                
-                if 'probp1' not in kwargs:
-                    probp1 = 0.95
-                else:
-                    probp1 = kwargs.get('probp1')
-                
-                if 'probp2' not in kwargs:
-                    probp2 = 0.975
-                else:
-                    probp2 = kwargs.get('probp2')
-                
-                if 'probp3' not in kwargs:
-                    probp3 = 0.99
-                else:
-                    probp3 = kwargs.get('probp3')
-
-            
-        if self.projection_index == dicomo:
-            
-            if self.pi_arguments['mode'] in ('M3','cos','cok'):
-            
-                if 'option' not in kwargs:
-                    option = 1
-                else:
-                    option = kwargs.get('option')
-                
-                if option > 3:
-                    print('Option value >3 will compute results, but meaning may be questionable')
-                
-        # Initiate projection index    
-        self.most = self.projection_index(**self.pi_arguments)         
-        
-        # Initiate some parameters and data frames
-        if self.copy:
-            X0 = copy.deepcopy(X)
-            self.X0 = X0
-        else:
-            X0 = X        
-        X = convert_X_input(X0)    
-        n,p = X0.shape 
-        trimming = self.trimming
-        
-        # Check dimensions 
-        if h > min(n,p):
-            raise(MyException('number of components cannot exceed number of samples'))
-            
-        if (self.projection_index == dicomo and self.pi_arguments['mode'] == 'kurt' and self.whiten_data==False):
-            warnings.warn('Whitening step is recommended for ICA')
-            
-        # Pre-processing adjustment if whitening
-        if self.whiten_data:
-            self.center_data = True
-            self.scale_data = False
-            self.compression = False
-            print('All results produced are for whitened data')
-        
-        # Centring and scaling
-        if self.scale_data:
-            if self.center=='mean':
-                scale = 'std'
-            elif ((self.center=='median')|(self.center=='l1median')):
-                scale = 'mad' 
-        else:
-            scale = 'None'
-            warnings.warn('Without scaling, convergence to optima is not given')
-            
-         # Data Compression for flat tables if required                
-        if ((p>n) and self.compression):
-            V,S,U = np.linalg.svd(X.T,full_matrices=False)
-            X = np.matmul(U.T,np.diag(S))
-            n,p = X.shape
-            
-            if (srs.mad(X)==0).any(): 
-                warnings.warn('Due to low scales in data, compression would induce zero scales.' 
-                              + '\n' + 'Proceeding without compression.')
-                dimensions = False
-                if copy:
-                    X = copy.deepcopy(X0)
-                else:
-                    X = X0
-            else:
-                dimensions = True
-        else:
-            dimensions = False
-        
-        # Initiate centring object and scale X data 
-        centring = VersatileScaler(center=self.center,scale=scale,trimming=trimming)      
-  
-        if self.center_data:
-            Xs = centring.fit_transform(X)
-            mX = centring.col_loc_
-            sX = centring.col_sca_
-        else:
-            Xs = X
-            mX = np.zeros((1,p))
-            sX = np.ones((1,p))
-
-        fit_arguments = {}
-            
-        # Data whitening (best practice for ICA)
-        if self.whiten_data:
-            V,S,U = np.linalg.svd(Xs.T,full_matrices=False)
-            del U
-            K = (V/S)[:,:p]
-            del V,S
-            Xs = np.matmul(Xs, K)
-            Xs *= np.sqrt(p)
-
-        # Pre-process y data when available 
-        if flag != 'one-block':
-            
-            ny = y.shape[0]
-            y = convert_y_input(y)
-            if len(y.shape) < 2:
-                y = np.array(y).reshape((ny,1))
-#            py = y.shape[1]
-            if ny != n:
-                raise(MyException('X and y number of rows must agree'))
-            if self.copy:
-                y0 = copy.deepcopy(y)
-                self.y0 = y0
-                
-            if self.center_data:
-                ys = centring.fit_transform(y)
-                my = centring.col_loc_
-                sy = centring.col_sca_ 
-            else:
-                ys = y
-                my = 0
-                sy = 1
-            ys = np.array(ys).astype('float64')
-        
-        else:
-            ys = None
-                
-
-        # Initializing output matrices
-        W = np.zeros((p,h))
-        T = np.zeros((n,h))
-        P = np.zeros((p,h))
-        B = np.zeros((p,h))
-        R = np.zeros((p,h))
-        B_scaled = np.zeros((p,h))
-        C = np.zeros((h,1))
-        Xev = np.zeros((h,1))
-        assovec = np.zeros((h,1))
-        Maxobjf = np.zeros((h,1))
-
-        # Initialize deflation matrices 
-        E = copy.deepcopy(Xs)
-        f = ys
-
-        bi = np.zeros((p,1))
-        
-        opt_args = { 
-                    'alpha': self.alpha,
-                    'trimming': self.trimming,
-                    'biascorr': biascorr, 
-                    'dmetric' : 'euclidean',
-                    }
-        
-        if self.optimizer=='grid':
-            # Define grid optimization ranges
-            if 'ndir' not in self.optimizer_options:
-                self.optimizer_options['ndir'] = 1000
-            optrange = np.sign(self.optrange)
-            optmax = self.optrange[1]
-            stop0s = np.arcsin(optrange[0])
-            stop1s = np.arcsin(optrange[1])
-            stop1c = np.arccos(optrange[0])
-            stop0c = np.arccos(optrange[1])
-            anglestart = max(stop0c,stop0s)
-            anglestop = max(stop1c,stop1s)
-            nangle = np.linspace(anglestart,anglestop,self.optimizer_options['ndir'],endpoint=False)            
-            alphamat = np.array([np.cos(nangle), np.sin(nangle)])
-            opt_args['_stop0c'] = stop0c
-            opt_args['_stop0s'] = stop0s
-            opt_args['_stop1c'] = stop1c
-            opt_args['_stop1s'] = stop1s
-            opt_args['optmax'] = optmax
-            opt_args['optrange'] = self.optrange
-            opt_args['square_pi'] = self.square_pi
-            if optmax != 1:
-                alphamat *= optmax
-        
-            if p>2:
-                anglestart = min(opt_args['_stop0c'],opt_args['_stop0s'])
-                anglestop = min(opt_args['_stop1c'],opt_args['_stop1s'])
-                nangle = np.linspace(anglestart,anglestop,self.optimizer_options['ndir'],endpoint=True)
-                alphamat2 = np.array([np.cos(nangle), np.sin(nangle)])
-                if optmax != 1:
-                    alphamat2 *= opt_args['optmax']
-                
-            # Arguments for grid plane
-            opt_args['alphamat'] = alphamat,
-            opt_args['ndir'] = self.optimizer_options['ndir'],
-            opt_args['maxiter'] = self.optimizer_options['maxiter']
-            if type(opt_args['ndir'] is tuple): 
-                opt_args['ndir'] = opt_args['ndir'][0]
-            
-            # Arguments for grid plane #2
-            grid_args_2 = { 
-                     'alpha': self.alpha,
-                     'alphamat': alphamat2,
-                     'ndir': self.optimizer_options['ndir'],
-                     'trimming': self.trimming,
-                     'biascorr': biascorr, 
-                     'dmetric' : 'euclidean',
-                     '_stop0c' : stop0c,
-                     '_stop0s' : stop0s,
-                     '_stop1c' : stop1c,
-                     '_stop1s' : stop1s,
-                     'optmax' : optmax,
-                     'optrange' : self.optrange,
-                     'square_pi' : self.square_pi
-                     }
-            if flag=='two-block':
-                grid_args_2['y'] = f
-        
-        if flag=='two-block':
-            opt_args['y'] = f
-            
-
-        # Itertive coefficient estimation
-        for i in range(0,h):
-
-            if self.optimizer=='grid':
-                if p==2:
-                    wi,maximo = gridplane(E,self.most,
-                                          pi_arguments=opt_args
-                                          )
-           
-                elif p>2:
-                
-                    afin = np.zeros((p,1)) # final parameters for linear combinations
-                    Z = copy.deepcopy(E)
-                    # sort variables according to criterion
-                    meas = [self.most.fit(E[:,k].reshape((-1,1)),
-                            **opt_args) 
-                            for k in np.arange(0,p)]
-                    if self.square_pi:
-                        meas = np.square(meas)
-                    wi,maximo = gridplane(Z[:,0:2],self.most,opt_args)
-                    Zopt = np.dot(Z[:,0:2],wi) 
-                    afin[0:2]=wi
-                    for j in np.arange(2,p):
-                        projmat = np.array([np.array(Zopt[:,0]).reshape(-1),
-                                         np.array(Z[:,j]).reshape(-1)]).T
-                        wi,maximo = gridplane(projmat,self.most,
-                                              opt_args
-                                              )
-                        
-                        Zopt = Zopt*float(wi[0]) + Z[:,j].reshape(-1,1)*float(wi[1])
-                        afin[0:(j+1)] = afin[0:(j+1)]*float(wi[0])
-                        afin[j] = float(wi[1])
-
-                    tj = np.dot(Z,afin)
-                    objf = self.most.fit(tj,
-                                     **{**fit_arguments,**opt_args}
-                                    )
-                    if self.square_pi:
-                        objf *= objf
-    
-
-                    # outer loop to run until convergence
-                    objfold = copy.deepcopy(objf)
-                    objf = -1000
-                    afinbest = afin
-                    ii = 0
-                    maxiter_2j = 2**round(np.log2(self.optimizer_options['maxiter'])) 
-                
-                    while ((ii < self.optimizer_options['maxiter'] + 1) and (abs(objfold - objf)/abs(objf) > 1e-4)):
-                        for j in np.arange(0,p):
-                            projmat = np.array([np.array(Zopt[:,0]).reshape(-1),
-                                         np.array(Z[:,j]).reshape(-1)]).T
-                            if j > 16:
-                                divv = maxiter_2j
-                            else:
-                                divv = min(2**j,maxiter_2j)
-                        
-                            wi,maximo = gridplane_2(projmat,
-                                                    self.most,
-                                                    q=afin[j],
-                                                    div=divv,
-                                                    pi_arguments=grid_args_2
-                                                    )
-                            Zopt = Zopt*float(wi[0,0]) + Z[:,j].reshape(-1,1)*float(wi[1,0])
-                            afin *= float(wi[0,0])
-                            afin[j] += float(wi[1,0])
-                        
-                        # % evaluate the objective function:
-                        tj = np.dot(Z,afin)
-                    
-                        objfold = copy.deepcopy(objf)
-                        objf = self.most.fit(tj,
-                                         q=afin,
-                                         **opt_args
-                                         )
-                        if self.square_pi:
-                            objf *= objf
-                    
-                        if  objf!=objfold:
-                            if self.constraint == 'norm':
-                                afinbest = afin/np.sqrt(np.sum(np.square(afin)))
-                            else:
-                                afinbest = afin
-                            
-                        ii +=1
-                        if self.verbose:
-                            print(str(ii))
-                    #endwhile
-                
-                    afinbest = afin
-                    wi = np.zeros((p,1))
-                    wi = afinbest
-                    Maxobjf[i] = objf
-                # endif;%if p>2;
-                
-            elif self.optimizer == 'IPOPT':
-                constraint = {'type':'eq',
-                              'fun': lambda x: np.linalg.norm(x) -1,
-                              }
-                if self.optimizer_constraints is not None: 
-                    constraint = [constraint,self.optimizer_constraints]
-                wi = minimize_ipopt(
-                    pp_objective,
-                    E[0,:].transpose(),
-                    args=(self.most,E,opt_args),
-                    constraints=constraint,
-                    options=self.optimizer_options,
-                ).x
-                wi = np.array(wi).reshape((p,1))
-                wi /= np.sqrt(np.sum(np.square(wi)))
-                
-            elif self.optimizer == 'differential_evolution':
-                constraint = NonlinearConstraint(lambda x: np.linalg.norm(x) -1, lb=0, ub=0)
-                if self.optimizer_constraints is not None: 
-                    constraint = [constraint,self.optimizer_constraints]
-                wi = differential_evolution(
-                    pp_objective,
-                    [(-1e8,1e8) for i in range(p)],
-                    x0=E[0,:].transpose(),
-                    popsize=100,
-                    args=(self.most,E,opt_args),
-                    constraints=constraint,
-                ).x
-                wi = np.array(wi).reshape((p,1))
-                wi /= np.sqrt(np.sum(np.square(wi)))
-                
-            else: # use scipy.optimize
-                if self.trimming > 0: 
-                    warnings.warn('Optimization that involves a trimmed objective is not a quadratic program. The scipy-optimize result will be off!!')
-                if 'center' in self.pi_arguments:
-                    if (self.pi_arguments['center']=='median'): 
-                        warnings.warn('Optimization that involves a median in the objective is not a quadratic program. The scipy-optimize result will be off!!')   
-                constraint = {'type':'eq',
-                              'fun': lambda x: np.linalg.norm(x) -1,
-                              }
-                if self.optimizer_constraints is not None: 
-                    constraint = [constraint,self.optimizer_constraints]
-                wi = minimize(pp_objective,
-                              E[0,:].transpose(),
-                              args=(self.most,E,opt_args),
-                              method=self.optimizer,
-                              constraints=constraint,
-                              options=self.optimizer_options).x
-                wi = np.array(wi).reshape((p,1))
-                wi /= np.sqrt(np.sum(np.square(wi)))
-                
-                
-            # Computing projection weights and scores
-            ti = np.dot(E,wi)
-            if self.optimizer != 'grid':
-                Maxobjf[i] = self.most.fit(np.dot(E,wi),**opt_args)
-            nti = np.linalg.norm(ti)
-            pi = np.dot(E.T,ti) / (nti**2)
-            if self.whiten_data:
-                wi /= np.sqrt((wi**2).sum())
-                wi = K*wi
-            wi0 = wi
-            wi = np.array(wi)
-            if len(W[:,i].shape) == 1:
-                wi = wi.reshape(-1)
-            W[:,i] = wi
-            T[:,i] = np.array(ti).reshape(-1)
-            P[:,i] = np.array(pi).reshape(-1)
-            
-            if flag != 'one-block':
-                criteval = self.most.fit(np.dot(E,wi0),
-                                         **opt_args
-                                         )
-                if self.square_pi:
-                    criteval *= criteval
-                    
-                assovec[i] = criteval
-                
-
-            # Deflation of the datamatrix guaranteeing orthogonality restrictions
-            E -= ti*pi.T
- 
-            # Calculate R-Weights
-            R = np.dot(W[:,0:(i+1)],pinv(np.dot(P[:,0:(i+1)].T,W[:,0:(i+1)]),check_finite=False))
-        
-            # Execute regression y~T if y is present. Generate regression estimates.
-            if flag != 'one-block':
-                if self.regopt=='OLS':
-                    ci = np.dot(ti.T,ys)/(nti**2)
-                elif self.regopt == 'robust':
-                    linfit = rm(fun=fun,probp1=probp1,probp2=probp2,probp3=probp3,
-                                centre=self.center,scale=scale,
-                                start_cutoff_mode='specific',verbose=self.verbose)
-                    linfit.fit(ti,ys)
-                    ci = linfit.coef_
-                elif self.regopt == 'quantile':
-                    linfit = QuantReg(y,ti)
-                    model = linfit.fit(q=quantile)
-                    ci = model.params
-                # end regression if
-                
-                C[i] = ci
-                bi = np.dot(R,C[0:(i+1)])
-                bi_scaled = bi
-                bi = np.multiply(np.reshape(sy/sX,(p,1)),bi)
-                B[:,i] = bi[:,0]
-                B_scaled[:,i] = bi_scaled[:,0]
-
-        # endfor; Loop for latent dimensions
-
-        # Re-adjust estimates to original dimensions if data have been compressed 
-        if dimensions:
-            B = np.matmul(V[:,0:p],B)
-            B_scaled = np.matmul(V[:,0:p],B_scaled)
-            R = np.matmul(V[:,0:p],R)
-            W = np.matmul(V[:,0:p],W)
-            P = np.matmul(V[:,0:p],P)
-            bi = B[:,h-1]
-            if self.center_data:
-                Xs = centring.fit_transform(X0)
-                mX = centring.col_loc_
-                sX = centring.col_sca_
-            else:
-                Xs = X0
-                mX = np.zeros((1,p))
-                sX = np.ones((1,p))
-        
-        bi = bi.astype("float64")
-        if flag != 'one-block':            
-            # Calculate scaled and unscaled intercepts
-            if dimensions:
-                X = convert_X_input(X0)
-            if(self.center == "mean"):
-                intercept = sps.trim_mean(y - np.matmul(X,bi),trimming)
-            else:
-                intercept = np.median(np.reshape(y - np.matmul(X,bi),(-1)))
-            yfit = np.matmul(X,bi) + intercept
-            if not(scale == 'None'):
-                if (self.center == "mean"):
-                    b0 = np.mean(ys - np.matmul(Xs.astype("float64"),bi))
-                else:
-                    b0 = np.median(np.array(ys.astype("float64") - np.matmul(Xs.astype("float64"),bi)))
-            else:
-                b0 = intercept
-            
-            # Calculate fit values and residuals 
-            yfit = yfit    
-            r = y - yfit
-            setattr(self,"coef_",B)
-            setattr(self,"intercept_",intercept)
-            setattr(self,"coef_scaled_",B_scaled)
-            setattr(self,"intercept_scaled_",b0)
-            setattr(self,"residuals_",r)
-            setattr(self,"fitted_",yfit)
-            setattr(self,"y_loadings_",C)
-            setattr(self,"y_loc_",my)
-            setattr(self,"y_sca_",sy)
-                
-        setattr(self,"x_weights_",W)
-        setattr(self,"x_loadings_",P)
-        setattr(self,"x_rotations_",R)
-        setattr(self,"x_scores_",T)
-        setattr(self,"x_ev_",Xev)
-        setattr(self,"crit_values_",assovec)
-        setattr(self,"Maxobjf_",Maxobjf)
-        
-        if self.whiten_data:
-            setattr(self,"whitening_",K)
-
-        
-        if mixing:
-            setattr(self,"mixing_",np.linalg.pinv(W))
-        
-        
-        setattr(self,"x_loc_",mX)
-        setattr(self,"x_sca_",sX)
-
-        setattr(self,'scaling',scale)
-        if self.return_scaling_object:
-            setattr(self,'scaling_object_',centring)
-        
-        return(self)   
-
-
-    def predict(self,Xn):
-        """
-        predicts the response  on new data Xn
-
-        Parameters
-        ----------
-                Xn : matrix or data frame
-                     Input data to be transformed 
-                
-        Returns
-        -------
-        predictions : numpy array 
-                      The predictions from the dimension reduction model
-        """
-        Xn = convert_X_input(Xn)
-        (n,p) = Xn.shape
-        (q,h) = self.coef_.shape
-        if p!=q:
-            raise(ValueError('New data must have seame number of columns as the ones the model has been trained with'))
-        return(np.array(np.matmul(Xn,self.coef_[:,h-1]) + self.intercept_).T.reshape(-1))
-        
-    def transform(self,Xn):
-        """
-        Computes the dimension reduction of the data Xn based on the fitted sudire model.
-
-        Parameters
-        ----------
-                Xn : matrix or data frame
-                     Input data to be transformed 
-
-        Returns
-        -------
-        transformed_data : numpy array
-                             the dimension reduced data 
-        """
-        Xn = convert_X_input(Xn)
-        (n,p) = Xn.shape
-        if p!= self.x_loadings_.shape[0]:
-            raise(ValueError('New data must have seame number of columns as the ones the model has been trained with'))
-        Xnc = scale_data(Xn,self.x_loc_,self.x_sca_)
-        return(Xnc*self.x_rotations_)
-        
-    @classmethod   
-    def _get_param_names(cls):
-        """Get parameter names for the estimator"""
-        # fetch the constructor or the original constructor before
-        # deprecation wrapping if any
-        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
-        if init is object.__init__:
-            # No explicit constructor to introspect
-            return []
-    
-        # introspect the constructor arguments to find the model parameters
-        # to represent
-        init_signature = inspect.signature(init)
-        # Consider the constructor parameters excluding 'self'
-        parameters = [p for p in init_signature.parameters.values()
-                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]
-        for p in parameters:
-            if p.kind == p.VAR_POSITIONAL:
-                raise RuntimeError("scikit-learn estimators should always "
-                                   "specify their parameters in the signature"
-                                   " of their __init__ (no varargs)."
-                                   " %s with constructor %s doesn't "
-                                   " follow this convention."
-                                   % (cls, init_signature))
-        # Extract and sort argument names excluding 'self'
-        return sorted([p.name for p in parameters])
-    
-    def get_params(self, deep=False):
-        """Get parameters for this estimator.
-        Parameters
-        ----------
-        deep : boolean, optional
-            If True, will return the parameters for this estimator and
-            contained subobjects that are estimators.
-        Returns
-        -------
-        params : mapping of string to any
-            Parameter names mapped to their values.
-        ------
-        Copied from ScikitLlearn instead of imported to avoid 'deep=True'
-        """
-        out = dict()
-        for key in self._get_param_names():
-            value = getattr(self, key, None)
-            if deep and hasattr(value, 'get_params'):
-                deep_items = value.get_params().items()
-                out.update((key + '__' + k, val) for k, val in deep_items)
-            out[key] = value
-        return out
-        
-    def set_params(self, **params):
-        """Set the parameters of this estimator.
-        Copied from ScikitLearn, adapted to avoid calling 'deep=True'
-        Returns
-        -------
-        self
-        ------
-        Copied from ScikitLlearn instead of imported to avoid 'deep=True'
-        """
-        if not params:
-            # Simple optimization to gain speed (inspect is slow)
-            return self
-        valid_params = self.get_params()
-    
-        nested_params = defaultdict(dict)  # grouped by prefix
-        for key, value in params.items():
-            key, delim, sub_key = key.partition('__')
-            if key not in valid_params:
-                raise ValueError('Invalid parameter %s for estimator %s. '
-                                 'Check the list of available parameters '
-                                 'with `estimator.get_params().keys()`.' %
-                                 (key, self))
-    
-            if delim:
-                nested_params[key][sub_key] = value
-            else:
-                setattr(self, key, value)
-                valid_params[key] = value
-    
-        for key, sub_params in nested_params.items():
-            valid_params[key].set_params(**sub_params)
-    
-        return self
-        
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+# Created on Sun Dec 30 12:02:12 2018
+
+# ppdire - Projection pursuit dimension reduction
+
+# @author: Sven Serneels (Ponalytics)
+
+
+
+#from .dicomo import dicomo
+import numpy as np
+from statsmodels.regression.quantile_regression import QuantReg
+import statsmodels.robust as srs
+import scipy.stats as sps
+from scipy.linalg import pinv
+from scipy.optimize import minimize, differential_evolution, NonlinearConstraint
+import copy
+from sklearn.utils.metaestimators import _BaseComposition
+from sklearn.base import RegressorMixin,BaseEstimator,TransformerMixin, defaultdict
+from sklearn.utils.extmath import svd_flip
+from ..sprm.rm import rm 
+from ..preprocessing.robcent import VersatileScaler
+import warnings
+from ..dicomo.dicomo import dicomo 
+from ..dicomo._dicomo_utils import * 
+from .capi import capi
+from ._ppdire_utils import *
+from ..preprocessing._preproc_utilities import scale_data
+from ..utils.utils import MyException, convert_X_input, convert_y_input
+import inspect
+from ..ipopt_temp.ipopt_wrapper import minimize_ipopt
+from ..ipopt_temp.jacobian import (
+    FunctionWithApproxJacobianCentral,
+    FunctionWithApproxJacobian,
+)
+
+class ppdire(_BaseComposition,BaseEstimator,TransformerMixin,RegressorMixin):
+    
+    """
+    PPDIRE Projection Pursuit Dimension Reduction
+    
+    The class allows for calculation of the projection pursuit optimization
+    either through `scipy.optimize` or through the grid algorithm, native to this
+    package. The class provides a very flexible way to access optimization of 
+    projection indices that can lead to either classical or robust dimension
+    reduction. Optimization through scipy.optimize is much more efficient, yet 
+    it will only provide correct results for classical projection indices. The
+    native grid algorithm should be used when the projection index involves 
+    order statistics of any kind, such as ranks, trimming, winsorizing, or 
+    empirical quantiles. The grid optimization algorithm for projection pursuit 
+    implemented here, was outlined in: 
+        
+        Filzmoser, P., Serneels, S., Croux, C. and Van Espen, P.J., 
+        Robust multivariate methods: The projection pursuit approach,
+        in: From Data and Information Analysis to Knowledge Engineering,
+        Spiliopoulou, M., Kruse, R., Borgelt, C., Nuernberger, A. and Gaul, W., eds., 
+        Springer Verlag, Berlin, Germany,
+        2006, pages 270--277.
+        
+    Parameters
+    ------------ 
+
+        projection_index : function or class. 
+                            dicomo and capi supplied in this package can both be used, 
+                            but user defined projection indices can be processed 
+                                    
+            
+        pi_arguments : dict 
+                        arguments to be passed on to projection index 
+
+        n_components : int 
+                        number of components to estimate
+
+        trimming : float
+                     trimming percentage to be entered as pct/100 
+
+        alpha : float.
+                 Continuum coefficient. Only relevant if ppdire is used to estimate 
+                 (classical or robust) continuum regression 
+
+        optimizer : str.
+                    Presently: either 'grid' (native optimizer), any of the 
+                    options in scipy-optimize (e.g. 'SLSQP'), 
+                    'IPOPT', or 'differential_evolution'
+
+        optimizer_options : dict 
+                            with options to pass on to the optimizer 
+                            If optimizer == 'grid',
+                            ndir: int: Number of directions to calculate per iteration.
+                            maxiter: int. Maximal number of iterations.
+
+        optimizer_constraints : dict or list of dicts, 
+                                further constraints to be passed on to the 
+                                optimizer function.
+
+        regopt : str. 
+                regression option for regression step y~T. Can be set to 
+                'OLS' (default), 'robust' (will run sprm.rm) or 
+                'quantile' (statsmodels.regression.quantreg). 
+
+        center : str, 
+                how to center the data. options accepted are options from sprm.preprocessing 
+
+        center_data : bool 
+
+        scale_data : bool. 
+                    Note: if set to False, convergence to correct optimum  is 
+                    not a given. Will throw a warning. 
+
+        whiten_data : bool. 
+                    Typically used for ICA (kurtosis as PI)
+
+        square_pi : bool. 
+                    Whether to square the projection index upon evaluation.
+
+        compression : bool. 
+                        Use internal data compresion step for flat data. 
+
+        copy : bool. 
+                Whether to make a deep copy of the input data or not. 
+
+        verbose : bool. 
+                    Set to True prints the iteration number. 
+
+        return_scaling_object : bool.
+                                If True, the rescaling object will be returned. 
+
+    Attributes
+    ------------ 
+    Attributes always provided 
+        -  `x_weights_`: X block PPDIRE weighting vectors (usually denoted W)
+        -  `x_loadings_`: X block PPDIRE loading vectors (usually denoted P)
+        -  `x_scores_`: X block PPDIRE score vectors (usually denoted T)
+        -  `x_ev_`: X block explained variance per component
+        -  `x_Rweights_`: X block SIMPLS style weighting vectors (usually denoted R)
+        -  `x_loc_`: X block location estimate 
+        -  `x_sca_`: X block scale estimate
+        -  `crit_values_`: vector of evaluated values for the optimization objective. 
+        -  `Maxobjf_`: vector containing the optimized objective per component. 
+
+    Attributes created when more than one block of data is provided: 
+        -  `C_`: vector of inner relationship between response and latent variables block
+        -  `coef_`: vector of regression coefficients, if second data block provided 
+        -  `intercept_`: intercept
+        -  `coef_scaled_`: vector of scaled regression coefficients (when scaling option used)
+        -  `intercept_scaled_`: scaled intercept
+        -  `residuals_`: vector of regression residuals
+        -  `y_ev_`: y block explained variance 
+        -  `fitted_`: fitted response
+        -  `y_loc_`: y location estimate
+        -  `y_sca_`: y scale estimate
+
+    Attributes created only when corresponding input flags are `True`:
+        -   `whitening_`: whitened data matrix (usually denoted K)
+        -   `mixing_`: mixing matrix estimate
+        -   `scaling_object_`: scaling object from `VersatileScaler`
+
+    
+    """
+
+    def __init__(self,
+                 projection_index, 
+                 pi_arguments = {}, 
+                 n_components = 1, 
+                 trimming = 0,
+                 alpha = 1,
+                 optimizer = 'SLSQP',
+                 optimizer_options = {'maxiter': 100000}, 
+                 optimizer_constraints = None,
+                 regopt = 'OLS',
+                 center = 'mean',
+                 center_data=True,
+                 scale_data=True,
+                 whiten_data=False,
+                 square_pi = False,
+                 compression = False,
+                 copy=True,
+                 verbose=True, 
+                 return_scaling_object=True):
+        # Called arguments
+        self.projection_index = projection_index
+        self.pi_arguments = pi_arguments
+        self.n_components = n_components
+        self.trimming = trimming
+        self.alpha = alpha
+        self.optimizer = optimizer
+        self.optimizer_options = optimizer_options
+        self.optimizer_constraints = optimizer_constraints
+        self.regopt = regopt
+        self.center = center
+        self.center_data = center_data
+        self.scale_data = scale_data
+        self.whiten_data = whiten_data
+        self.square_pi = square_pi
+        self.compression = compression
+        self.copy = copy
+        self.verbose = verbose
+        self.return_scaling_object = return_scaling_object
+        
+        # Other global parameters 
+        self.constraint = 'norm'
+        self.optrange = (-1,1)
+        self.licenter = ['mean','median']
+        if not(self.center in self.licenter):
+            raise(ValueError('Only location estimator classes allowed are: "mean", "median"'))
+    
+
+    def fit(self,X,*args,**kwargs):
+        
+        """
+            Fit a projection pursuit dimension reduction model. 
+
+            Parameters
+            ------------ 
+                
+                X : numpy array 
+                    Input data.
+        
+        """
+
+        # Collect optional fit arguments
+        biascorr = kwargs.pop('biascorr',False)
+            
+        if 'h' not in kwargs:
+            h = self.n_components
+        else:
+            h = kwargs.pop('h')
+            self.n_components = h
+            
+        if 'dmetric' not in kwargs:
+            dmetric = 'euclidean'
+        else:
+            dmetric = kwargs.get('dmetric')
+            
+        if 'mixing' not in kwargs:
+            mixing = False
+        else:
+            mixing = kwargs.get('mixing')
+            
+        if 'y' not in kwargs:
+            na = len(args)
+            if na > 0: #Use of *args makes it sklearn consistent
+                flag = 'two-block'
+                y = args[0]
+            else:
+                flag = 'one-block'
+                y = 0 # to allow calls with 'y=y' in spit of no real y argument present
+        else:
+            flag = 'two-block'
+            y = kwargs.get('y')
+                            
+            if 'quantile' not in kwargs:
+                quantile = .5
+            else:
+                quantile = kwargs.get('quantile')
+                
+            if self.regopt == 'robust':
+            
+                if 'fun' not in kwargs:
+                    fun = 'Hampel'
+                else:
+                    fun = kwargs.get('fun')
+                
+                if 'probp1' not in kwargs:
+                    probp1 = 0.95
+                else:
+                    probp1 = kwargs.get('probp1')
+                
+                if 'probp2' not in kwargs:
+                    probp2 = 0.975
+                else:
+                    probp2 = kwargs.get('probp2')
+                
+                if 'probp3' not in kwargs:
+                    probp3 = 0.99
+                else:
+                    probp3 = kwargs.get('probp3')
+
+            
+        if self.projection_index == dicomo:
+            
+            if self.pi_arguments['mode'] in ('M3','cos','cok'):
+            
+                if 'option' not in kwargs:
+                    option = 1
+                else:
+                    option = kwargs.get('option')
+                
+                if option > 3:
+                    print('Option value >3 will compute results, but meaning may be questionable')
+                
+        # Initiate projection index    
+        self.most = self.projection_index(**self.pi_arguments)         
+        
+        # Initiate some parameters and data frames
+        if self.copy:
+            X0 = copy.deepcopy(X)
+            self.X0 = X0
+        else:
+            X0 = X        
+        X = convert_X_input(X0)    
+        n,p = X0.shape 
+        trimming = self.trimming
+        
+        # Check dimensions 
+        if h > min(n,p):
+            raise(MyException('number of components cannot exceed number of samples'))
+            
+        if (self.projection_index == dicomo and self.pi_arguments['mode'] == 'kurt' and self.whiten_data==False):
+            warnings.warn('Whitening step is recommended for ICA')
+            
+        # Pre-processing adjustment if whitening
+        if self.whiten_data:
+            self.center_data = True
+            self.scale_data = False
+            self.compression = False
+            print('All results produced are for whitened data')
+        
+        # Centring and scaling
+        if self.scale_data:
+            if self.center=='mean':
+                scale = 'std'
+            elif ((self.center=='median')|(self.center=='l1median')):
+                scale = 'mad' 
+        else:
+            scale = 'None'
+            warnings.warn('Without scaling, convergence to optima is not given')
+            
+         # Data Compression for flat tables if required                
+        if ((p>n) and self.compression):
+            V,S,U = np.linalg.svd(X.T,full_matrices=False)
+            X = np.matmul(U.T,np.diag(S))
+            n,p = X.shape
+            
+            if (srs.mad(X)==0).any(): 
+                warnings.warn('Due to low scales in data, compression would induce zero scales.' 
+                              + '\n' + 'Proceeding without compression.')
+                dimensions = False
+                if copy:
+                    X = copy.deepcopy(X0)
+                else:
+                    X = X0
+            else:
+                dimensions = True
+        else:
+            dimensions = False
+        
+        # Initiate centring object and scale X data 
+        centring = VersatileScaler(center=self.center,scale=scale,trimming=trimming)      
+  
+        if self.center_data:
+            Xs = centring.fit_transform(X)
+            mX = centring.col_loc_
+            sX = centring.col_sca_
+        else:
+            Xs = X
+            mX = np.zeros((1,p))
+            sX = np.ones((1,p))
+
+        fit_arguments = {}
+            
+        # Data whitening (best practice for ICA)
+        if self.whiten_data:
+            V,S,U = np.linalg.svd(Xs.T,full_matrices=False)
+            del U
+            K = (V/S)[:,:p]
+            del V,S
+            Xs = np.matmul(Xs, K)
+            Xs *= np.sqrt(p)
+
+        # Pre-process y data when available 
+        if flag != 'one-block':
+            
+            ny = y.shape[0]
+            y = convert_y_input(y)
+            if len(y.shape) < 2:
+                y = np.array(y).reshape((ny,1))
+#            py = y.shape[1]
+            if ny != n:
+                raise(MyException('X and y number of rows must agree'))
+            if self.copy:
+                y0 = copy.deepcopy(y)
+                self.y0 = y0
+                
+            if self.center_data:
+                ys = centring.fit_transform(y)
+                my = centring.col_loc_
+                sy = centring.col_sca_ 
+            else:
+                ys = y
+                my = 0
+                sy = 1
+            ys = np.array(ys).astype('float64')
+        
+        else:
+            ys = None
+                
+
+        # Initializing output matrices
+        W = np.zeros((p,h))
+        T = np.zeros((n,h))
+        P = np.zeros((p,h))
+        B = np.zeros((p,h))
+        R = np.zeros((p,h))
+        B_scaled = np.zeros((p,h))
+        C = np.zeros((h,1))
+        Xev = np.zeros((h,1))
+        assovec = np.zeros((h,1))
+        Maxobjf = np.zeros((h,1))
+
+        # Initialize deflation matrices 
+        E = copy.deepcopy(Xs)
+        f = ys
+
+        bi = np.zeros((p,1))
+        
+        opt_args = { 
+                    'alpha': self.alpha,
+                    'trimming': self.trimming,
+                    'biascorr': biascorr, 
+                    'dmetric' : 'euclidean',
+                    }
+        
+        if self.optimizer=='grid':
+            # Define grid optimization ranges
+            if 'ndir' not in self.optimizer_options:
+                self.optimizer_options['ndir'] = 1000
+            optrange = np.sign(self.optrange)
+            optmax = self.optrange[1]
+            stop0s = np.arcsin(optrange[0])
+            stop1s = np.arcsin(optrange[1])
+            stop1c = np.arccos(optrange[0])
+            stop0c = np.arccos(optrange[1])
+            anglestart = max(stop0c,stop0s)
+            anglestop = max(stop1c,stop1s)
+            nangle = np.linspace(anglestart,anglestop,self.optimizer_options['ndir'],endpoint=False)            
+            alphamat = np.array([np.cos(nangle), np.sin(nangle)])
+            opt_args['_stop0c'] = stop0c
+            opt_args['_stop0s'] = stop0s
+            opt_args['_stop1c'] = stop1c
+            opt_args['_stop1s'] = stop1s
+            opt_args['optmax'] = optmax
+            opt_args['optrange'] = self.optrange
+            opt_args['square_pi'] = self.square_pi
+            if optmax != 1:
+                alphamat *= optmax
+        
+            if p>2:
+                anglestart = min(opt_args['_stop0c'],opt_args['_stop0s'])
+                anglestop = min(opt_args['_stop1c'],opt_args['_stop1s'])
+                nangle = np.linspace(anglestart,anglestop,self.optimizer_options['ndir'],endpoint=True)
+                alphamat2 = np.array([np.cos(nangle), np.sin(nangle)])
+                if optmax != 1:
+                    alphamat2 *= opt_args['optmax']
+                
+            # Arguments for grid plane
+            opt_args['alphamat'] = alphamat,
+            opt_args['ndir'] = self.optimizer_options['ndir'],
+            opt_args['maxiter'] = self.optimizer_options['maxiter']
+            if type(opt_args['ndir'] is tuple): 
+                opt_args['ndir'] = opt_args['ndir'][0]
+            
+            # Arguments for grid plane #2
+            grid_args_2 = { 
+                     'alpha': self.alpha,
+                     'alphamat': alphamat2,
+                     'ndir': self.optimizer_options['ndir'],
+                     'trimming': self.trimming,
+                     'biascorr': biascorr, 
+                     'dmetric' : 'euclidean',
+                     '_stop0c' : stop0c,
+                     '_stop0s' : stop0s,
+                     '_stop1c' : stop1c,
+                     '_stop1s' : stop1s,
+                     'optmax' : optmax,
+                     'optrange' : self.optrange,
+                     'square_pi' : self.square_pi
+                     }
+            if flag=='two-block':
+                grid_args_2['y'] = f
+        
+        if flag=='two-block':
+            opt_args['y'] = f
+            
+
+        # Itertive coefficient estimation
+        for i in range(0,h):
+
+            if self.optimizer=='grid':
+                if p==2:
+                    wi,maximo = gridplane(E,self.most,
+                                          pi_arguments=opt_args
+                                          )
+           
+                elif p>2:
+                
+                    afin = np.zeros((p,1)) # final parameters for linear combinations
+                    Z = copy.deepcopy(E)
+                    # sort variables according to criterion
+                    meas = [self.most.fit(E[:,k].reshape((-1,1)),
+                            **opt_args) 
+                            for k in np.arange(0,p)]
+                    if self.square_pi:
+                        meas = np.square(meas)
+                    wi,maximo = gridplane(Z[:,0:2],self.most,opt_args)
+                    Zopt = np.dot(Z[:,0:2],wi) 
+                    afin[0:2]=wi
+                    for j in np.arange(2,p):
+                        projmat = np.array([np.array(Zopt[:,0]).reshape(-1),
+                                         np.array(Z[:,j]).reshape(-1)]).T
+                        wi,maximo = gridplane(projmat,self.most,
+                                              opt_args
+                                              )
+                        
+                        Zopt = Zopt*float(wi[0]) + Z[:,j].reshape(-1,1)*float(wi[1])
+                        afin[0:(j+1)] = afin[0:(j+1)]*float(wi[0])
+                        afin[j] = float(wi[1])
+
+                    tj = np.dot(Z,afin)
+                    objf = self.most.fit(tj,
+                                     **{**fit_arguments,**opt_args}
+                                    )
+                    if self.square_pi:
+                        objf *= objf
+    
+
+                    # outer loop to run until convergence
+                    objfold = copy.deepcopy(objf)
+                    objf = -1000
+                    afinbest = afin
+                    ii = 0
+                    maxiter_2j = 2**round(np.log2(self.optimizer_options['maxiter'])) 
+                
+                    while ((ii < self.optimizer_options['maxiter'] + 1) and (abs(objfold - objf)/abs(objf) > 1e-4)):
+                        for j in np.arange(0,p):
+                            projmat = np.array([np.array(Zopt[:,0]).reshape(-1),
+                                         np.array(Z[:,j]).reshape(-1)]).T
+                            if j > 16:
+                                divv = maxiter_2j
+                            else:
+                                divv = min(2**j,maxiter_2j)
+                        
+                            wi,maximo = gridplane_2(projmat,
+                                                    self.most,
+                                                    q=afin[j],
+                                                    div=divv,
+                                                    pi_arguments=grid_args_2
+                                                    )
+                            Zopt = Zopt*float(wi[0,0]) + Z[:,j].reshape(-1,1)*float(wi[1,0])
+                            afin *= float(wi[0,0])
+                            afin[j] += float(wi[1,0])
+                        
+                        # % evaluate the objective function:
+                        tj = np.dot(Z,afin)
+                    
+                        objfold = copy.deepcopy(objf)
+                        objf = self.most.fit(tj,
+                                         q=afin,
+                                         **opt_args
+                                         )
+                        if self.square_pi:
+                            objf *= objf
+                    
+                        if  objf!=objfold:
+                            if self.constraint == 'norm':
+                                afinbest = afin/np.sqrt(np.sum(np.square(afin)))
+                            else:
+                                afinbest = afin
+                            
+                        ii +=1
+                        if self.verbose:
+                            print(str(ii))
+                    #endwhile
+                
+                    afinbest = afin
+                    wi = np.zeros((p,1))
+                    wi = afinbest
+                    Maxobjf[i] = objf
+                # endif;%if p>2;
+                
+            elif self.optimizer == 'IPOPT':
+                constraint = {'type':'eq',
+                              'fun': lambda x: np.linalg.norm(x) -1,
+                              }
+                if self.optimizer_constraints is not None: 
+                    constraint = [constraint,self.optimizer_constraints]
+                wi = minimize_ipopt(
+                    pp_objective,
+                    E[0,:].transpose(),
+                    args=(self.most,E,opt_args),
+                    constraints=constraint,
+                    options=self.optimizer_options,
+                ).x
+                wi = np.array(wi).reshape((p,1))
+                wi /= np.sqrt(np.sum(np.square(wi)))
+                
+            elif self.optimizer == 'differential_evolution':
+                constraint = NonlinearConstraint(lambda x: np.linalg.norm(x) -1, lb=0, ub=0)
+                if self.optimizer_constraints is not None: 
+                    constraint = [constraint,self.optimizer_constraints]
+                wi = differential_evolution(
+                    pp_objective,
+                    [(-1e8,1e8) for i in range(p)],
+                    x0=E[0,:].transpose(),
+                    popsize=100,
+                    args=(self.most,E,opt_args),
+                    constraints=constraint,
+                ).x
+                wi = np.array(wi).reshape((p,1))
+                wi /= np.sqrt(np.sum(np.square(wi)))
+                
+            else: # use scipy.optimize
+                if self.trimming > 0: 
+                    warnings.warn('Optimization that involves a trimmed objective is not a quadratic program. The scipy-optimize result will be off!!')
+                if 'center' in self.pi_arguments:
+                    if (self.pi_arguments['center']=='median'): 
+                        warnings.warn('Optimization that involves a median in the objective is not a quadratic program. The scipy-optimize result will be off!!')   
+                constraint = {'type':'eq',
+                              'fun': lambda x: np.linalg.norm(x) -1,
+                              }
+                if self.optimizer_constraints is not None: 
+                    constraint = [constraint,self.optimizer_constraints]
+                wi = minimize(pp_objective,
+                              E[0,:].transpose(),
+                              args=(self.most,E,opt_args),
+                              method=self.optimizer,
+                              constraints=constraint,
+                              options=self.optimizer_options).x
+                wi = np.array(wi).reshape((p,1))
+                wi /= np.sqrt(np.sum(np.square(wi)))
+                
+                
+            # Computing projection weights and scores
+            ti = np.dot(E,wi)
+            if self.optimizer != 'grid':
+                Maxobjf[i] = self.most.fit(np.dot(E,wi),**opt_args)
+            nti = np.linalg.norm(ti)
+            pi = np.dot(E.T,ti) / (nti**2)
+            if self.whiten_data:
+                wi /= np.sqrt((wi**2).sum())
+                wi = K*wi
+            wi0 = wi
+            wi = np.array(wi)
+            if len(W[:,i].shape) == 1:
+                wi = wi.reshape(-1)
+            W[:,i] = wi
+            T[:,i] = np.array(ti).reshape(-1)
+            P[:,i] = np.array(pi).reshape(-1)
+            
+            if flag != 'one-block':
+                criteval = self.most.fit(np.dot(E,wi0),
+                                         **opt_args
+                                         )
+                if self.square_pi:
+                    criteval *= criteval
+                    
+                assovec[i] = criteval
+                
+
+            # Deflation of the datamatrix guaranteeing orthogonality restrictions
+            E -= ti*pi.T
+ 
+            # Calculate R-Weights
+            R = np.dot(W[:,0:(i+1)],pinv(np.dot(P[:,0:(i+1)].T,W[:,0:(i+1)]),check_finite=False))
+        
+            # Execute regression y~T if y is present. Generate regression estimates.
+            if flag != 'one-block':
+                if self.regopt=='OLS':
+                    ci = np.dot(ti.T,ys)/(nti**2)
+                elif self.regopt == 'robust':
+                    linfit = rm(fun=fun,probp1=probp1,probp2=probp2,probp3=probp3,
+                                centre=self.center,scale=scale,
+                                start_cutoff_mode='specific',verbose=self.verbose)
+                    linfit.fit(ti,ys)
+                    ci = linfit.coef_
+                elif self.regopt == 'quantile':
+                    linfit = QuantReg(y,ti)
+                    model = linfit.fit(q=quantile)
+                    ci = model.params
+                # end regression if
+                
+                C[i] = ci
+                bi = np.dot(R,C[0:(i+1)])
+                bi_scaled = bi
+                bi = np.multiply(np.reshape(sy/sX,(p,1)),bi)
+                B[:,i] = bi[:,0]
+                B_scaled[:,i] = bi_scaled[:,0]
+
+        # endfor; Loop for latent dimensions
+
+        # Re-adjust estimates to original dimensions if data have been compressed 
+        if dimensions:
+            B = np.matmul(V[:,0:p],B)
+            B_scaled = np.matmul(V[:,0:p],B_scaled)
+            R = np.matmul(V[:,0:p],R)
+            W = np.matmul(V[:,0:p],W)
+            P = np.matmul(V[:,0:p],P)
+            bi = B[:,h-1]
+            if self.center_data:
+                Xs = centring.fit_transform(X0)
+                mX = centring.col_loc_
+                sX = centring.col_sca_
+            else:
+                Xs = X0
+                mX = np.zeros((1,p))
+                sX = np.ones((1,p))
+        
+        bi = bi.astype("float64")
+        if flag != 'one-block':            
+            # Calculate scaled and unscaled intercepts
+            if dimensions:
+                X = convert_X_input(X0)
+            if(self.center == "mean"):
+                intercept = sps.trim_mean(y - np.matmul(X,bi),trimming)
+            else:
+                intercept = np.median(np.reshape(y - np.matmul(X,bi),(-1)))
+            yfit = np.matmul(X,bi) + intercept
+            if not(scale == 'None'):
+                if (self.center == "mean"):
+                    b0 = np.mean(ys - np.matmul(Xs.astype("float64"),bi))
+                else:
+                    b0 = np.median(np.array(ys.astype("float64") - np.matmul(Xs.astype("float64"),bi)))
+            else:
+                b0 = intercept
+            
+            # Calculate fit values and residuals 
+            yfit = yfit    
+            r = y - yfit
+            setattr(self,"coef_",B)
+            setattr(self,"intercept_",intercept)
+            setattr(self,"coef_scaled_",B_scaled)
+            setattr(self,"intercept_scaled_",b0)
+            setattr(self,"residuals_",r)
+            setattr(self,"fitted_",yfit)
+            setattr(self,"y_loadings_",C)
+            setattr(self,"y_loc_",my)
+            setattr(self,"y_sca_",sy)
+                
+        setattr(self,"x_weights_",W)
+        setattr(self,"x_loadings_",P)
+        setattr(self,"x_rotations_",R)
+        setattr(self,"x_scores_",T)
+        setattr(self,"x_ev_",Xev)
+        setattr(self,"crit_values_",assovec)
+        setattr(self,"Maxobjf_",Maxobjf)
+        
+        if self.whiten_data:
+            setattr(self,"whitening_",K)
+
+        
+        if mixing:
+            setattr(self,"mixing_",np.linalg.pinv(W))
+        
+        
+        setattr(self,"x_loc_",mX)
+        setattr(self,"x_sca_",sX)
+
+        setattr(self,'scaling',scale)
+        if self.return_scaling_object:
+            setattr(self,'scaling_object_',centring)
+        
+        return(self)   
+
+
+    def predict(self,Xn):
+        """
+        predicts the response  on new data Xn
+
+        Parameters
+        ----------
+                Xn : matrix or data frame
+                     Input data to be transformed 
+                
+        Returns
+        -------
+        predictions : numpy array 
+                      The predictions from the dimension reduction model
+        """
+        Xn = convert_X_input(Xn)
+        (n,p) = Xn.shape
+        (q,h) = self.coef_.shape
+        if p!=q:
+            raise(ValueError('New data must have seame number of columns as the ones the model has been trained with'))
+        return(np.array(np.matmul(Xn,self.coef_[:,h-1]) + self.intercept_).T.reshape(-1))
+        
+    def transform(self,Xn):
+        """
+        Computes the dimension reduction of the data Xn based on the fitted sudire model.
+
+        Parameters
+        ----------
+                Xn : matrix or data frame
+                     Input data to be transformed 
+
+        Returns
+        -------
+        transformed_data : numpy array
+                             the dimension reduced data 
+        """
+        Xn = convert_X_input(Xn)
+        (n,p) = Xn.shape
+        if p!= self.x_loadings_.shape[0]:
+            raise(ValueError('New data must have seame number of columns as the ones the model has been trained with'))
+        Xnc = scale_data(Xn,self.x_loc_,self.x_sca_)
+        return(Xnc*self.x_rotations_)
+        
+    @classmethod   
+    def _get_param_names(cls):
+        """Get parameter names for the estimator"""
+        # fetch the constructor or the original constructor before
+        # deprecation wrapping if any
+        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
+        if init is object.__init__:
+            # No explicit constructor to introspect
+            return []
+    
+        # introspect the constructor arguments to find the model parameters
+        # to represent
+        init_signature = inspect.signature(init)
+        # Consider the constructor parameters excluding 'self'
+        parameters = [p for p in init_signature.parameters.values()
+                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]
+        for p in parameters:
+            if p.kind == p.VAR_POSITIONAL:
+                raise RuntimeError("scikit-learn estimators should always "
+                                   "specify their parameters in the signature"
+                                   " of their __init__ (no varargs)."
+                                   " %s with constructor %s doesn't "
+                                   " follow this convention."
+                                   % (cls, init_signature))
+        # Extract and sort argument names excluding 'self'
+        return sorted([p.name for p in parameters])
+    
+    def get_params(self, deep=False):
+        """Get parameters for this estimator.
+        Parameters
+        ----------
+        deep : boolean, optional
+            If True, will return the parameters for this estimator and
+            contained subobjects that are estimators.
+        Returns
+        -------
+        params : mapping of string to any
+            Parameter names mapped to their values.
+        ------
+        Copied from ScikitLlearn instead of imported to avoid 'deep=True'
+        """
+        out = dict()
+        for key in self._get_param_names():
+            value = getattr(self, key, None)
+            if deep and hasattr(value, 'get_params'):
+                deep_items = value.get_params().items()
+                out.update((key + '__' + k, val) for k, val in deep_items)
+            out[key] = value
+        return out
+        
+    def set_params(self, **params):
+        """Set the parameters of this estimator.
+        Copied from ScikitLearn, adapted to avoid calling 'deep=True'
+        Returns
+        -------
+        self
+        ------
+        Copied from ScikitLlearn instead of imported to avoid 'deep=True'
+        """
+        if not params:
+            # Simple optimization to gain speed (inspect is slow)
+            return self
+        valid_params = self.get_params()
+    
+        nested_params = defaultdict(dict)  # grouped by prefix
+        for key, value in params.items():
+            key, delim, sub_key = key.partition('__')
+            if key not in valid_params:
+                raise ValueError('Invalid parameter %s for estimator %s. '
+                                 'Check the list of available parameters '
+                                 'with `estimator.get_params().keys()`.' %
+                                 (key, self))
+    
+            if delim:
+                nested_params[key][sub_key] = value
+            else:
+                setattr(self, key, value)
+                valid_params[key] = value
+    
+        for key, sub_params in nested_params.items():
+            valid_params[key].set_params(**sub_params)
+    
+        return self
+
```

## direpack/preprocessing/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Jul 22 12:17:17 2018
-
-@author: Sven Serneels, Ponalytics
-"""
-
-__name__ = "preprocessing"
-__author__ = "Sven Serneels"
-__license__ = "MIT"
-__version__ = "0.8.0"
-__date__ = "2024-02-23"
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Jul 22 12:17:17 2018
+
+@author: Sven Serneels, Ponalytics
+"""
+
+__name__ = "preprocessing"
+__author__ = "Sven Serneels"
+__license__ = "MIT"
+__version__ = "0.8.0"
+__date__ = "2024-02-23"
```

## direpack/preprocessing/_gsspp_utils.py

 * *Ordering differences only*

```diff
@@ -1,154 +1,154 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Wed Mar 25 09:02:05 2020
-
-@author: Sven Serneels, Ponalytics. 
-
-Code for radial transform functions largely adapted from 
-R code by Jakob Raymaekers
-
-"""
-
-import numpy as np
-
-def quad(dd, p, n): 
-    """
-    Computes the quadratic radial function
-    args:
-        dd: vector of distances
-        p: number of variables in original data
-        n: number of rows in original data
-    returns:
-        xi: radial function
-    """
-    d_hmed = np.sort(dd,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
-    idx = np.where(dd > d_hmed)[0]
-    xi = np.ones((n,1))
-    xi[idx] = (1 / np.square(dd[idx])) * (d_hmed**2)
-    return(xi) 
-
-def ss(dd, p,*args,prec=1e-10):
-
-    """
-    Computes the spatial sign radial function
-    args:
-      dd: vector of distances
-      p: dimension of original data
-      *args flag to be able to pass on n - has no effect
-    returns:
-      xi: radial function
-    """
-    dd = np.maximum(dd,prec)
-    xi = 1 / dd
-    return(xi)
-
-def winsor(dd, p, n) :
-    """
-    Computes the Winsor radial function
-    args:
-      dd: vector of distances
-      p: number of variables in original data
-      n: number of rows in original data
-    returns:
-      xi: radial function
-    """ 
-    d_hmed  = np.sort(dd,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
-    idx = np.where(dd > d_hmed)[0]
-    xi = np.ones((n,1))
-    xi[idx] = (1 / dd[idx]) * d_hmed
-    return(xi)   
-    
-def ball(dd, p, n): 
-    
-    """
-    Computes the Ball radial function
-    args:
-      dd: vector of distances
-      p: number of variables in original data
-      n: number of rows in original data
-    returns:
-      xi: radial function
-    """
-    
-    dWH = np.power(dd,2/3) 
-    dWH_hmed = np.sort(dWH,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
-    d_hmed = np.power(dWH_hmed,3/2)
-    idx = np.where(dd > d_hmed)[0]
-    xi = np.ones((n,1))
-    xi[idx] =  0
-    return(xi)
-
-
-def shell(dd, p, n) :
-    """
-    Computes the Shell radial function
-    args:
-      dd: vector of distances
-      p: number of variables in original data
-      n: number of rows in original data
-    returns:
-      xi: radial function
-    """
-    
-    dWH = np.power(dd,2/3) 
-    dWH_hmed = np.sort(dWH,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
-    dWH_hmad = np.sort(np.abs(dWH - dWH_hmed),axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
-    cutoff1 = np.power(np.maximum(0, dWH_hmed - dWH_hmad),3/2)
-    cutoff2 = np.power(dWH_hmed + dWH_hmad,3/2)
-    idxlow = np.where(dd < cutoff1)[0] 
-    idxhigh = np.where(dd > cutoff2)[0] 
-    xi = np.ones((n,1))
-    xi[idxlow] = 0
-    xi[idxhigh] = 0
-    return(xi)
-
-
-def linear_redescending(dd, p,n): 
-    """
-    # Computes the Linear redescending radial function
-    args:
-      dd: vector of distances
-      p: number of variables in original data
-      n: number of rows in original data
-    returns:
-      xi: radial function
-    """
-    
-    dWH = np.power(dd,2/3) 
-    dWH_hmed = np.sort(dWH,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
-    dWH_hmad = np.sort(np.abs(dWH - dWH_hmed),axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
-    d_hmed = dWH_hmed**(3/2)
-    cutoff = (dWH_hmed + 1.4826 * dWH_hmad)**(3/2)
-    idxmid = np.where(np.logical_and(dd > d_hmed,dd <= cutoff))[0]
-    idxhigh = np.where(dd > cutoff)[0]
-    xi = np.ones((n,1))
-    xi[idxmid] = 1 - (dd[idxmid,:] - d_hmed) / (cutoff - d_hmed)
-    xi[idxhigh] = 0
-    return(xi)
-    
-
-def _norms(X,**kwargs):
-    """
-    Casewise norms of a matrix
-    """
-    return(np.linalg.norm(X,axis=1,keepdims=True,**kwargs))
-    
-    
-def _gsspp(X,p,n,fun=ss):
-    """
-    Generalized Spatial Sign Pre-Processing for Centred Data
-    """
-    return(np.multiply(X,fun(_norms(X),p,n)))
-        
-def _spatial_sign(X,**kwargs):
-    """
-    Spatial Sign Pre-Processing for Centred Data
-    """
-    return(X/_norms(X))
-    
-
-    
-    
-
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Wed Mar 25 09:02:05 2020
+
+@author: Sven Serneels, Ponalytics. 
+
+Code for radial transform functions largely adapted from 
+R code by Jakob Raymaekers
+
+"""
+
+import numpy as np
+
+def quad(dd, p, n): 
+    """
+    Computes the quadratic radial function
+    args:
+        dd: vector of distances
+        p: number of variables in original data
+        n: number of rows in original data
+    returns:
+        xi: radial function
+    """
+    d_hmed = np.sort(dd,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
+    idx = np.where(dd > d_hmed)[0]
+    xi = np.ones((n,1))
+    xi[idx] = (1 / np.square(dd[idx])) * (d_hmed**2)
+    return(xi) 
+
+def ss(dd, p,*args,prec=1e-10):
+
+    """
+    Computes the spatial sign radial function
+    args:
+      dd: vector of distances
+      p: dimension of original data
+      *args flag to be able to pass on n - has no effect
+    returns:
+      xi: radial function
+    """
+    dd = np.maximum(dd,prec)
+    xi = 1 / dd
+    return(xi)
+
+def winsor(dd, p, n) :
+    """
+    Computes the Winsor radial function
+    args:
+      dd: vector of distances
+      p: number of variables in original data
+      n: number of rows in original data
+    returns:
+      xi: radial function
+    """ 
+    d_hmed  = np.sort(dd,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
+    idx = np.where(dd > d_hmed)[0]
+    xi = np.ones((n,1))
+    xi[idx] = (1 / dd[idx]) * d_hmed
+    return(xi)   
+    
+def ball(dd, p, n): 
+    
+    """
+    Computes the Ball radial function
+    args:
+      dd: vector of distances
+      p: number of variables in original data
+      n: number of rows in original data
+    returns:
+      xi: radial function
+    """
+    
+    dWH = np.power(dd,2/3) 
+    dWH_hmed = np.sort(dWH,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
+    d_hmed = np.power(dWH_hmed,3/2)
+    idx = np.where(dd > d_hmed)[0]
+    xi = np.ones((n,1))
+    xi[idx] =  0
+    return(xi)
+
+
+def shell(dd, p, n) :
+    """
+    Computes the Shell radial function
+    args:
+      dd: vector of distances
+      p: number of variables in original data
+      n: number of rows in original data
+    returns:
+      xi: radial function
+    """
+    
+    dWH = np.power(dd,2/3) 
+    dWH_hmed = np.sort(dWH,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
+    dWH_hmad = np.sort(np.abs(dWH - dWH_hmed),axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
+    cutoff1 = np.power(np.maximum(0, dWH_hmed - dWH_hmad),3/2)
+    cutoff2 = np.power(dWH_hmed + dWH_hmad,3/2)
+    idxlow = np.where(dd < cutoff1)[0] 
+    idxhigh = np.where(dd > cutoff2)[0] 
+    xi = np.ones((n,1))
+    xi[idxlow] = 0
+    xi[idxhigh] = 0
+    return(xi)
+
+
+def linear_redescending(dd, p,n): 
+    """
+    # Computes the Linear redescending radial function
+    args:
+      dd: vector of distances
+      p: number of variables in original data
+      n: number of rows in original data
+    returns:
+      xi: radial function
+    """
+    
+    dWH = np.power(dd,2/3) 
+    dWH_hmed = np.sort(dWH,axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
+    dWH_hmad = np.sort(np.abs(dWH - dWH_hmed),axis=0)[int(np.floor((n + p + 1) / 2))-1][0]
+    d_hmed = dWH_hmed**(3/2)
+    cutoff = (dWH_hmed + 1.4826 * dWH_hmad)**(3/2)
+    idxmid = np.where(np.logical_and(dd > d_hmed,dd <= cutoff))[0]
+    idxhigh = np.where(dd > cutoff)[0]
+    xi = np.ones((n,1))
+    xi[idxmid] = 1 - (dd[idxmid,:] - d_hmed) / (cutoff - d_hmed)
+    xi[idxhigh] = 0
+    return(xi)
+    
+
+def _norms(X,**kwargs):
+    """
+    Casewise norms of a matrix
+    """
+    return(np.linalg.norm(X,axis=1,keepdims=True,**kwargs))
+    
+    
+def _gsspp(X,p,n,fun=ss):
+    """
+    Generalized Spatial Sign Pre-Processing for Centred Data
+    """
+    return(np.multiply(X,fun(_norms(X),p,n)))
+        
+def _spatial_sign(X,**kwargs):
+    """
+    Spatial Sign Pre-Processing for Centred Data
+    """
+    return(X/_norms(X))
+    
+
+    
+    
+
```

## direpack/preprocessing/_preproc_utilities.py

 * *Ordering differences only*

```diff
@@ -1,368 +1,368 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sat Dec 21 10:55:24 2019
-
-Set of help functions for robust centring and scaling 
-
-@author: Sven Serneels, Ponalytics
-"""
-
-import numpy as np
-import pandas as ps
-import scipy.stats as sps
-import scipy.optimize as spo
-from statsmodels import robust as srs
-import copy
-
-
-def _handle_zeros_in_scale(scale, copy=True):
-    """
-    Makes sure that whenever scale is zero, we handle it correctly.
-    This happens in most scalers when we have constant features.
-    Taken from ScikitLearn.preprocesssing"""
-
-    # if we are fitting on 1D arrays, scale might be a scalar
-    if np.isscalar(scale):
-        if scale == 0.0:
-            scale = 1.0
-        return scale
-    elif isinstance(scale, np.ndarray):
-        if copy:
-            # New array to avoid side-effects
-            scale = scale.copy()
-        scale[scale == 0.0] = 1.0
-        return scale
-
-
-def _check_trimming(t):
-
-    if (t > 0.99) or (t < 0):
-        raise (ValueError("Trimming fraction must be in [0,1)"))
-
-
-def mad(X, c=0.6744897501960817, **kwargs):
-    """
-    Column-wise median absolute deviation. **kwargs included to allow
-    general function call in scale_data.
-    """
-
-    s = median(np.abs(X - median(X, axis=0)), axis=0) / c
-    s = np.array(s).reshape(-1)
-    # statsmodels.robust.mad is not as flexible toward matrix input,
-    # sometimes throws a value error in ufunc
-    return s
-
-
-def median(X, **kwargs):
-    """
-    Column-wise median. **kwargs included to allow
-    general function call in scale_data.
-    """
-
-    if np.isnan(X).any():
-        m = np.nanmedian(X, axis=0)
-    else:
-        m = np.median(X, axis=0)
-    m = np.array(m).reshape(-1)
-
-    return m
-
-
-def mean(X, trimming=0):
-    """
-    Column-wise mean or trimmed mean. Trimming to be entered as fraction.
-    """
-
-    if trimming == 0:
-        if np.isnan(X).any():
-            m = np.nanmean(X, axis=0)
-        else:
-            m = np.mean(X, axis=0)
-    else:
-        # Returns all NaN if missings in X
-        m = sps.trim_mean(X, trimming, 0)
-
-    return m
-
-
-def std(X, trimming=0):
-    """
-    Column-wise standard devaition or trimmed std.
-    Trimming to be entered as fraction.
-    """
-
-    if trimming == 0:
-        if np.isnan(X).any():
-            s = np.power(np.nanvar(X, axis=0), 0.5)
-        else:
-            s = np.power(np.var(X, axis=0), 0.5)
-        s = np.array(s).reshape(-1)
-    else:
-        var = sps.trim_mean(
-            np.square(X - sps.trim_mean(X, trimming, 0)), trimming, 0
-        )
-        s = np.sqrt(var)
-    return s
-
-
-def _euclidnorm(x):
-    """
-    Euclidean norm of a vector
-    """
-
-    if np.isnan(x).any():
-        return np.sqrt(np.nansum(np.square(x)))
-    else:
-        return np.sqrt(np.sum(np.square(x)))
-
-
-def _diffmat_objective(a, X):
-    """
-    Utility to l1median, matrix of differences
-    """
-
-    (n, p) = X.shape
-    return X - np.tile(a, (n, 1))
-
-
-def _l1m_objective(a, X, *args):
-    """
-    Optimization objective for l1median
-    """
-
-    if np.isnan(X).any():
-        return np.nansum(
-            np.apply_along_axis(_euclidnorm, 1, _diffmat_objective(a, X))
-        )
-    else:
-        return np.sum(
-            np.apply_along_axis(_euclidnorm, 1, _diffmat_objective(a, X))
-        )
-
-
-def _l1m_jacobian(a, X):
-    """
-    Jacobian for l1median
-    """
-
-    (n, p) = X.shape
-    dX = _diffmat_objective(a, X)
-    dists = np.apply_along_axis(_euclidnorm, 1, dX)
-    dists = _handle_zeros_in_scale(dists)
-    dX /= np.tile(np.array(dists).reshape(n, 1), (1, p))
-    if np.isnan(X).any():
-        return -np.nansum(dX, axis=0)
-    else:
-        return -np.sum(dX, axis=0)
-
-
-def _l1median(
-    X, x0, method="SLSQP", tol=1e-8, options={"maxiter": 2000}, **kwargs
-):
-    """
-    Optimization for l1median
-    """
-
-    mu = spo.minimize(
-        _l1m_objective,
-        x0,
-        args=(X),
-        jac=_l1m_jacobian,
-        tol=tol,
-        options=options,
-        method=method,
-    )
-    return mu
-
-
-def l1median(X, **kwargs):
-    """
-    l1median wrapper to generically convert matrices as some of the scipy
-    optimization options will crash when provided matrix input.
-    """
-
-    if "x0" not in kwargs:
-        x0 = median(X)
-
-    if type(X) == np.matrix:
-        X = np.array(X)
-
-    if len(X.shape) == 2:
-        (n, p) = X.shape
-    else:
-        p = 1
-
-    if p < 2:
-        return median(X)
-    else:
-        return _l1median(X, x0, **kwargs).x
-
-
-def kstepLTS(X, maxit=5, tol=1e-10, **kwargs):
-    """
-    Computes the K-step LTS estimator of location
-    It uses the spatial median as a starting value, and yields an
-    estimator with improved statistical efficiency, but at a higher
-    computational cost.
-    Inputs:
-        X: data matrix
-        maxit: maximum number of iterations
-        tol: convergence tolerance
-    Outputs:
-        m2: location estimate
-    """
-    n, p = X.shape
-    m1 = l1median(X)  # initial estimate
-    m2 = copy.deepcopy(m1)
-    iteration = 0
-    unconverged = True
-    while unconverged and (iteration < maxit):
-        if np.isnan(X).any():
-            dists = np.nansum(np.square(X - m1), axis=1)
-        else:
-            dists = np.sum(np.square(X - m1), axis=1)
-        cutdist = np.sort(dists, axis=0)[int(np.floor((n + 1) / 2)) - 1]
-        hsubset = np.where(dists <= cutdist)[0]
-        m2 = np.array(mean(X[hsubset, :])).reshape((p,))
-        unconverged = max(abs(m1 - m2)) > tol
-        iteration += 1
-        m1 = copy.deepcopy(m2)
-
-    return m2
-
-
-def scaleTau2(x0, c1=4.5, c2=3, consistency=True, **kwargs):
-    """
-    Tau estimator of scale
-    Inputs:
-        x0: array or matrix, data
-        c1: consistency factor for initial estimate
-        c2: consistency factor for final estimate
-        consistency: str or bool,
-            False, True, or "finiteSample"
-    Output:
-        the scale estimate
-    """
-
-    x = copy.deepcopy(x0)
-    n, p = x.shape
-    if np.isnan(x).any():
-        summ = np.nansum
-    else:
-        summ = np.sum
-    medx = median(x)
-    xc = abs(x - medx)
-    sigma0 = median(xc)
-    if c1 > 0:
-        xc /= sigma0 * c1
-        w = 1 - np.square(xc)
-        w = np.square((abs(w) + w) / 2)
-        mu = summ(np.multiply(x, w)) / summ(w)
-    else:
-        mu = medx
-    x -= mu
-    x /= sigma0
-    rho = np.square(x)
-    rho[np.where(rho > c2**2)[0]] = c2**2
-    if consistency:
-
-        def Erho(b):
-            return (
-                2
-                * (
-                    (1 - b**2) * sps.norm.cdf(b)
-                    - b * sps.norm.pdf(b)
-                    + b**2
-                )
-                - 1
-            )
-
-        def Es2(c2):
-            return Erho(c2 * sps.norm.ppf(3 / 4))
-
-        if consistency == "finiteSample":
-            nEs2 = (n - 2) * Es2(c2)
-        else:
-            nEs2 = n * Es2(c2)
-    else:
-        nEs2 = n
-    return np.array(sigma0 * np.sqrt(summ(rho) / nEs2)).reshape((p,))
-
-
-def scale_data(X, m, s):
-    """
-    Column-wise data scaling on location and scale estimates.
-
-    """
-
-    n = X.shape
-    if len(n) > 1:
-        p = n[1]
-    else:
-        p = 1
-    n = n[0]
-
-    s = _handle_zeros_in_scale(s)
-
-    if p == 1:
-        Xm = X - float(m)
-        Xs = Xm / s
-    else:
-        Xm = X - np.array([m for i in range(1, n + 1)])
-        Xs = Xm / np.array([s for i in range(1, n + 1)])
-    return Xs
-
-
-def wrap_univ(dd, scale=False, locX=None, scaleX=None):
-    """
-    # Computes the univariate wrapping transformation
-    # Reference: Jakob Raymaekers & Peter J. Rousseeuw (2021)
-    # Fast Robust Correlation for High-Dimensional Data,
-    # Technometrics, 63:2, 184-198.
-    args:
-        dd, np.array: vector of distances
-        scale, bool: if True, will scale data about med/mad
-    returns:
-        xi, np.array: wrapped vector
-    """
-    b = 1.5
-    c = 4
-    q1 = 1.540793
-    q2 = 0.8622731
-    if dd.dtype == "O":
-        dd = dd.astype("float")
-    if scale:
-        locX = median(dd)
-        scaleX = mad(dd)
-        xi = (dd - locX) / scaleX
-    else:
-        xi = np.array(dd)
-    indMid = np.where((np.abs(xi) < c) & (np.abs(xi) >= b))[0]
-    indHigh = np.where(np.abs(xi) >= c)[0]
-    xi[indMid] = (
-        q1
-        * np.tanh(q2 * (c - np.abs(xi[indMid])))
-        * np.abs(xi[indMid])
-        / xi[indMid]
-    )
-    xi[indHigh] = 0
-    xi = xi * scaleX + locX
-
-    return xi
-
-
-def wrap(X, locX, scaleX):
-    """
-    wrap - wrap matrix column wise
-    """
-
-    if len(X.shape) == 1:
-        X = X.reshape(-1, 1)
-
-    return np.array(
-        [
-            wrap_univ(X[:, i], locX=locX[i], scale=True, scaleX=scaleX[i])
-            for i in range(X.shape[1])
-        ]
-    ).transpose()
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sat Dec 21 10:55:24 2019
+
+Set of help functions for robust centring and scaling 
+
+@author: Sven Serneels, Ponalytics
+"""
+
+import numpy as np
+import pandas as ps
+import scipy.stats as sps
+import scipy.optimize as spo
+from statsmodels import robust as srs
+import copy
+
+
+def _handle_zeros_in_scale(scale, copy=True):
+    """
+    Makes sure that whenever scale is zero, we handle it correctly.
+    This happens in most scalers when we have constant features.
+    Taken from ScikitLearn.preprocesssing"""
+
+    # if we are fitting on 1D arrays, scale might be a scalar
+    if np.isscalar(scale):
+        if scale == 0.0:
+            scale = 1.0
+        return scale
+    elif isinstance(scale, np.ndarray):
+        if copy:
+            # New array to avoid side-effects
+            scale = scale.copy()
+        scale[scale == 0.0] = 1.0
+        return scale
+
+
+def _check_trimming(t):
+
+    if (t > 0.99) or (t < 0):
+        raise (ValueError("Trimming fraction must be in [0,1)"))
+
+
+def mad(X, c=0.6744897501960817, **kwargs):
+    """
+    Column-wise median absolute deviation. **kwargs included to allow
+    general function call in scale_data.
+    """
+
+    s = median(np.abs(X - median(X, axis=0)), axis=0) / c
+    s = np.array(s).reshape(-1)
+    # statsmodels.robust.mad is not as flexible toward matrix input,
+    # sometimes throws a value error in ufunc
+    return s
+
+
+def median(X, **kwargs):
+    """
+    Column-wise median. **kwargs included to allow
+    general function call in scale_data.
+    """
+
+    if np.isnan(X).any():
+        m = np.nanmedian(X, axis=0)
+    else:
+        m = np.median(X, axis=0)
+    m = np.array(m).reshape(-1)
+
+    return m
+
+
+def mean(X, trimming=0):
+    """
+    Column-wise mean or trimmed mean. Trimming to be entered as fraction.
+    """
+
+    if trimming == 0:
+        if np.isnan(X).any():
+            m = np.nanmean(X, axis=0)
+        else:
+            m = np.mean(X, axis=0)
+    else:
+        # Returns all NaN if missings in X
+        m = sps.trim_mean(X, trimming, 0)
+
+    return m
+
+
+def std(X, trimming=0):
+    """
+    Column-wise standard devaition or trimmed std.
+    Trimming to be entered as fraction.
+    """
+
+    if trimming == 0:
+        if np.isnan(X).any():
+            s = np.power(np.nanvar(X, axis=0), 0.5)
+        else:
+            s = np.power(np.var(X, axis=0), 0.5)
+        s = np.array(s).reshape(-1)
+    else:
+        var = sps.trim_mean(
+            np.square(X - sps.trim_mean(X, trimming, 0)), trimming, 0
+        )
+        s = np.sqrt(var)
+    return s
+
+
+def _euclidnorm(x):
+    """
+    Euclidean norm of a vector
+    """
+
+    if np.isnan(x).any():
+        return np.sqrt(np.nansum(np.square(x)))
+    else:
+        return np.sqrt(np.sum(np.square(x)))
+
+
+def _diffmat_objective(a, X):
+    """
+    Utility to l1median, matrix of differences
+    """
+
+    (n, p) = X.shape
+    return X - np.tile(a, (n, 1))
+
+
+def _l1m_objective(a, X, *args):
+    """
+    Optimization objective for l1median
+    """
+
+    if np.isnan(X).any():
+        return np.nansum(
+            np.apply_along_axis(_euclidnorm, 1, _diffmat_objective(a, X))
+        )
+    else:
+        return np.sum(
+            np.apply_along_axis(_euclidnorm, 1, _diffmat_objective(a, X))
+        )
+
+
+def _l1m_jacobian(a, X):
+    """
+    Jacobian for l1median
+    """
+
+    (n, p) = X.shape
+    dX = _diffmat_objective(a, X)
+    dists = np.apply_along_axis(_euclidnorm, 1, dX)
+    dists = _handle_zeros_in_scale(dists)
+    dX /= np.tile(np.array(dists).reshape(n, 1), (1, p))
+    if np.isnan(X).any():
+        return -np.nansum(dX, axis=0)
+    else:
+        return -np.sum(dX, axis=0)
+
+
+def _l1median(
+    X, x0, method="SLSQP", tol=1e-8, options={"maxiter": 2000}, **kwargs
+):
+    """
+    Optimization for l1median
+    """
+
+    mu = spo.minimize(
+        _l1m_objective,
+        x0,
+        args=(X),
+        jac=_l1m_jacobian,
+        tol=tol,
+        options=options,
+        method=method,
+    )
+    return mu
+
+
+def l1median(X, **kwargs):
+    """
+    l1median wrapper to generically convert matrices as some of the scipy
+    optimization options will crash when provided matrix input.
+    """
+
+    if "x0" not in kwargs:
+        x0 = median(X)
+
+    if type(X) == np.matrix:
+        X = np.array(X)
+
+    if len(X.shape) == 2:
+        (n, p) = X.shape
+    else:
+        p = 1
+
+    if p < 2:
+        return median(X)
+    else:
+        return _l1median(X, x0, **kwargs).x
+
+
+def kstepLTS(X, maxit=5, tol=1e-10, **kwargs):
+    """
+    Computes the K-step LTS estimator of location
+    It uses the spatial median as a starting value, and yields an
+    estimator with improved statistical efficiency, but at a higher
+    computational cost.
+    Inputs:
+        X: data matrix
+        maxit: maximum number of iterations
+        tol: convergence tolerance
+    Outputs:
+        m2: location estimate
+    """
+    n, p = X.shape
+    m1 = l1median(X)  # initial estimate
+    m2 = copy.deepcopy(m1)
+    iteration = 0
+    unconverged = True
+    while unconverged and (iteration < maxit):
+        if np.isnan(X).any():
+            dists = np.nansum(np.square(X - m1), axis=1)
+        else:
+            dists = np.sum(np.square(X - m1), axis=1)
+        cutdist = np.sort(dists, axis=0)[int(np.floor((n + 1) / 2)) - 1]
+        hsubset = np.where(dists <= cutdist)[0]
+        m2 = np.array(mean(X[hsubset, :])).reshape((p,))
+        unconverged = max(abs(m1 - m2)) > tol
+        iteration += 1
+        m1 = copy.deepcopy(m2)
+
+    return m2
+
+
+def scaleTau2(x0, c1=4.5, c2=3, consistency=True, **kwargs):
+    """
+    Tau estimator of scale
+    Inputs:
+        x0: array or matrix, data
+        c1: consistency factor for initial estimate
+        c2: consistency factor for final estimate
+        consistency: str or bool,
+            False, True, or "finiteSample"
+    Output:
+        the scale estimate
+    """
+
+    x = copy.deepcopy(x0)
+    n, p = x.shape
+    if np.isnan(x).any():
+        summ = np.nansum
+    else:
+        summ = np.sum
+    medx = median(x)
+    xc = abs(x - medx)
+    sigma0 = median(xc)
+    if c1 > 0:
+        xc /= sigma0 * c1
+        w = 1 - np.square(xc)
+        w = np.square((abs(w) + w) / 2)
+        mu = summ(np.multiply(x, w)) / summ(w)
+    else:
+        mu = medx
+    x -= mu
+    x /= sigma0
+    rho = np.square(x)
+    rho[np.where(rho > c2**2)[0]] = c2**2
+    if consistency:
+
+        def Erho(b):
+            return (
+                2
+                * (
+                    (1 - b**2) * sps.norm.cdf(b)
+                    - b * sps.norm.pdf(b)
+                    + b**2
+                )
+                - 1
+            )
+
+        def Es2(c2):
+            return Erho(c2 * sps.norm.ppf(3 / 4))
+
+        if consistency == "finiteSample":
+            nEs2 = (n - 2) * Es2(c2)
+        else:
+            nEs2 = n * Es2(c2)
+    else:
+        nEs2 = n
+    return np.array(sigma0 * np.sqrt(summ(rho) / nEs2)).reshape((p,))
+
+
+def scale_data(X, m, s):
+    """
+    Column-wise data scaling on location and scale estimates.
+
+    """
+
+    n = X.shape
+    if len(n) > 1:
+        p = n[1]
+    else:
+        p = 1
+    n = n[0]
+
+    s = _handle_zeros_in_scale(s)
+
+    if p == 1:
+        Xm = X - float(m)
+        Xs = Xm / s
+    else:
+        Xm = X - np.array([m for i in range(1, n + 1)])
+        Xs = Xm / np.array([s for i in range(1, n + 1)])
+    return Xs
+
+
+def wrap_univ(dd, scale=False, locX=None, scaleX=None):
+    """
+    # Computes the univariate wrapping transformation
+    # Reference: Jakob Raymaekers & Peter J. Rousseeuw (2021)
+    # Fast Robust Correlation for High-Dimensional Data,
+    # Technometrics, 63:2, 184-198.
+    args:
+        dd, np.array: vector of distances
+        scale, bool: if True, will scale data about med/mad
+    returns:
+        xi, np.array: wrapped vector
+    """
+    b = 1.5
+    c = 4
+    q1 = 1.540793
+    q2 = 0.8622731
+    if dd.dtype == "O":
+        dd = dd.astype("float")
+    if scale:
+        locX = median(dd)
+        scaleX = mad(dd)
+        xi = (dd - locX) / scaleX
+    else:
+        xi = np.array(dd)
+    indMid = np.where((np.abs(xi) < c) & (np.abs(xi) >= b))[0]
+    indHigh = np.where(np.abs(xi) >= c)[0]
+    xi[indMid] = (
+        q1
+        * np.tanh(q2 * (c - np.abs(xi[indMid])))
+        * np.abs(xi[indMid])
+        / xi[indMid]
+    )
+    xi[indHigh] = 0
+    xi = xi * scaleX + locX
+
+    return xi
+
+
+def wrap(X, locX, scaleX):
+    """
+    wrap - wrap matrix column wise
+    """
+
+    if len(X.shape) == 1:
+        X = X.reshape(-1, 1)
+
+    return np.array(
+        [
+            wrap_univ(X[:, i], locX=locX[i], scale=True, scaleX=scaleX[i])
+            for i in range(X.shape[1])
+        ]
+    ).transpose()
```

## direpack/preprocessing/gsspp.py

 * *Ordering differences only*

```diff
@@ -1,161 +1,161 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Created on Wed Mar 25 09:01:53 2020
-
-# @author: Sven Serneels, Ponalytics, Mar 2020. 
-
-
-__all__ = ['GenSpatialSignPrePprocessor','gen_ss_pp','gen_ss_covmat']
-
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted
-from .robcent import VersatileScaler, versatile_scale
-from ._preproc_utilities import *
-from ..utils.utils import _check_input
-from ._gsspp_utils import *
-from ._gsspp_utils import _norms, _gsspp
-
-__all__ = ['GenSpatialSignPreProcessor', 'gen_ss_covmat', 'gen_ss_pp']
-    
-class GenSpatialSignPreProcessor(TransformerMixin,BaseEstimator):
-    
-    """
-    GenSpatialSignPreProcessor Generalized Spatial Sign Pre-Processing as a scikit-learn compatible object
-    that can be used in ML pipelines. 
-    
-    Parameters
-    ---------- 
-        center: str or function, 
-            location estimator for centring.str options: 'mean', 'median', 'l1median', 'kstepLTS', 'None' 
-
-        fun: str or function, 
-            radial transformation function, str options: 'ss' (the non-generalized spatial sign, equivalent to sklearn's Normalizer), 'ball', 'shell', 'quad' (quadratic), 'winsor', or 'linear_redescending'
-            Methods: sklearn API: `fit(X)`, `transform(X)` and `fit_transform(X)` with 
-
-        
-    Attributes
-    ---------- 
-    Attributes always provided : 
-
-        -  `gss_` : the generalized spatial signs 
-        -  `Xm_` : the centred data 
-        -  `centring_` : VersatileScaler centring object 
-        -  `X_gss_pp_` : Data preprocessed by Generalized Spatial Sign
-    """    
-
-    def __init__(self,center='l1median',fun='linear_redescending'):
-        
-        self.center = center
-        self.fun = fun 
-        
-    def fit(self,X): 
-        
-        """ 
-        Calculate and store generalized spatial signs
-        """
-        
-        X = _check_input(X)
-        n,p = X.shape
-        if type(self.fun) is str: 
-            fun = eval(self.fun)
-        else:
-            fun = self.fun
-        vs = VersatileScaler(center=self.center,scale='None')
-        Xm = vs.fit_transform(X)
-        gss_ = fun(_norms(Xm),p,n)
-        setattr(self,'gss_',gss_)
-        setattr(self,'Xm_',Xm)
-        setattr(self,'centring_',vs)
-        
-    def transform(self,X):
-        
-        """
-        Calculate Generalized Spatial Sign Pre-Pprocessed Data
-        """
-        
-        check_is_fitted(self,('gss_','Xm_'))
-        Xgss = np.multiply(self.Xm_,self.gss_)
-        setattr(self,'X_gsspp_',Xgss)
-        return(Xgss)
-
-    def fit_transform(self,X): 
-
-        self.fit(X)
-        self.transform(X)
-        return(self.X_gsspp_)        
-        
-    
-    
-
-def gen_ss_pp(X,center='l1median',fun='linear_redescending'):
-    
-    """
-    Generalized Spatial Sign Pre-Processing as a one pass function
-    Inputs: 
-        X: Data matrix 
-        center: str or function, location estimator for centring.
-                str options: 'mean', 'median', 'l1median', 'kstepLTS', 'None' 
-        fun: str or function, radial transformation function,
-                str options: 'ss' (the non-generalized spatial sign, equivalent
-                to sklearn's Normalizer), 'ball', 'shell', 'quad' (quadratic), 
-                'winsor', or 'linear_redescending'
-    Outputs: the pre-processed data 
-    """    
-    
-    if type(center) is str: 
-        center = eval(center)
-            
-    if type(fun) is str: 
-        fun = eval(fun)
-    
-    X = _check_input(X)        
-    n = X.shape
-    if len(n) > 1:
-        p = n[1]
-    else:
-        p = 1
-    n = n[0]
-    
-    if center != 'None':
-        X = versatile_scale(X,center=center,scale='None')
-        
-    return(_gsspp(X,p,n,fun=fun))
-    
-    
-def gen_ss_covmat(X,center='kstepLTS',fun=linear_redescending): 
-    
-    """
-    Generalized Spatial Sign Covariance Matrix 
-    Is equivalent to the covariance matrix of generalized spatial sign 
-    pre-processed data. 
-    
-    First published in: 
-        A generalized spatial sign covariance matrix,
-        Jakob Raymaekers, Peter Rousseeuw, 
-        Journal of Multivariate Analysis, 171 (2019), 94–111.
-        
-    Inputs: 
-        X: Data matrix 
-        center: str or function, location estimator for centring.
-                str options: 'mean', 'median', 'l1median', 'kstepLTS', 'None' 
-        fun: str or function, radial transformation function,
-                str options: 'ss' (the non-generalized spatial sign, equivalent
-                to sklearn's Normalizer), 'ball', 'shell', 'quad' (quadratic), 
-                'winsor', or 'linear_redescending'
-                
-    Outputs: the generalized spatial sign covariance matrix
-    """    
-    
-    X = _check_input(X)  
-    rc = VersatileScaler(center=center, scale='None')
-    n,p = X.shape
-    Xm = rc.fit_transform(X)
-    Xgss = _gsspp(Xm,p,n,fun=fun)
-    return(Xgss.T*Xgss/n)
-    
-    
-
-    
-    
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+# Created on Wed Mar 25 09:01:53 2020
+
+# @author: Sven Serneels, Ponalytics, Mar 2020. 
+
+
+__all__ = ['GenSpatialSignPrePprocessor','gen_ss_pp','gen_ss_covmat']
+
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted
+from .robcent import VersatileScaler, versatile_scale
+from ._preproc_utilities import *
+from ..utils.utils import _check_input
+from ._gsspp_utils import *
+from ._gsspp_utils import _norms, _gsspp
+
+__all__ = ['GenSpatialSignPreProcessor', 'gen_ss_covmat', 'gen_ss_pp']
+    
+class GenSpatialSignPreProcessor(TransformerMixin,BaseEstimator):
+    
+    """
+    GenSpatialSignPreProcessor Generalized Spatial Sign Pre-Processing as a scikit-learn compatible object
+    that can be used in ML pipelines. 
+    
+    Parameters
+    ---------- 
+        center: str or function, 
+            location estimator for centring.str options: 'mean', 'median', 'l1median', 'kstepLTS', 'None' 
+
+        fun: str or function, 
+            radial transformation function, str options: 'ss' (the non-generalized spatial sign, equivalent to sklearn's Normalizer), 'ball', 'shell', 'quad' (quadratic), 'winsor', or 'linear_redescending'
+            Methods: sklearn API: `fit(X)`, `transform(X)` and `fit_transform(X)` with 
+
+        
+    Attributes
+    ---------- 
+    Attributes always provided : 
+
+        -  `gss_` : the generalized spatial signs 
+        -  `Xm_` : the centred data 
+        -  `centring_` : VersatileScaler centring object 
+        -  `X_gss_pp_` : Data preprocessed by Generalized Spatial Sign
+    """    
+
+    def __init__(self,center='l1median',fun='linear_redescending'):
+        
+        self.center = center
+        self.fun = fun 
+        
+    def fit(self,X): 
+        
+        """ 
+        Calculate and store generalized spatial signs
+        """
+        
+        X = _check_input(X)
+        n,p = X.shape
+        if type(self.fun) is str: 
+            fun = eval(self.fun)
+        else:
+            fun = self.fun
+        vs = VersatileScaler(center=self.center,scale='None')
+        Xm = vs.fit_transform(X)
+        gss_ = fun(_norms(Xm),p,n)
+        setattr(self,'gss_',gss_)
+        setattr(self,'Xm_',Xm)
+        setattr(self,'centring_',vs)
+        
+    def transform(self,X):
+        
+        """
+        Calculate Generalized Spatial Sign Pre-Pprocessed Data
+        """
+        
+        check_is_fitted(self,('gss_','Xm_'))
+        Xgss = np.multiply(self.Xm_,self.gss_)
+        setattr(self,'X_gsspp_',Xgss)
+        return(Xgss)
+
+    def fit_transform(self,X): 
+
+        self.fit(X)
+        self.transform(X)
+        return(self.X_gsspp_)        
+        
+    
+    
+
+def gen_ss_pp(X,center='l1median',fun='linear_redescending'):
+    
+    """
+    Generalized Spatial Sign Pre-Processing as a one pass function
+    Inputs: 
+        X: Data matrix 
+        center: str or function, location estimator for centring.
+                str options: 'mean', 'median', 'l1median', 'kstepLTS', 'None' 
+        fun: str or function, radial transformation function,
+                str options: 'ss' (the non-generalized spatial sign, equivalent
+                to sklearn's Normalizer), 'ball', 'shell', 'quad' (quadratic), 
+                'winsor', or 'linear_redescending'
+    Outputs: the pre-processed data 
+    """    
+    
+    if type(center) is str: 
+        center = eval(center)
+            
+    if type(fun) is str: 
+        fun = eval(fun)
+    
+    X = _check_input(X)        
+    n = X.shape
+    if len(n) > 1:
+        p = n[1]
+    else:
+        p = 1
+    n = n[0]
+    
+    if center != 'None':
+        X = versatile_scale(X,center=center,scale='None')
+        
+    return(_gsspp(X,p,n,fun=fun))
+    
+    
+def gen_ss_covmat(X,center='kstepLTS',fun=linear_redescending): 
+    
+    """
+    Generalized Spatial Sign Covariance Matrix 
+    Is equivalent to the covariance matrix of generalized spatial sign 
+    pre-processed data. 
+    
+    First published in: 
+        A generalized spatial sign covariance matrix,
+        Jakob Raymaekers, Peter Rousseeuw, 
+        Journal of Multivariate Analysis, 171 (2019), 94–111.
+        
+    Inputs: 
+        X: Data matrix 
+        center: str or function, location estimator for centring.
+                str options: 'mean', 'median', 'l1median', 'kstepLTS', 'None' 
+        fun: str or function, radial transformation function,
+                str options: 'ss' (the non-generalized spatial sign, equivalent
+                to sklearn's Normalizer), 'ball', 'shell', 'quad' (quadratic), 
+                'winsor', or 'linear_redescending'
+                
+    Outputs: the generalized spatial sign covariance matrix
+    """    
+    
+    X = _check_input(X)  
+    rc = VersatileScaler(center=center, scale='None')
+    n,p = X.shape
+    Xm = rc.fit_transform(X)
+    Xgss = _gsspp(Xm,p,n,fun=fun)
+    return(Xgss.T*Xgss/n)
+    
+    
+
+    
+
```

## direpack/preprocessing/robcent.py

 * *Ordering differences only*

```diff
@@ -1,325 +1,325 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# @author: Sven Serneels, Ponalytics
-# Created on Sun Feb 4 2018
-# Updated on Sun Dec 16 2018
-# Refactored on Sat Dec 21 2019
-# Refactored on Sat Mar 28 2020
-
-
-# Class for classical and robust centering and scaling of input data for
-# regression and machine learning
-
-# Version 2.0: Code entirely restructured compared to version 1.0.
-# Code made consistent with sklearn logic: fit(data,params) yields results.
-# Code makes more effciient use of numpy builtin estimators.
-# Version 3.0:
-# Code now takes strings or functions as input to centring and scaling.
-# Utility functions have been moved to _preproc_utilities.py
-# Code now supplied for l1median cetring, with options to use different
-# scipy.optimize optimization algorithms
-# Version 4.0:
-# Made the API compatible for ScikitLearn pipelines. However, some nonstandard
-# functions and output remain for backwards compatibility. Functionality for
-# sparse matrices still has to be implemented.
-
-
-# Ancillary functions in _preproc_utilities.py:
-
-#         -   `scale_data(X,m,s)`: centers and scales X on center m (as vector) and scale s (as vector).
-#         -   `mean(X,trimming)`: Column-wise mean.
-#         -   `median(X)`: Column-wise median.
-#         -   `l1median(X)`: L1 or spatial median. Optional arguments:
-#         -   `x0`: starting point for optimization, defaults to column wise median
-#         -   `method`: optimization algorithm, defaults to 'SLSQP'
-#         -   `tol`: tolerance, defaults to 1e-8
-#         -   `options`: list of options for `scipy.optimize.minimize`
-#         -   `kstepLTS(X): k-step LTS estimator of location.
-#         -   `maxit`: int, number of iterations to compute maximally
-#         -   `tol`: float, tolerance for convergence
-#         -   `std(X,trimming)`: Column-wise std.
-#         -   `mad(X,c)`: Column-wise median absolute deviation, with consistency factor c.
-#         -   `scaleTau2(x0, c1 = 4.5, c2 = 3, consistency = True)`: Tau estimator of scale
-#             with consistency parameters c1 and c2 and option for consistency correction
-#             (True, False or 'finiteSample')
-
-
-from __future__ import absolute_import, division, print_function
-from __future__ import unicode_literals
-
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.metaestimators import _BaseComposition
-from sklearn.utils.validation import check_is_fitted
-import numpy as np
-from ..utils.utils import (
-    MyException,
-    convert_X_input,
-    convert_y_input,
-    _check_input,
-)
-from ._preproc_utilities import *
-from ._preproc_utilities import _check_trimming, wrap_univ
-
-__all__ = ["VersatileScaler", "robcent", "versatile_scale", "Wrapper", "wrap"]
-
-
-class VersatileScaler(_BaseComposition, TransformerMixin, BaseEstimator):
-
-    """
-    VersatileScaler Center and Scale data about classical or robust location and scale estimates
-
-    Parameters
-    ----------
-        `center`: str or callable, location estimator. String has to be name of the
-                function to be used, or 'None'.
-        `scale`: str or callable, scale estimator
-        `trimming`: trimming percentage to be used in location and scale estimation.
-
-
-    Attributes
-    ----------
-    Arguments for methods:
-        -   `X`: array-like, n x p, the data.
-        -   `trimming`: float, fraction to be trimmed (must be in (0,1)).
-
-
-
-
-    Remarks
-    -------
-    Options for classical estimators 'mean' and 'std' also give access to robust
-    trimmed versions.
-
-    """
-
-    def __init__(self, center="mean", scale="std", trimming=0):
-        """
-        Initialize values. Check if correct options provided.
-        """
-
-        self.center = center
-        self.scale = scale
-        self.trimming = trimming
-
-    def fit(self, X):
-        """
-        Estimate location and scale, store these in the class object.
-        Trimming fraction can be provided as keyword argument.
-        """
-
-        X = _check_input(X)
-
-        _check_trimming(self.trimming)
-
-        if type(self.center) is str:
-            center = eval(self.center)
-        else:
-            center = self.center
-
-        if type(self.scale) is str:
-            scale = eval(self.scale)
-        else:
-            scale = self.scale
-
-        n = X.shape
-        if len(n) > 1:
-            p = n[1]
-        else:
-            p = 1
-        n = n[0]
-
-        if self.center == "None":
-            m = np.repeat(0, p)
-        else:
-            m = center(X, trimming=self.trimming)
-
-        # Keeping col_loc_ for older version compatibility
-        setattr(self, "col_loc_", m)
-        # sklearn standard
-        setattr(self, "center_", m)
-
-        if self.scale == "None":
-            s = np.repeat(1, p)
-        else:
-            s = scale(X, trimming=self.trimming)
-
-        # Keeping col_sca_ for older version compatibility
-        setattr(self, "col_sca_", s)
-        # sklearn standard
-        setattr(self, "scale_", s)
-
-    def transform(self, X):
-        """
-        Center and/or scale training data to pre-estimated location and scale
-        """
-
-        X = _check_input(X)
-        check_is_fitted(self, ["center_", "scale_"])
-
-        Xs = scale_data(X, self.center_, self.scale_)
-        setattr(self, "datas_", Xs)
-
-        return Xs
-
-    def predict(self, Xn):
-        """
-        Standardize new data on previously estimated location and scale.
-        Number of columns needs to match.
-        """
-
-        Xn = _check_input(Xn)
-        Xns = scale_data(Xn, self.col_loc_, self.col_sca_)
-        setattr(self, "datans_", Xns)
-
-        return Xns
-
-    def fit_transform(self, X):
-        """
-        Estimate center and scale for training data and scale these data
-        """
-
-        self.fit(X)
-        self.transform(X)
-
-        return self.datas_
-
-    def inverse_transform(self, Xs=None):
-        """
-        Transform scaled data back to their original scale
-        """
-
-        check_is_fitted(self, ["center_", "scale_"])
-        if Xs is not None:
-            Xs = _check_input(Xs)
-        else:
-            Xs = self.datas_
-
-        return np.multiply(Xs, self.scale_) + self.center_
-
-
-# For backwards compatibility
-robcent = VersatileScaler
-
-
-def versatile_scale(X, center="l1median", scale="mad", trimming=0):
-    """
-    Wrapper to scale based on present robcent implementation that uses
-    `fit` instead of `transform`
-    """
-
-    rc = VersatileScaler(center=center, scale=scale, trimming=trimming)
-    return rc.fit_transform(X)
-
-
-class Wrapper(_BaseComposition, TransformerMixin, BaseEstimator):
-
-    """
-    Wrapper Perform robustness inducing 'wrapping' transformation using
-    optimal plugins and parameters from the literature
-
-    Parameters
-    ----------
-
-
-    Attributes
-    ----------
-    Arguments for methods:
-        -   `X`: array-like, n x p, the data.
-
-    Reference
-    ---------
-    Jakob Raymaekers & Peter J. Rousseeuw (2021), Fast Robust Correlation for
-    High-Dimensional Data, Technometrics, 63:2, 184-198.
-
-    """
-
-    def __init__(self):
-        """
-        Initialize values. Check if correct options provided.
-        """
-
-        self.center = "median"
-        self.scale = "mad"
-        self.trimming = 0
-
-    def fit(self, X):
-        """
-        Estimate location and scale, store these in the class object.
-        Trimming fraction can be provided as keyword argument.
-        """
-
-        X = _check_input(X)
-
-        _check_trimming(self.trimming)
-
-        if type(self.center) is str:
-            center = eval(self.center)
-        else:
-            center = self.center
-
-        if type(self.scale) is str:
-            scale = eval(self.scale)
-        else:
-            scale = self.scale
-
-        n = X.shape
-        if len(n) > 1:
-            p = n[1]
-        else:
-            p = 1
-        n = n[0]
-
-        if self.center == "None":
-            m = np.repeat(0, p)
-        else:
-            m = center(X, trimming=self.trimming)
-
-        # Keeping col_loc_ for older version compatibility
-        setattr(self, "col_loc_", m)
-        # sklearn standard
-        setattr(self, "center_", m)
-
-        if self.scale == "None":
-            s = np.repeat(1, p)
-        else:
-            s = scale(X, trimming=self.trimming)
-
-        # Keeping col_sca_ for older version compatibility
-        setattr(self, "col_sca_", s)
-        # sklearn standard
-        setattr(self, "scale_", s)
-
-    def transform(self, X):
-        """
-        Project data points to their wrapped counterparts
-        """
-
-        X = _check_input(X)
-        check_is_fitted(self, ["center_", "scale_"])
-
-        Xw = wrap(X, self.center_, self.scale_)
-        setattr(self, "dataw_", Xw)
-
-        return Xw
-
-    def predict(self, Xn):
-        """
-        Wrap new data using previously estimated location and scale.
-        Number of columns needs to match.
-        """
-
-        Xn = _check_input(Xn)
-        Xnw = wrap(Xn, self.col_loc_, self.col_sca_)
-        setattr(self, "datanw_", Xnw)
-
-        return Xnw
-
-    def fit_transform(self, X):
-        """
-        Estimate center and scale for training data wrap these data
-        """
-
-        self.fit(X)
-        self.transform(X)
-
-        return self.dataw_
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+# @author: Sven Serneels, Ponalytics
+# Created on Sun Feb 4 2018
+# Updated on Sun Dec 16 2018
+# Refactored on Sat Dec 21 2019
+# Refactored on Sat Mar 28 2020
+
+
+# Class for classical and robust centering and scaling of input data for
+# regression and machine learning
+
+# Version 2.0: Code entirely restructured compared to version 1.0.
+# Code made consistent with sklearn logic: fit(data,params) yields results.
+# Code makes more effciient use of numpy builtin estimators.
+# Version 3.0:
+# Code now takes strings or functions as input to centring and scaling.
+# Utility functions have been moved to _preproc_utilities.py
+# Code now supplied for l1median cetring, with options to use different
+# scipy.optimize optimization algorithms
+# Version 4.0:
+# Made the API compatible for ScikitLearn pipelines. However, some nonstandard
+# functions and output remain for backwards compatibility. Functionality for
+# sparse matrices still has to be implemented.
+
+
+# Ancillary functions in _preproc_utilities.py:
+
+#         -   `scale_data(X,m,s)`: centers and scales X on center m (as vector) and scale s (as vector).
+#         -   `mean(X,trimming)`: Column-wise mean.
+#         -   `median(X)`: Column-wise median.
+#         -   `l1median(X)`: L1 or spatial median. Optional arguments:
+#         -   `x0`: starting point for optimization, defaults to column wise median
+#         -   `method`: optimization algorithm, defaults to 'SLSQP'
+#         -   `tol`: tolerance, defaults to 1e-8
+#         -   `options`: list of options for `scipy.optimize.minimize`
+#         -   `kstepLTS(X): k-step LTS estimator of location.
+#         -   `maxit`: int, number of iterations to compute maximally
+#         -   `tol`: float, tolerance for convergence
+#         -   `std(X,trimming)`: Column-wise std.
+#         -   `mad(X,c)`: Column-wise median absolute deviation, with consistency factor c.
+#         -   `scaleTau2(x0, c1 = 4.5, c2 = 3, consistency = True)`: Tau estimator of scale
+#             with consistency parameters c1 and c2 and option for consistency correction
+#             (True, False or 'finiteSample')
+
+
+from __future__ import absolute_import, division, print_function
+from __future__ import unicode_literals
+
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.metaestimators import _BaseComposition
+from sklearn.utils.validation import check_is_fitted
+import numpy as np
+from ..utils.utils import (
+    MyException,
+    convert_X_input,
+    convert_y_input,
+    _check_input,
+)
+from ._preproc_utilities import *
+from ._preproc_utilities import _check_trimming, wrap_univ
+
+__all__ = ["VersatileScaler", "robcent", "versatile_scale", "Wrapper", "wrap"]
+
+
+class VersatileScaler(_BaseComposition, TransformerMixin, BaseEstimator):
+
+    """
+    VersatileScaler Center and Scale data about classical or robust location and scale estimates
+
+    Parameters
+    ----------
+        `center`: str or callable, location estimator. String has to be name of the
+                function to be used, or 'None'.
+        `scale`: str or callable, scale estimator
+        `trimming`: trimming percentage to be used in location and scale estimation.
+
+
+    Attributes
+    ----------
+    Arguments for methods:
+        -   `X`: array-like, n x p, the data.
+        -   `trimming`: float, fraction to be trimmed (must be in (0,1)).
+
+
+
+
+    Remarks
+    -------
+    Options for classical estimators 'mean' and 'std' also give access to robust
+    trimmed versions.
+
+    """
+
+    def __init__(self, center="mean", scale="std", trimming=0):
+        """
+        Initialize values. Check if correct options provided.
+        """
+
+        self.center = center
+        self.scale = scale
+        self.trimming = trimming
+
+    def fit(self, X):
+        """
+        Estimate location and scale, store these in the class object.
+        Trimming fraction can be provided as keyword argument.
+        """
+
+        X = _check_input(X)
+
+        _check_trimming(self.trimming)
+
+        if type(self.center) is str:
+            center = eval(self.center)
+        else:
+            center = self.center
+
+        if type(self.scale) is str:
+            scale = eval(self.scale)
+        else:
+            scale = self.scale
+
+        n = X.shape
+        if len(n) > 1:
+            p = n[1]
+        else:
+            p = 1
+        n = n[0]
+
+        if self.center == "None":
+            m = np.repeat(0, p)
+        else:
+            m = center(X, trimming=self.trimming)
+
+        # Keeping col_loc_ for older version compatibility
+        setattr(self, "col_loc_", m)
+        # sklearn standard
+        setattr(self, "center_", m)
+
+        if self.scale == "None":
+            s = np.repeat(1, p)
+        else:
+            s = scale(X, trimming=self.trimming)
+
+        # Keeping col_sca_ for older version compatibility
+        setattr(self, "col_sca_", s)
+        # sklearn standard
+        setattr(self, "scale_", s)
+
+    def transform(self, X):
+        """
+        Center and/or scale training data to pre-estimated location and scale
+        """
+
+        X = _check_input(X)
+        check_is_fitted(self, ["center_", "scale_"])
+
+        Xs = scale_data(X, self.center_, self.scale_)
+        setattr(self, "datas_", Xs)
+
+        return Xs
+
+    def predict(self, Xn):
+        """
+        Standardize new data on previously estimated location and scale.
+        Number of columns needs to match.
+        """
+
+        Xn = _check_input(Xn)
+        Xns = scale_data(Xn, self.col_loc_, self.col_sca_)
+        setattr(self, "datans_", Xns)
+
+        return Xns
+
+    def fit_transform(self, X):
+        """
+        Estimate center and scale for training data and scale these data
+        """
+
+        self.fit(X)
+        self.transform(X)
+
+        return self.datas_
+
+    def inverse_transform(self, Xs=None):
+        """
+        Transform scaled data back to their original scale
+        """
+
+        check_is_fitted(self, ["center_", "scale_"])
+        if Xs is not None:
+            Xs = _check_input(Xs)
+        else:
+            Xs = self.datas_
+
+        return np.multiply(Xs, self.scale_) + self.center_
+
+
+# For backwards compatibility
+robcent = VersatileScaler
+
+
+def versatile_scale(X, center="l1median", scale="mad", trimming=0):
+    """
+    Wrapper to scale based on present robcent implementation that uses
+    `fit` instead of `transform`
+    """
+
+    rc = VersatileScaler(center=center, scale=scale, trimming=trimming)
+    return rc.fit_transform(X)
+
+
+class Wrapper(_BaseComposition, TransformerMixin, BaseEstimator):
+
+    """
+    Wrapper Perform robustness inducing 'wrapping' transformation using
+    optimal plugins and parameters from the literature
+
+    Parameters
+    ----------
+
+
+    Attributes
+    ----------
+    Arguments for methods:
+        -   `X`: array-like, n x p, the data.
+
+    Reference
+    ---------
+    Jakob Raymaekers & Peter J. Rousseeuw (2021), Fast Robust Correlation for
+    High-Dimensional Data, Technometrics, 63:2, 184-198.
+
+    """
+
+    def __init__(self):
+        """
+        Initialize values. Check if correct options provided.
+        """
+
+        self.center = "median"
+        self.scale = "mad"
+        self.trimming = 0
+
+    def fit(self, X):
+        """
+        Estimate location and scale, store these in the class object.
+        Trimming fraction can be provided as keyword argument.
+        """
+
+        X = _check_input(X)
+
+        _check_trimming(self.trimming)
+
+        if type(self.center) is str:
+            center = eval(self.center)
+        else:
+            center = self.center
+
+        if type(self.scale) is str:
+            scale = eval(self.scale)
+        else:
+            scale = self.scale
+
+        n = X.shape
+        if len(n) > 1:
+            p = n[1]
+        else:
+            p = 1
+        n = n[0]
+
+        if self.center == "None":
+            m = np.repeat(0, p)
+        else:
+            m = center(X, trimming=self.trimming)
+
+        # Keeping col_loc_ for older version compatibility
+        setattr(self, "col_loc_", m)
+        # sklearn standard
+        setattr(self, "center_", m)
+
+        if self.scale == "None":
+            s = np.repeat(1, p)
+        else:
+            s = scale(X, trimming=self.trimming)
+
+        # Keeping col_sca_ for older version compatibility
+        setattr(self, "col_sca_", s)
+        # sklearn standard
+        setattr(self, "scale_", s)
+
+    def transform(self, X):
+        """
+        Project data points to their wrapped counterparts
+        """
+
+        X = _check_input(X)
+        check_is_fitted(self, ["center_", "scale_"])
+
+        Xw = wrap(X, self.center_, self.scale_)
+        setattr(self, "dataw_", Xw)
+
+        return Xw
+
+    def predict(self, Xn):
+        """
+        Wrap new data using previously estimated location and scale.
+        Number of columns needs to match.
+        """
+
+        Xn = _check_input(Xn)
+        Xnw = wrap(Xn, self.col_loc_, self.col_sca_)
+        setattr(self, "datanw_", Xnw)
+
+        return Xnw
+
+    def fit_transform(self, X):
+        """
+        Estimate center and scale for training data wrap these data
+        """
+
+        self.fit(X)
+        self.transform(X)
+
+        return self.dataw_
```

## direpack/sprm/__init__.py

```diff
@@ -1,13 +1,13 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Sun Jul 22 12:17:17 2018
-
-@author: Sven Serneels, Ponalytics
-"""
-
-__name__ = "sprm"
-__author__ = "Sven Serneels"
-__license__ = "MIT"
-__version__ = "0.8.0"
-__date__ = "2024-02-23"
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Sun Jul 22 12:17:17 2018
+
+@author: Sven Serneels, Ponalytics
+"""
+
+__name__ = "sprm"
+__author__ = "Sven Serneels"
+__license__ = "MIT"
+__version__ = "0.8.1"
+__date__ = "2024-05-23"
```

## direpack/sprm/_m_support_functions.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-"""
-Created on Fri Jan 25 18:22:27 2019
-
-Functions called internally in M-estimation 
-
-@author: Sven Serneels, Ponalytics
-"""
-
-import numpy as np
-import pandas as ps
-
-
-def Fair(x, probct, *args):
-    return 1 / (1 + abs(x / (probct * 2))) ** 2
-
-
-def Huber(x, probct, *args):
-    x[np.where(x <= probct)[0]] = 1
-    x[np.where(x > probct)] = probct / abs(x[np.where(x > probct)])
-    return x
-
-
-def Hampel(x, probct, hampelb, hampelr):
-    wx = x
-    wx[np.where(x <= probct)[0]] = 1
-    wx[np.where((x > probct) & (x <= hampelb))[0]] = probct / abs(
-        x[np.where((x > probct) & (x <= hampelb))[0]]
-    )
-    wx[np.where((x > hampelb) & (x <= hampelr))[0]] = np.divide(
-        probct * (hampelr - (x[np.where((x > hampelb) & (x <= hampelr))[0]])),
-        (hampelr - hampelb) * abs(x[np.where((x > hampelb) & (x <= hampelr))[0]]),
-    )
-    wx[np.where(x > hampelr)[0]] = 0
-    return wx
-
-
-def brokenstick(n_components):
-    q = np.triu(np.ones((n_components, n_components)))
-    r = np.empty((n_components, 1), float)
-    r[0:n_components, 0] = range(1, n_components + 1)
-    q = np.matmul(q, 1 / r)
-    q /= n_components
-    return q
+"""
+Created on Fri Jan 25 18:22:27 2019
+
+Functions called internally in M-estimation 
+
+@author: Sven Serneels, Ponalytics
+"""
+
+import numpy as np
+import pandas as ps
+
+
+def Fair(x, probct, *args):
+    return 1 / (1 + abs(x / (probct * 2))) ** 2
+
+
+def Huber(x, probct, *args):
+    x[np.where(x <= probct)[0]] = 1
+    x[np.where(x > probct)] = probct / abs(x[np.where(x > probct)])
+    return x
+
+
+def Hampel(x, probct, hampelb, hampelr):
+    wx = x
+    wx[np.where(x <= probct)[0]] = 1
+    wx[np.where((x > probct) & (x <= hampelb))[0]] = probct / abs(
+        x[np.where((x > probct) & (x <= hampelb))[0]]
+    )
+    wx[np.where((x > hampelb) & (x <= hampelr))[0]] = np.divide(
+        probct * (hampelr - (x[np.where((x > hampelb) & (x <= hampelr))[0]])),
+        (hampelr - hampelb) * abs(x[np.where((x > hampelb) & (x <= hampelr))[0]]),
+    )
+    wx[np.where(x > hampelr)[0]] = 0
+    return wx
+
+
+def brokenstick(n_components):
+    q = np.triu(np.ones((n_components, n_components)))
+    r = np.empty((n_components, 1), float)
+    r[0:n_components, 0] = range(1, n_components + 1)
+    q = np.matmul(q, 1 / r)
+    q /= n_components
+    return q
```

## direpack/sprm/rm.py

 * *Ordering differences only*

```diff
@@ -1,271 +1,271 @@
-"""
-Created on Thu Jan 24 2019
-
-Module containing:
-    
-    Estimators
-    ----------
-    Robust M Regression (RM)
-
-Depends on robcent class for robustly centering and scaling data, as well as on
-the functions in _m_support_functions. 
-
-@author: Sven Serneels, Ponalytics
-"""
-from __future__ import absolute_import, division, print_function
-from __future__ import unicode_literals
-from sklearn.base import RegressorMixin, BaseEstimator
-from sklearn.utils.metaestimators import _BaseComposition
-import copy
-import numpy as np
-import pandas as ps
-from scipy.stats import norm, chi2
-from ..preprocessing.robcent import VersatileScaler
-from ..utils.utils import MyException, _predict_check_input, _check_input
-from ._m_support_functions import *
-
-
-class rm(_BaseComposition, BaseEstimator, RegressorMixin):
-
-    """
-    Robust M Regression 
-    
-    Parameters:
-    -----------
-    fun: str, downweighting function. 'Hampel' (recommended), 'Fair' or 
-                'Huber'
-    probp1: float, probability cutoff for start of downweighting 
-                 (e.g. 0.95)
-    probp2: float, probability cutoff for start of steep downweighting 
-                 (e.g. 0.975, only relevant if fun='Hampel')
-    probp3: float, probability cutoff for start of outlier omission 
-                 (e.g. 0.999, only relevant if fun='Hampel')
-    centre: str, type of centring (`'mean'`, `'median'` or `'l1median'`, 
-            the latter recommended statistically, if too slow, switch to `'median'`)
-    scale: str, type of scaling ('std','mad' [recommended] or 'None')
-    verbose: boolean, specifying verbose mode
-    maxit: int, maximal number of iterations in M algorithm
-    tol: float, tolerance for convergence in M algorithm 
-    start_cutoff_mode: str, values:
-        'specific' will set starting value cutoffs specific to X and y (preferred); 
-        any other value will set X and y stating cutoffs identically. 
-        The latter yields identical results to the SPRM R implementation available from
-        CRAN. 
-    copy (def True): boolean, whether to copy data
-        Note: copy not yet aligned with sklearn def  
-    
-    """
-
-    def __init__(
-        self,
-        fun="Hampel",
-        probp1=0.95,
-        probp2=0.975,
-        probp3=0.999,
-        centre="median",
-        scale="mad",
-        start_cutoff_mode="specific",
-        verbose=True,
-        maxit=100,
-        tol=0.01,
-        copy=True,
-    ):
-        self.fun = fun
-        self.probp1 = probp1
-        self.probp2 = probp2
-        self.probp3 = probp3
-        self.centre = centre
-        self.scale = scale
-        self.start_cutoff_mode = start_cutoff_mode
-        self.verbose = verbose
-        self.maxit = maxit
-        self.tol = tol
-        self.copy = copy
-        self.probctx_ = "irrelevant"
-        self.probcty_ = "irrelevant"
-        self.hampelbx_ = "irrelevant"
-        self.hampelby__ = "irrelevant"
-        self.hampelrx_ = "irrelevant"
-        self.hampelry_ = "irrelevant"
-
-    def fit(self, X, y):
-        if self.copy:
-            self.X = copy.deepcopy(X)
-            self.y = copy.deepcopy(y)
-        (n, p) = X.shape
-        if not (self.fun in ("Hampel", "Huber", "Fair")):
-            raise MyException(
-                "Invalid weighting function. Choose Hampel, Huber or Fair for parameter fun."
-            )
-        if (self.probp1 > 1) | (self.probp1 <= 0):
-            raise MyException("probp1 is a probability. Choose a value between 0 and 1")
-        if self.fun == "Hampel":
-            if not (
-                (self.probp1 < self.probp2)
-                & (self.probp2 < self.probp3)
-                & (self.probp3 <= 1)
-            ):
-                raise MyException(
-                    "Wrong choise of parameters for Hampel function. Use 0<probp1<hampelp2<hampelp3<=1"
-                )
-
-        if type(X) == ps.core.frame.DataFrame:
-            X = X.to_numpy()
-        if type(y) in [ps.core.frame.DataFrame, ps.core.series.Series]:
-            y = y.to_numpy().astype("float64")
-        X = _check_input(X)
-        y = _check_input(y)
-        if len(y.shape) > 1:
-            y = np.array(y).reshape(-1).astype("float64")
-        ny = y.shape[0]
-        if ny != n:
-            raise MyException("Number of cases in y and X must be identical.")
-
-        scaling = VersatileScaler(center=self.centre, scale=self.scale)
-        Xs = scaling.fit_transform(X).astype("float64")
-        mX = scaling.col_loc_
-        sX = scaling.col_sca_
-        ys = scaling.fit_transform(y).astype("float64")
-        my = scaling.col_loc_
-        sy = scaling.col_sca_
-        ys = np.array(ys).reshape(-1)
-
-        wx = np.sqrt(np.array(np.sum(np.square(Xs), 1), dtype=np.float64))
-        wx = wx / np.median(wx)
-        if [self.centre, self.scale] == ["median", "mad"]:
-            wy = np.array(abs(ys), dtype=np.float64)
-        else:
-            wy = (y - np.median(y)) / (1.4826 * np.median(abs(y - np.median(y))))
-        self.probcty_ = norm.ppf(self.probp1)
-        if self.start_cutoff_mode == "specific":
-            self.probctx_ = chi2.ppf(self.probp1, p)
-        else:
-            self.probctx_ = self.probcty_
-        if self.fun == "Fair":
-            wx = Fair(wx, self.probctx_)
-            wy = Fair(wy, self.probcty_)
-        if self.fun == "Huber":
-            wx = Huber(wx, self.probctx_)
-            wy = Huber(wy, self.probcty_)
-        if self.fun == "Hampel":
-            self.hampelby_ = norm.ppf(self.probp2)
-            self.hampelry_ = norm.ppf(self.probp3)
-            if self.start_cutoff_mode == "specific":
-                self.hampelbx_ = chi2.ppf(self.probp2, p)
-                self.hampelrx_ = chi2.ppf(self.probp3, p)
-            else:
-                self.hampelbx_ = self.hampelby_
-                self.hampelrx_ = self.hampelry_
-            wx = Hampel(wx, self.probctx_, self.hampelbx_, self.hampelrx_)
-            wy = Hampel(wy, self.probcty_, self.hampelby_, self.hampelry_)
-        wx = np.array(wx).reshape(-1)
-        w = (wx * wy).astype("float64")
-        if (w < 1e-06).any():
-            w0 = np.where(w < 1e-06)[0]
-            w[w0] = 1e-06
-            we = np.array(w, dtype=np.float64)
-        else:
-            we = np.array(w, dtype=np.float64)
-        wye = wy
-        WEmat = np.array([np.sqrt(we) for i in range(1, p + 1)], ndmin=1).T
-        Xw = np.multiply(Xs, WEmat).astype("float64")
-        yw = ys * np.sqrt(we)
-        loops = 1
-        rold = 1e-5
-        difference = 1
-
-        while (difference > self.tol) & (loops < self.maxit):
-            b = np.linalg.lstsq(Xw, yw, rcond=None)
-            b = np.array(b[0]).reshape(-1, 1)
-            yp = np.dot(Xs, b).reshape(-1)
-            r = ys - yp
-            if len(r) / 2 > np.sum(r == 0):
-                r = abs(r) / (1.4826 * np.median(abs(r)))
-            else:
-                r = abs(r) / (1.4826 * np.median(abs(r[r != 0])))
-            wye = r
-            if self.fun == "Fair":
-                wye = Fair(wye, self.probcty_)
-            if self.fun == "Huber":
-                wye = Huber(wye, self.probcty_)
-            if self.fun == "Hampel":
-                wye = Hampel(wye, self.probcty_, self.hampelby_, self.hampelry_)
-            b2sum = np.sum(np.square(b))
-            difference = abs(b2sum - rold) / rold
-            rold = b2sum
-            we = (wye * wx).astype("float64")
-            w0 = []
-            if any(we < 1e-06):
-                w0 = np.where(we < 1e-06)[0]
-                we[w0] = 1e-06
-                we = np.array(we, dtype=np.float64)
-            if len(w0) >= (n / 2):
-                break
-            WEmat = np.array([np.sqrt(we) for i in range(1, p + 1)], ndmin=1).T
-            Xw = np.multiply(Xs, WEmat).astype("float64")
-            yw = ys * np.sqrt(we)
-            loops += 1
-        if difference > self.maxit:
-            print(
-                "Warning: Method did not converge. The scaled difference between norms of the coefficient vectors is "
-                + str(round(difference, 4))
-            )
-        plotprec = False
-        if plotprec:
-            print(str(loops - 1))
-        w = we
-        w[w0] = 0
-        wx[w0] = 0
-        wy = wye
-        wy[w0] = 0
-        Xrw = np.array(np.multiply(Xs, np.sqrt(WEmat)).astype("float64"))
-        scaling.set_params(scale="None")
-        Xrw = scaling.fit_transform(Xrw)
-        b_rescaled = np.multiply(np.reshape(sy / sX, (p, 1)), b)
-        yp_rescaled = np.matmul(X, b_rescaled).reshape(-1)
-        if self.centre == "mean":
-            intercept = np.mean(y - yp_rescaled)
-        else:
-            intercept = np.median(y - yp_rescaled)
-        yfit = yp_rescaled + intercept
-        if self.scale != "None":
-            if self.centre == "mean":
-                b0 = np.mean(ys.astype("float64") - np.matmul(Xs.astype("float64"), b))
-            else:
-                b0 = np.median(
-                    np.array(ys.astype("float64") - np.matmul(Xs.astype("float64"), b))
-                )
-        else:
-            if self.centre == "mean":
-                ytil = np.array(np.matmul(X, b)).reshape(-1)
-                intercept = np.mean(y - ytil)
-            else:
-                intercept = np.median(y - ytil)
-            b0 = intercept
-        r = y - yfit
-        setattr(self, "coef_", b_rescaled)
-        setattr(self, "intercept_", intercept)
-        setattr(self, "coef_scaled_", b)
-        setattr(self, "intercept_scaled_", b0)
-        setattr(self, "residuals_", r)
-        setattr(self, "fitted_", yfit)
-        setattr(self, "x_caseweights_", wx)
-        setattr(self, "y_caseweights_", wy)
-        setattr(self, "caseweights_", w)
-        setattr(self, "x_loc_", mX)
-        setattr(self, "y_loc_", my)
-        setattr(self, "x_sca_", sX)
-        setattr(self, "y_sca_", sy)
-        setattr(self, "scaling_", scaling)
-        return self
-        pass
-
-    def predict(self, Xn):
-        n, p, Xn = _predict_check_input(Xn)
-        if p != self.X.shape[1]:
-            raise (
-                ValueError(
-                    "New data must have seame number of columns as the ones the model has been trained with"
-                )
-            )
-        return np.matmul(Xn, self.coef_) + self.intercept_
+"""
+Created on Thu Jan 24 2019
+
+Module containing:
+    
+    Estimators
+    ----------
+    Robust M Regression (RM)
+
+Depends on robcent class for robustly centering and scaling data, as well as on
+the functions in _m_support_functions. 
+
+@author: Sven Serneels, Ponalytics
+"""
+from __future__ import absolute_import, division, print_function
+from __future__ import unicode_literals
+from sklearn.base import RegressorMixin, BaseEstimator
+from sklearn.utils.metaestimators import _BaseComposition
+import copy
+import numpy as np
+import pandas as ps
+from scipy.stats import norm, chi2
+from ..preprocessing.robcent import VersatileScaler
+from ..utils.utils import MyException, _predict_check_input, _check_input
+from ._m_support_functions import *
+
+
+class rm(_BaseComposition, BaseEstimator, RegressorMixin):
+
+    """
+    Robust M Regression 
+    
+    Parameters:
+    -----------
+    fun: str, downweighting function. 'Hampel' (recommended), 'Fair' or 
+                'Huber'
+    probp1: float, probability cutoff for start of downweighting 
+                 (e.g. 0.95)
+    probp2: float, probability cutoff for start of steep downweighting 
+                 (e.g. 0.975, only relevant if fun='Hampel')
+    probp3: float, probability cutoff for start of outlier omission 
+                 (e.g. 0.999, only relevant if fun='Hampel')
+    centre: str, type of centring (`'mean'`, `'median'` or `'l1median'`, 
+            the latter recommended statistically, if too slow, switch to `'median'`)
+    scale: str, type of scaling ('std','mad' [recommended] or 'None')
+    verbose: boolean, specifying verbose mode
+    maxit: int, maximal number of iterations in M algorithm
+    tol: float, tolerance for convergence in M algorithm 
+    start_cutoff_mode: str, values:
+        'specific' will set starting value cutoffs specific to X and y (preferred); 
+        any other value will set X and y stating cutoffs identically. 
+        The latter yields identical results to the SPRM R implementation available from
+        CRAN. 
+    copy (def True): boolean, whether to copy data
+        Note: copy not yet aligned with sklearn def  
+    
+    """
+
+    def __init__(
+        self,
+        fun="Hampel",
+        probp1=0.95,
+        probp2=0.975,
+        probp3=0.999,
+        centre="median",
+        scale="mad",
+        start_cutoff_mode="specific",
+        verbose=True,
+        maxit=100,
+        tol=0.01,
+        copy=True,
+    ):
+        self.fun = fun
+        self.probp1 = probp1
+        self.probp2 = probp2
+        self.probp3 = probp3
+        self.centre = centre
+        self.scale = scale
+        self.start_cutoff_mode = start_cutoff_mode
+        self.verbose = verbose
+        self.maxit = maxit
+        self.tol = tol
+        self.copy = copy
+        self.probctx_ = "irrelevant"
+        self.probcty_ = "irrelevant"
+        self.hampelbx_ = "irrelevant"
+        self.hampelby__ = "irrelevant"
+        self.hampelrx_ = "irrelevant"
+        self.hampelry_ = "irrelevant"
+
+    def fit(self, X, y):
+        if self.copy:
+            self.X = copy.deepcopy(X)
+            self.y = copy.deepcopy(y)
+        (n, p) = X.shape
+        if not (self.fun in ("Hampel", "Huber", "Fair")):
+            raise MyException(
+                "Invalid weighting function. Choose Hampel, Huber or Fair for parameter fun."
+            )
+        if (self.probp1 > 1) | (self.probp1 <= 0):
+            raise MyException("probp1 is a probability. Choose a value between 0 and 1")
+        if self.fun == "Hampel":
+            if not (
+                (self.probp1 < self.probp2)
+                & (self.probp2 < self.probp3)
+                & (self.probp3 <= 1)
+            ):
+                raise MyException(
+                    "Wrong choise of parameters for Hampel function. Use 0<probp1<hampelp2<hampelp3<=1"
+                )
+
+        if type(X) == ps.core.frame.DataFrame:
+            X = X.to_numpy()
+        if type(y) in [ps.core.frame.DataFrame, ps.core.series.Series]:
+            y = y.to_numpy().astype("float64")
+        X = _check_input(X)
+        y = _check_input(y)
+        if len(y.shape) > 1:
+            y = np.array(y).reshape(-1).astype("float64")
+        ny = y.shape[0]
+        if ny != n:
+            raise MyException("Number of cases in y and X must be identical.")
+
+        scaling = VersatileScaler(center=self.centre, scale=self.scale)
+        Xs = scaling.fit_transform(X).astype("float64")
+        mX = scaling.col_loc_
+        sX = scaling.col_sca_
+        ys = scaling.fit_transform(y).astype("float64")
+        my = scaling.col_loc_
+        sy = scaling.col_sca_
+        ys = np.array(ys).reshape(-1)
+
+        wx = np.sqrt(np.array(np.sum(np.square(Xs), 1), dtype=np.float64))
+        wx = wx / np.median(wx)
+        if [self.centre, self.scale] == ["median", "mad"]:
+            wy = np.array(abs(ys), dtype=np.float64)
+        else:
+            wy = (y - np.median(y)) / (1.4826 * np.median(abs(y - np.median(y))))
+        self.probcty_ = norm.ppf(self.probp1)
+        if self.start_cutoff_mode == "specific":
+            self.probctx_ = chi2.ppf(self.probp1, p)
+        else:
+            self.probctx_ = self.probcty_
+        if self.fun == "Fair":
+            wx = Fair(wx, self.probctx_)
+            wy = Fair(wy, self.probcty_)
+        if self.fun == "Huber":
+            wx = Huber(wx, self.probctx_)
+            wy = Huber(wy, self.probcty_)
+        if self.fun == "Hampel":
+            self.hampelby_ = norm.ppf(self.probp2)
+            self.hampelry_ = norm.ppf(self.probp3)
+            if self.start_cutoff_mode == "specific":
+                self.hampelbx_ = chi2.ppf(self.probp2, p)
+                self.hampelrx_ = chi2.ppf(self.probp3, p)
+            else:
+                self.hampelbx_ = self.hampelby_
+                self.hampelrx_ = self.hampelry_
+            wx = Hampel(wx, self.probctx_, self.hampelbx_, self.hampelrx_)
+            wy = Hampel(wy, self.probcty_, self.hampelby_, self.hampelry_)
+        wx = np.array(wx).reshape(-1)
+        w = (wx * wy).astype("float64")
+        if (w < 1e-06).any():
+            w0 = np.where(w < 1e-06)[0]
+            w[w0] = 1e-06
+            we = np.array(w, dtype=np.float64)
+        else:
+            we = np.array(w, dtype=np.float64)
+        wye = wy
+        WEmat = np.array([np.sqrt(we) for i in range(1, p + 1)], ndmin=1).T
+        Xw = np.multiply(Xs, WEmat).astype("float64")
+        yw = ys * np.sqrt(we)
+        loops = 1
+        rold = 1e-5
+        difference = 1
+
+        while (difference > self.tol) & (loops < self.maxit):
+            b = np.linalg.lstsq(Xw, yw, rcond=None)
+            b = np.array(b[0]).reshape(-1, 1)
+            yp = np.dot(Xs, b).reshape(-1)
+            r = ys - yp
+            if len(r) / 2 > np.sum(r == 0):
+                r = abs(r) / (1.4826 * np.median(abs(r)))
+            else:
+                r = abs(r) / (1.4826 * np.median(abs(r[r != 0])))
+            wye = r
+            if self.fun == "Fair":
+                wye = Fair(wye, self.probcty_)
+            if self.fun == "Huber":
+                wye = Huber(wye, self.probcty_)
+            if self.fun == "Hampel":
+                wye = Hampel(wye, self.probcty_, self.hampelby_, self.hampelry_)
+            b2sum = np.sum(np.square(b))
+            difference = abs(b2sum - rold) / rold
+            rold = b2sum
+            we = (wye * wx).astype("float64")
+            w0 = []
+            if any(we < 1e-06):
+                w0 = np.where(we < 1e-06)[0]
+                we[w0] = 1e-06
+                we = np.array(we, dtype=np.float64)
+            if len(w0) >= (n / 2):
+                break
+            WEmat = np.array([np.sqrt(we) for i in range(1, p + 1)], ndmin=1).T
+            Xw = np.multiply(Xs, WEmat).astype("float64")
+            yw = ys * np.sqrt(we)
+            loops += 1
+        if difference > self.maxit:
+            print(
+                "Warning: Method did not converge. The scaled difference between norms of the coefficient vectors is "
+                + str(round(difference, 4))
+            )
+        plotprec = False
+        if plotprec:
+            print(str(loops - 1))
+        w = we
+        w[w0] = 0
+        wx[w0] = 0
+        wy = wye
+        wy[w0] = 0
+        Xrw = np.array(np.multiply(Xs, np.sqrt(WEmat)).astype("float64"))
+        scaling.set_params(scale="None")
+        Xrw = scaling.fit_transform(Xrw)
+        b_rescaled = np.multiply(np.reshape(sy / sX, (p, 1)), b)
+        yp_rescaled = np.matmul(X, b_rescaled).reshape(-1)
+        if self.centre == "mean":
+            intercept = np.mean(y - yp_rescaled)
+        else:
+            intercept = np.median(y - yp_rescaled)
+        yfit = yp_rescaled + intercept
+        if self.scale != "None":
+            if self.centre == "mean":
+                b0 = np.mean(ys.astype("float64") - np.matmul(Xs.astype("float64"), b))
+            else:
+                b0 = np.median(
+                    np.array(ys.astype("float64") - np.matmul(Xs.astype("float64"), b))
+                )
+        else:
+            if self.centre == "mean":
+                ytil = np.array(np.matmul(X, b)).reshape(-1)
+                intercept = np.mean(y - ytil)
+            else:
+                intercept = np.median(y - ytil)
+            b0 = intercept
+        r = y - yfit
+        setattr(self, "coef_", b_rescaled)
+        setattr(self, "intercept_", intercept)
+        setattr(self, "coef_scaled_", b)
+        setattr(self, "intercept_scaled_", b0)
+        setattr(self, "residuals_", r)
+        setattr(self, "fitted_", yfit)
+        setattr(self, "x_caseweights_", wx)
+        setattr(self, "y_caseweights_", wy)
+        setattr(self, "caseweights_", w)
+        setattr(self, "x_loc_", mX)
+        setattr(self, "y_loc_", my)
+        setattr(self, "x_sca_", sX)
+        setattr(self, "y_sca_", sy)
+        setattr(self, "scaling_", scaling)
+        return self
+        pass
+
+    def predict(self, Xn):
+        n, p, Xn = _predict_check_input(Xn)
+        if p != self.X.shape[1]:
+            raise (
+                ValueError(
+                    "New data must have seame number of columns as the ones the model has been trained with"
+                )
+            )
+        return np.matmul(Xn, self.coef_) + self.intercept_
```

## direpack/sprm/snipls.py

```diff
@@ -1,321 +1,321 @@
-# Created on Fri Apr 26 19:27:52 2019
-
-# @author: sven
-
-
-from __future__ import absolute_import, division, print_function
-from __future__ import unicode_literals
-from sklearn.base import RegressorMixin, BaseEstimator, TransformerMixin
-from sklearn.utils.metaestimators import _BaseComposition
-import copy
-import numpy as np
-import pandas as ps
-from ..preprocessing.robcent import VersatileScaler
-from ..utils.utils import MyException, _predict_check_input, _check_input, nandot, nanmatdot
-from ..preprocessing._preproc_utilities import scale_data
-
-
-class snipls(_BaseComposition, BaseEstimator, TransformerMixin, RegressorMixin):
-    """
-    SNIPLS Sparse Nipals Algorithm 
-
-    Algorithm first outlined in: 
-        Sparse and robust PLS for binary classification, 
-        I. Hoffmann, P. Filzmoser, S. Serneels, K. Varmuza, 
-        Journal of Chemometrics, 30 (2016), 153-162.
-
-    As of driepack-1.1.2, snipls works when there are missing data in the inputs
-
-    Parameters
-    -----------
-
-    eta : float.
-         Sparsity parameter in [0,1)
-
-    n_components : int,
-                     min 1. Note that if applied on data, n_components shall take a value <= min(x_data.shape)
-
-    verbose: Boolean (def true)
-                to print intermediate set of columns retained
-
-    columns : Either boolean, list, numpy array or pandas Index (def false)
-                if False, no column names supplied; if True, if X data are supplied as a pandas data frame, will extract column names from the frame throws an error for other data input types if a list, array or Index (will only take length x_data.shape[1]), the column names of the x_data supplied in this list, will be printed in verbose mode.
-
-    centre : str, 
-                type of centring (`'mean'` [recommended], `'median'` or `'l1median'`), 
-
-    scale : str,
-             type of scaling ('std','mad' or 'None')
-
-    copy : (def True): boolean,
-             whether to copy data.  Note : copy not yet aligned with sklearn def  - we always copy  
-
-
-    Attributes
-    ------------
-    Attributes always provided:
-
-        -  `x_weights_`: X block PLS weighting vectors (usually denoted W)
-        -  `x_loadings_`: X block PLS loading vectors (usually denoted P)
-        -  `C_`: vector of inner relationship between response and latent variablesblock re
-        -  `x_scores_`: X block PLS score vectors (usually denoted T)
-        -  `coef_`: vector of regression coefficients 
-        -  `intercept_`: intercept
-        -  `coef_scaled_`: vector of scaled regression coeeficients (when scaling option used)
-        -  `intercept_scaled_`: scaled intercept
-        -  `residuals_`: vector of regression residuals
-        -  `x_ev_`: X block explained variance per component
-        -  `y_ev_`: y block explained variance 
-        -  `fitted_`: fitted response
-        -  `x_Rweights_`: X block SIMPLS style weighting vectors (usually denoted R)
-        -  `colret_`: names of variables retained in the sparse model
-        -  `x_loc_`: X block location estimate 
-        -  `y_loc_`: y location estimate
-        -  `x_sca_`: X block scale estimate
-        -  `y_sca_`: y scale estimate
-        -  `centring_`: scaling object used internally (from `VersatileScaler`)
-
-    """
-
-    def __init__(
-        self,
-        eta=0.5,
-        n_components=1,
-        verbose=True,
-        columns=False,
-        centre="mean",
-        scale="None",
-        copy=True,
-    ):
-        assert eta >= 0 and eta < 1,  "eta needs to be in [0,1)"
-        assert isinstance(
-            n_components, int) and n_components > 0, "number of components needs to be positive integer"
-        self.eta = eta
-        self.n_components = n_components
-        self.verbose = verbose
-        self.columns = columns
-        self.centre = centre
-        self.scale = scale
-        self.copy = copy
-
-    def fit(self, X, y):
-        """
-            Fit a SNIPLS model. 
-
-            Parameters
-            ------------ 
-
-                X : numpy array 
-                    Input data.
-
-                y :   vector or 1D matrix
-                    Response data
-
-        """
-        if type(self.columns) is list:
-            self.columns = np.array(self.columns)
-        elif type(self.columns) is bool:
-            if type(X) != ps.core.frame.DataFrame and self.columns:
-                raise (
-                    MyException(
-                        "Columns set to true can only extract column names for data frame input"
-                    )
-                )
-        if type(X) == ps.core.frame.DataFrame:
-            if type(self.columns) is bool and self.columns:
-                self.columns = X.columns
-            X = X.to_numpy()
-        (n, p) = X.shape
-        if type(y) in [ps.core.frame.DataFrame, ps.core.series.Series]:
-            y = y.to_numpy()
-        X = _check_input(X)
-        y = _check_input(y)
-        ny = y.shape[0]
-        if ny != n:
-            if y.ndim == 2:
-                y = y.T
-            else:
-                raise (MyException("Number of cases in X and y needs to agree"))
-        y = y.astype("float64")
-        if self.copy:
-            X0 = copy.deepcopy(X)
-            y0 = copy.deepcopy(y)
-        else:
-            X0 = X
-            y0 = y
-        self.X = X0
-        self.y = y0
-        X0 = X0.astype("float64")
-        centring = VersatileScaler(center=self.centre, scale=self.scale)
-        X0 = centring.fit_transform(X0).astype("float64")
-        mX = centring.col_loc_
-        sX = centring.col_sca_
-        y0 = centring.fit_transform(y0).astype("float64")
-        my = centring.col_loc_
-        sy = centring.col_sca_
-        if np.isnan(X0).any() or np.isnan(y0).any():
-            S = nanmatdot(X0.T, X0)
-            dot = nandot
-        else:
-            S = np.matmul(X0.T, X0)
-            dot = np.dot
-        s0 = dot(X0.T, y0)
-        T = np.empty((n, self.n_components), float)
-        W = np.empty((p, self.n_components), float)
-        P = np.empty((p, self.n_components), float)
-        C = np.empty((self.n_components, 1), float)
-        Xev = np.empty((self.n_components, 1), float)
-        yev = np.empty((self.n_components, 1), float)
-        B = np.empty((p, 1), float)
-        oldgoodies = np.array([])
-        Xi = X0
-        yi = y0
-        for i in range(1, self.n_components + 1):
-            wh = dot(Xi.T, yi)
-            wh = wh / np.linalg.norm(wh, "fro")
-            # goodies = abs(wh)-llambda/2 lambda definition
-            goodies = abs(wh) - self.eta * max(abs(wh))
-            wh = np.multiply(goodies, np.sign(wh))
-            goodies = np.where((goodies > 0))[0]
-            goodies = np.union1d(oldgoodies, goodies)
-            oldgoodies = goodies
-            if len(goodies) == 0:
-                colret = None
-                print(
-                    "No variables retained at"
-                    + str(i)
-                    + "latent variables"
-                    + "and lambda = "
-                    + str(self.eta)
-                    + ", try lower lambda"
-                )
-                break
-            elimvars = np.setdiff1d(range(0, p), goodies)
-            wh[elimvars] = 0
-            th = dot(Xi, wh)
-            nth = np.linalg.norm(th, "fro")
-            ch = dot(yi.T, th) / (nth ** 2)
-            ph = dot(Xi.T, dot(Xi, wh)) / (nth ** 2)
-            Xi = Xi - np.dot(th, ph.T)
-            yi = yi - np.dot(th, ch)
-            ph[elimvars] = 0
-            W[:, i - 1] = np.reshape(wh, p)
-            P[:, i - 1] = np.reshape(ph, p)
-            C[i - 1] = ch
-            T[:, i - 1] = np.reshape(th, n)
-            Xev[i - 1] = (
-                (nth ** 2 * np.linalg.norm(ph, "fro") ** 2)
-                / np.nansum(np.square(X0))
-                * 100
-            )
-            yev[i - 1] = np.nansum(nth ** 2 * (ch ** 2)) / \
-                np.nansum(np.power(y0, 2)) * 100
-            if type(self.columns) == bool:
-                colret = goodies
-            else:
-                colret = self.columns[np.setdiff1d(range(0, p), elimvars)]
-            if self.verbose:
-                print(
-                    "Variables retained for "
-                    + str(i)
-                    + " latent variable(s):"
-                    + "\n"
-                    + str(colret)
-                    + ".\n"
-                )
-        if len(goodies) > 0:
-            R = np.matmul(
-                W[:, range(0, i)],
-                np.linalg.inv(
-                    np.matmul(P[:, range(0, i)].T, W[:, range(0, i)])),
-            )
-            B = np.matmul(
-                W[:, range(0, i)],
-                np.matmul(
-                    np.linalg.inv(
-                        np.matmul(
-                            np.matmul(W[:, range(0, i)].T, S),
-                            W[:, range(0, i)],
-                        )
-                    ),
-                    np.matmul(W[:, range(0, i)].T, s0),
-                ),
-            )
-        else:
-            B = np.empty((p, 1))
-            B.fill(0)
-            R = B
-            T = np.empty((n, self.n_components))
-            T.fill(0)
-        B_rescaled = np.multiply(np.array(sy / sX).reshape((p, 1)), B)
-        yp_rescaled = dot(X, B_rescaled)
-        if self.centre == "mean":
-            intercept = np.nanmean(y - yp_rescaled)
-        elif self.centre == "None":
-            intercept = 0
-        else:
-            intercept = np.nanmedian(y - yp_rescaled)
-        yfit = yp_rescaled + intercept
-        yfit = yfit.reshape(-1)
-        r = y.ravel() - yfit
-        setattr(self, "x_weights_", W)
-        setattr(self, "x_loadings_", P)
-        setattr(self, "C_", C)
-        setattr(self, "x_scores_", T)
-        setattr(self, "coef_", B_rescaled)
-        setattr(self, "coef_scaled_", B)
-        setattr(self, "intercept_", intercept)
-        setattr(self, "x_ev_", Xev)
-        setattr(self, "y_ev_", yev)
-        setattr(self, "fitted_", yfit)
-        setattr(self, "residuals_", r)
-        setattr(self, "x_Rweights_", R)
-        setattr(self, "colret_", colret)
-        setattr(self, "x_loc_", mX)
-        setattr(self, "y_loc_", my)
-        setattr(self, "x_sca_", sX)
-        setattr(self, "y_sca_", sy)
-        setattr(self, "centring_", centring)
-        return self
-
-    def predict(self, Xn):
-        """
-        Predict using a  SNIPLS model. 
-
-        Parameters
-        ------------ 
-
-            Xn : numpy array or data frame 
-                Input data.
-
-        """
-        n, p, Xn = _predict_check_input(Xn)
-        if p != self.X.shape[1]:
-            raise (
-                ValueError(
-                    "New data must have same number of columns as the ones the model has been trained with"
-                )
-            )
-        return np.matmul(Xn, self.coef_) + self.intercept_
-
-    def transform(self, Xn):
-        """
-        Transform input data. 
-
-
-        Parameters
-        ------------ 
-
-            Xn : numpy array or data frame 
-                Input data.
-
-        """
-        n, p, Xn = _predict_check_input(Xn)
-        if p != self.X.shape[1]:
-            raise (
-                ValueError(
-                    "New data must have seame number of columns as the ones the model has been trained with"
-                )
-            )
-        Xnc = scale_data(Xn, self.x_loc_, self.x_sca_)
-        return Xnc * self.x_Rweights_
+# Created on Fri Apr 26 19:27:52 2019
+
+# @author: sven
+
+
+from __future__ import absolute_import, division, print_function
+from __future__ import unicode_literals
+from sklearn.base import RegressorMixin, BaseEstimator, TransformerMixin
+from sklearn.utils.metaestimators import _BaseComposition
+import copy
+import numpy as np
+import pandas as ps
+from ..preprocessing.robcent import VersatileScaler
+from ..utils.utils import MyException, _predict_check_input, _check_input, nandot, nanmatdot
+from ..preprocessing._preproc_utilities import scale_data
+
+
+class snipls(_BaseComposition, BaseEstimator, TransformerMixin, RegressorMixin):
+    """
+    SNIPLS Sparse Nipals Algorithm 
+
+    Algorithm first outlined in: 
+        Sparse and robust PLS for binary classification, 
+        I. Hoffmann, P. Filzmoser, S. Serneels, K. Varmuza, 
+        Journal of Chemometrics, 30 (2016), 153-162.
+
+    As of driepack-1.1.2, snipls works when there are missing data in the inputs
+
+    Parameters
+    -----------
+
+    eta : float.
+         Sparsity parameter in [0,1)
+
+    n_components : int,
+                     min 1. Note that if applied on data, n_components shall take a value <= min(x_data.shape)
+
+    verbose: Boolean (def true)
+                to print intermediate set of columns retained
+
+    columns : Either boolean, list, numpy array or pandas Index (def false)
+                if False, no column names supplied; if True, if X data are supplied as a pandas data frame, will extract column names from the frame throws an error for other data input types if a list, array or Index (will only take length x_data.shape[1]), the column names of the x_data supplied in this list, will be printed in verbose mode.
+
+    centre : str, 
+                type of centring (`'mean'` [recommended], `'median'` or `'l1median'`), 
+
+    scale : str,
+             type of scaling ('std','mad' or 'None')
+
+    copy : (def True): boolean,
+             whether to copy data.  Note : copy not yet aligned with sklearn def  - we always copy  
+
+
+    Attributes
+    ------------
+    Attributes always provided:
+
+        -  `x_weights_`: X block PLS weighting vectors (usually denoted W)
+        -  `x_loadings_`: X block PLS loading vectors (usually denoted P)
+        -  `C_`: vector of inner relationship between response and latent variablesblock re
+        -  `x_scores_`: X block PLS score vectors (usually denoted T)
+        -  `coef_`: vector of regression coefficients 
+        -  `intercept_`: intercept
+        -  `coef_scaled_`: vector of scaled regression coeeficients (when scaling option used)
+        -  `intercept_scaled_`: scaled intercept
+        -  `residuals_`: vector of regression residuals
+        -  `x_ev_`: X block explained variance per component
+        -  `y_ev_`: y block explained variance 
+        -  `fitted_`: fitted response
+        -  `x_Rweights_`: X block SIMPLS style weighting vectors (usually denoted R)
+        -  `colret_`: names of variables retained in the sparse model
+        -  `x_loc_`: X block location estimate 
+        -  `y_loc_`: y location estimate
+        -  `x_sca_`: X block scale estimate
+        -  `y_sca_`: y scale estimate
+        -  `centring_`: scaling object used internally (from `VersatileScaler`)
+
+    """
+
+    def __init__(
+        self,
+        eta=0.5,
+        n_components=1,
+        verbose=True,
+        columns=False,
+        centre="mean",
+        scale="None",
+        copy=True,
+    ):
+        assert eta >= 0 and eta < 1,  "eta needs to be in [0,1)"
+        assert isinstance(
+            n_components, int) and n_components > 0, "number of components needs to be positive integer"
+        self.eta = eta
+        self.n_components = n_components
+        self.verbose = verbose
+        self.columns = columns
+        self.centre = centre
+        self.scale = scale
+        self.copy = copy
+
+    def fit(self, X, y):
+        """
+            Fit a SNIPLS model. 
+
+            Parameters
+            ------------ 
+
+                X : numpy array 
+                    Input data.
+
+                y :   vector or 1D matrix
+                    Response data
+
+        """
+        if type(self.columns) is list:
+            self.columns = np.array(self.columns)
+        elif type(self.columns) is bool:
+            if type(X) != ps.core.frame.DataFrame and self.columns:
+                raise (
+                    MyException(
+                        "Columns set to true can only extract column names for data frame input"
+                    )
+                )
+        if type(X) == ps.core.frame.DataFrame:
+            if type(self.columns) is bool and self.columns:
+                self.columns = X.columns
+            X = X.to_numpy()
+        (n, p) = X.shape
+        if type(y) in [ps.core.frame.DataFrame, ps.core.series.Series]:
+            y = y.to_numpy()
+        X = _check_input(X)
+        y = _check_input(y)
+        ny = y.shape[0]
+        if ny != n:
+            if y.ndim == 2:
+                y = y.T
+            else:
+                raise (MyException("Number of cases in X and y needs to agree"))
+        y = y.astype("float64")
+        if self.copy:
+            X0 = copy.deepcopy(X)
+            y0 = copy.deepcopy(y)
+        else:
+            X0 = X
+            y0 = y
+        self.X = X0
+        self.y = y0
+        X0 = X0.astype("float64")
+        centring = VersatileScaler(center=self.centre, scale=self.scale)
+        X0 = centring.fit_transform(X0).astype("float64")
+        mX = centring.col_loc_
+        sX = centring.col_sca_
+        y0 = centring.fit_transform(y0).astype("float64")
+        my = centring.col_loc_
+        sy = centring.col_sca_
+        if np.isnan(X0).any() or np.isnan(y0).any():
+            S = nanmatdot(X0.T, X0)
+            dot = nandot
+        else:
+            S = np.matmul(X0.T, X0)
+            dot = np.dot
+        s0 = dot(X0.T, y0)
+        T = np.empty((n, self.n_components), float)
+        W = np.empty((p, self.n_components), float)
+        P = np.empty((p, self.n_components), float)
+        C = np.empty((self.n_components, 1), float)
+        Xev = np.empty((self.n_components, 1), float)
+        yev = np.empty((self.n_components, 1), float)
+        B = np.empty((p, 1), float)
+        oldgoodies = np.array([])
+        Xi = X0
+        yi = y0
+        for i in range(1, self.n_components + 1):
+            wh = dot(Xi.T, yi)
+            wh = wh / np.linalg.norm(wh, "fro")
+            # goodies = abs(wh)-llambda/2 lambda definition
+            goodies = abs(wh) - self.eta * max(abs(wh))
+            wh = np.multiply(goodies, np.sign(wh))
+            goodies = np.where((goodies > 0))[0]
+            goodies = np.union1d(oldgoodies, goodies)
+            oldgoodies = goodies
+            if len(goodies) == 0:
+                colret = None
+                print(
+                    "No variables retained at"
+                    + str(i)
+                    + "latent variables"
+                    + "and lambda = "
+                    + str(self.eta)
+                    + ", try lower lambda"
+                )
+                break
+            elimvars = np.setdiff1d(range(0, p), goodies)
+            wh[elimvars] = 0
+            th = dot(Xi, wh)
+            nth = np.linalg.norm(th, "fro")
+            ch = dot(yi.T, th) / (nth ** 2)
+            ph = dot(Xi.T, dot(Xi, wh)) / (nth ** 2)
+            Xi = Xi - np.dot(th, ph.T)
+            yi = yi - np.dot(th, ch)
+            ph[elimvars] = 0
+            W[:, i - 1] = np.reshape(wh, p)
+            P[:, i - 1] = np.reshape(ph, p)
+            C[i - 1] = ch
+            T[:, i - 1] = np.reshape(th, n)
+            Xev[i - 1] = (
+                (nth ** 2 * np.linalg.norm(ph, "fro") ** 2)
+                / np.nansum(np.square(X0))
+                * 100
+            )
+            yev[i - 1] = np.nansum(nth ** 2 * (ch ** 2)) / \
+                np.nansum(np.power(y0, 2)) * 100
+            if type(self.columns) == bool:
+                colret = goodies
+            else:
+                colret = self.columns[np.setdiff1d(range(0, p), elimvars)]
+            if self.verbose:
+                print(
+                    "Variables retained for "
+                    + str(i)
+                    + " latent variable(s):"
+                    + "\n"
+                    + str(colret)
+                    + ".\n"
+                )
+        if len(goodies) > 0:
+            R = np.matmul(
+                W[:, range(0, i)],
+                np.linalg.inv(
+                    np.matmul(P[:, range(0, i)].T, W[:, range(0, i)])),
+            )
+            B = np.matmul(
+                W[:, range(0, i)],
+                np.matmul(
+                    np.linalg.inv(
+                        np.matmul(
+                            np.matmul(W[:, range(0, i)].T, S),
+                            W[:, range(0, i)],
+                        )
+                    ),
+                    np.matmul(W[:, range(0, i)].T, s0),
+                ),
+            )
+        else:
+            B = np.empty((p, 1))
+            B.fill(0)
+            R = B
+            T = np.empty((n, self.n_components))
+            T.fill(0)
+        B_rescaled = np.multiply(np.array(sy / sX).reshape((p, 1)), B)
+        yp_rescaled = dot(X, B_rescaled)
+        if self.centre == "mean":
+            intercept = np.nanmean(y - yp_rescaled)
+        elif self.centre == "None":
+            intercept = 0
+        else:
+            intercept = np.nanmedian(y - yp_rescaled)
+        yfit = yp_rescaled + intercept
+        yfit = yfit.reshape(-1)
+        r = y.ravel() - yfit
+        setattr(self, "x_weights_", W)
+        setattr(self, "x_loadings_", P)
+        setattr(self, "C_", C)
+        setattr(self, "x_scores_", T)
+        setattr(self, "coef_", B_rescaled)
+        setattr(self, "coef_scaled_", B)
+        setattr(self, "intercept_", intercept)
+        setattr(self, "x_ev_", Xev)
+        setattr(self, "y_ev_", yev)
+        setattr(self, "fitted_", yfit)
+        setattr(self, "residuals_", r)
+        setattr(self, "x_Rweights_", R)
+        setattr(self, "colret_", colret)
+        setattr(self, "x_loc_", mX)
+        setattr(self, "y_loc_", my)
+        setattr(self, "x_sca_", sX)
+        setattr(self, "y_sca_", sy)
+        setattr(self, "centring_", centring)
+        return self
+
+    def predict(self, Xn):
+        """
+        Predict using a  SNIPLS model. 
+
+        Parameters
+        ------------ 
+
+            Xn : numpy array or data frame 
+                Input data.
+
+        """
+        n, p, Xn = _predict_check_input(Xn)
+        if p != self.X.shape[1]:
+            raise (
+                ValueError(
+                    "New data must have same number of columns as the ones the model has been trained with"
+                )
+            )
+        return np.matmul(Xn, self.coef_) + self.intercept_
+
+    def transform(self, Xn):
+        """
+        Transform input data. 
+
+
+        Parameters
+        ------------ 
+
+            Xn : numpy array or data frame 
+                Input data.
+
+        """
+        n, p, Xn = _predict_check_input(Xn)
+        if p != self.X.shape[1]:
+            raise (
+                ValueError(
+                    "New data must have seame number of columns as the ones the model has been trained with"
+                )
+            )
+        Xnc = scale_data(Xn, self.x_loc_, self.x_sca_)
+        return np.dot(Xnc, self.x_Rweights_)
```

## direpack/sprm/sprm.py

 * *Ordering differences only*

```diff
@@ -1,555 +1,555 @@
-from __future__ import absolute_import, division, print_function
-from __future__ import unicode_literals
-from sklearn.base import RegressorMixin, BaseEstimator, TransformerMixin
-from sklearn.utils.metaestimators import _BaseComposition
-from scipy.stats import norm, chi2
-import copy
-import numpy as np
-import pandas as ps
-import warnings
-from ..preprocessing.robcent import VersatileScaler
-from .snipls import snipls
-from ..utils.utils import MyException, _predict_check_input, _check_input
-from ._m_support_functions import *
-from ..preprocessing._preproc_utilities import scale_data, mad
-
-
-class sprm(_BaseComposition, BaseEstimator, TransformerMixin, RegressorMixin):
-
-    """
-    SPRM Sparse Partial Robust M Regression 
-
-    Algorithm first outlined in: 
-        Sparse partial robust M regression, 
-        Irene Hoffmann, Sven Serneels, Peter Filzmoser, Christophe Croux, 
-        Chemometrics and Intelligent Laboratory Systems, 149 (2015), 50-59. 
-
-    Parameters
-    -----------
-
-    eta : float.
-          Sparsity parameter in [0,1)
-
-    n_components : int
-                     min 1. Note that if applied on data, n_components shall take a value <= min(x_data.shape)
-
-    fun : str
-         downweighting function. 'Hampel' (recommended), 'Fair' or  'Huber'
-
-    probp1 : float
-             probability cutoff for start of downweighting (e.g. 0.95)
-
-    probp2 : float
-            probability cutoff for start of steep downweighting (e.g. 0.975, only relevant if fun='Hampel')
-
-    probp3 : float
-            probability cutoff for start of outlier omission (e.g. 0.999, only relevant if fun='Hampel')
-
-    centre : str
-            type of centring (`'mean'`, `'median'`, `'l1median'`, or `'kstepLTS'`, 
-            the latter recommended statistically, if too slow, switch to `'median'`)
-
-    scale : str
-            type of scaling ('std','mad', 'scaleTau2' [recommended] or 'None')
-
-    verbose : booleans
-             specifying verbose mode
-
-    maxit : int
-            maximal number of iterations in M algorithm
-
-    tol : float
-         tolerance for convergence in M algorithm 
-
-    start_cutoff_mode : str,
-                    values:'specific' will set starting value cutoffs specific to X and y (preferred); any other value will set X and y stating cutoffs identically. The latter yields identical results to the SPRM R implementation available from CRAN.
-    start_X_init: str,
-                 values: 'pcapp' will include a PCA/broken stick projection to  calculate the staring weights, else just based on X; any other value will calculate the X starting values based on the X matrix itself. This is less stable for very flat data (p >> n), yet yields identical results to the SPRM R implementation available from CRAN.   
-
-    columns : (def false) Either boolean, list, numpy array or pandas Index
-                if False, no column names supplied; if True, if X data are supplied as a pandas data frame, will extract column names from the frame throws an error for other data input types if a list, array or Index (will only take length x_data.shape[1]), the column names of the x_data supplied in this list, will be printed in verbose mode. 
-
-    copy : (def True) boolean, whether to copy data
-
-    Attributes
-    ---------------
-    Attributes always provided 
-
-        -  `x_weights_`: X block PLS weighting vectors (usually denoted W)
-        -  `x_loadings_`: X block PLS loading vectors (usually denoted P)
-        -  `C_`: vector of inner relationship between response and latent variablesblock re
-        -  `x_scores_`: X block PLS score vectors (usually denoted T)
-        -  `coef_`: vector of regression coefficients 
-        -  `intercept_`: intercept
-        -  `coef_scaled_`: vector of scaled regression coeeficients (when scaling option used)
-        -  `intercept_scaled_`: scaled intercept
-        -  `residuals_`: vector of regression residuals
-        -  `x_ev_`: X block explained variance per component
-        -  `y_ev_`: y block explained variance 
-        -  `fitted_`: fitted response
-        -  `x_Rweights_`: X block SIMPLS style weighting vectors (usually denoted R)
-        -  `x_caseweights_`: X block case weights
-        -  `y_caseweights_`: y block case weights
-        -  `caseweights_`: combined case weights
-        -  `colret_`: names of variables retained in the sparse model
-        -  `x_loc_`: X block location estimate 
-        -  `y_loc_`: y location estimate
-        -  `x_sca_`: X block scale estimate
-        -  `y_sca_`: y scale estimate
-        -  `non_zero_scale_vars_`: indicator vector of variables in X with nonzero scale
-
-
-    """
-
-    def __init__(
-        self,
-        n_components=1,
-        eta=0.5,
-        fun="Hampel",
-        probp1=0.95,
-        probp2=0.975,
-        probp3=0.999,
-        centre="median",
-        scale="mad",
-        verbose=True,
-        maxit=100,
-        tol=0.01,
-        start_cutoff_mode="specific",
-        start_X_init="pcapp",
-        columns=False,
-        copy=True,
-    ):
-        self.n_components = int(n_components)
-        self.eta = float(eta)
-        self.fun = fun
-        self.probp1 = probp1
-        self.probp2 = probp2
-        self.probp3 = probp3
-        self.centre = centre
-        self.scale = scale
-        self.verbose = verbose
-        self.maxit = maxit
-        self.tol = tol
-        self.start_cutoff_mode = start_cutoff_mode
-        self.start_X_init = start_X_init
-        self.columns = columns
-        self.copy = copy
-        self.probctx_ = "irrelevant"
-        self.probcty_ = "irrelevant"
-        self.hampelbx_ = "irrelevant"
-        self.hampelby__ = "irrelevant"
-        self.hampelrx_ = "irrelevant"
-        self.hampelry_ = "irrelevant"
-        self.non_zero_scale_vars_ = None
-
-    def fit(self, X, y):
-        """
-        Fit a  SPRM model. 
-
-        Parameters
-        ------------ 
-
-            X : numpy array 
-                Input data.
-
-            y :   vector or 1D matrix
-                Response data
-
-        """
-        if self.copy:
-            self.X = copy.deepcopy(X)
-            self.y = copy.deepcopy(y)
-        (n, p) = X.shape
-        if not (type(self.n_components) == int) | (self.n_components <= 0):
-            raise MyException(
-                "Number of components has to be a positive integer")
-        if (self.n_components > n) | (self.n_components > p):
-            raise MyException("The number of components is too large.")
-        if self.n_components <= 0:
-            raise MyException("The number of components has to be positive.")
-        if not (type(self.eta) == float):
-            raise MyException(
-                "Sparsity parameter eta has to be a floating point number"
-            )
-        if (self.eta < 0) | (self.eta >= 1):
-            raise MyException("eta has to come from the interval [0,1)")
-        if not (self.fun in ("Hampel", "Huber", "Fair")):
-            raise MyException(
-                "Invalid weighting function. Choose Hampel, Huber or Fair for parameter fun."
-            )
-        if (self.probp1 > 1) | (self.probp1 <= 0):
-            raise MyException(
-                "probp1 is a probability. Choose a value between 0 and 1")
-        if self.fun == "Hampel":
-            if not (
-                (self.probp1 < self.probp2)
-                & (self.probp2 < self.probp3)
-                & (self.probp3 <= 1)
-            ):
-                raise MyException(
-                    "Wrong choise of parameters for Hampel function. Use 0<probp1<hampelp2<hampelp3<=1"
-                )
-        if type(self.columns) is list:
-            self.columns = np.array(self.columns)
-        elif type(self.columns) is bool:
-            if type(X) != ps.core.frame.DataFrame and self.columns:
-                raise (
-                    MyException(
-                        "Columns set to true can only extract column names for data frame input"
-                    )
-                )
-        if type(X) == ps.core.frame.DataFrame:
-            if type(self.columns) is bool and self.columns:
-                self.columns = X.columns
-        X = _check_input(X)
-        y = _check_input(y)
-        ny = y.shape[0]
-        if ny != n:
-            raise MyException("Number of cases in y and X must be identical.")
-
-        if self.scale == "scaleTau2":
-            if (mad(X) == 0).any():
-                # kstepLTS divides by MAD. Zero MAD leads to nan -> detect zero MAD, scale by MAD and remove zero MAD variable.
-                self.scale = "mad"
-                warnings.warn("Scale set to `mad`")
-
-        scaling = VersatileScaler(center=self.centre, scale=self.scale)
-        Xs = scaling.fit_transform(X).astype("float64")
-        mX = scaling.col_loc_
-        sX = scaling.col_sca_
-        ys = scaling.fit_transform(y).astype("float64")
-        my = scaling.col_loc_
-        sy = scaling.col_sca_
-        setattr(self, "x_loc_", mX)
-        setattr(self, "y_loc_", my)
-        setattr(self, "x_sca_", sX)
-        setattr(self, "y_sca_", sy)
-        ys = np.array(ys).reshape(-1)
-
-        zero_scale = np.where(sX < 1e-5)[0]
-        vars_to_keep = np.arange(0, p)
-        if len(zero_scale) > 0:
-            if type(self.columns) != bool:
-                warntext = (
-                    "Zero scale variables with indices "
-                    + str(self.columns[zero_scale])
-                    + " detected and removed"
-                )
-            else:
-                warntext = (
-                    "Zero scale variables with indices "
-                    + str(zero_scale)
-                    + " detected and removed"
-                )
-
-            warnings.warn(warntext)
-            vars_to_keep = np.setdiff1d(np.arange(0, p), zero_scale)
-            Xs = Xs[:, vars_to_keep]
-            X = X[:, vars_to_keep]
-            sX = sX[vars_to_keep]
-            if type(self.columns) != bool:
-                self.columns = self.columns[vars_to_keep]
-            p = len(vars_to_keep)
-            if self.n_components > p:
-                raise MyException(
-                    "Upon removal of zero scale variables, the number of components is too large. Please reduce to {p} maximally."
-                )
-
-        self.non_zero_scale_vars_ = vars_to_keep
-
-        if self.start_X_init == "pcapp":
-            U, S, V = np.linalg.svd(Xs)
-            spc = np.square(S)
-            spc /= np.sum(spc)
-            relcomp = max(
-                np.where(spc - brokenstick(min(p, n))[:, 0] <= 0)[0][0], 1)
-            Urc = np.array(U[:, 0:relcomp])
-            Us = scaling.fit_transform(Urc)
-        else:
-            Us = Xs
-        wx = np.sqrt(np.array(np.sum(np.square(Us), 1), dtype=np.float64))
-        wx = wx / np.median(wx)
-        if [self.centre, self.scale] == ["median", "mad"]:
-            wy = np.array(abs(ys), dtype=np.float64)
-        else:
-            wy = (y - np.median(y, axis=0)) / (
-                1.4826 * np.median(abs(y - np.median(y, axis=0)), axis=0)
-            )
-        self.probcty_ = norm.ppf(self.probp1)
-        if self.start_cutoff_mode == "specific":
-            self.probctx_ = chi2.ppf(self.probp1, relcomp)
-        else:
-            self.probctx_ = self.probcty_
-        if self.fun == "Fair":
-            wx = Fair(wx, self.probctx_)
-            wy = Fair(wy, self.probcty_)
-        if self.fun == "Huber":
-            wx = Huber(wx, self.probctx_)
-            wy = Huber(wy, self.probcty_)
-        if self.fun == "Hampel":
-            self.hampelby_ = norm.ppf(self.probp2)
-            self.hampelry_ = norm.ppf(self.probp3)
-            if self.start_cutoff_mode == "specific":
-                self.hampelbx_ = chi2.ppf(self.probp2, relcomp)
-                self.hampelrx_ = chi2.ppf(self.probp3, relcomp)
-            else:
-                self.hampelbx_ = self.hampelby_
-                self.hampelrx_ = self.hampelry_
-            wx = Hampel(wx, self.probctx_, self.hampelbx_, self.hampelrx_)
-            wy = Hampel(wy, self.probcty_, self.hampelby_, self.hampelry_)
-        wx = np.array(wx).reshape(-1)
-        wy = np.array(wy).reshape(-1)
-        w = (wx * wy).astype("float64")
-        if (w < 1e-06).any():
-            w0 = np.where(w < 1e-06)[0]
-            w[w0] = 1e-06
-            we = np.array(w, dtype=np.float64)
-        else:
-            we = np.array(w, dtype=np.float64)
-        wte = wx
-        wye = wy
-        WEmat = np.array([np.sqrt(we) for i in range(1, p + 1)], ndmin=1).T
-        Xw = np.multiply(Xs, WEmat).astype("float64")
-        yw = ys * np.sqrt(we)
-        scalingt = copy.deepcopy(scaling)
-        loops = 1
-        rold = 1e-5
-        difference = 1
-        # Begin at iteration
-        res_snipls = snipls(
-            self.eta,
-            self.n_components,
-            self.verbose,
-            self.columns,
-            "mean",
-            "None",
-            self.copy,
-        )
-        while (difference > self.tol) & (loops < self.maxit):
-            res_snipls.fit(Xw, yw)
-            T = np.divide(res_snipls.x_scores_,
-                          WEmat[:, 0: (self.n_components)])
-            b = res_snipls.coef_
-            yp = res_snipls.fitted_
-            r = ys - yp
-            if len(r) / 2 > np.sum(r == 0):
-                r = abs(r) / (1.4826 * np.median(abs(r)))
-            else:
-                r = abs(r) / (1.4826 * np.median(abs(r[r != 0])))
-            scalet = self.scale
-            if scalet == "None":
-                scalingt.set_params(scale="mad")
-            dt = scalingt.fit_transform(T)
-            wtn = np.sqrt(np.array(np.sum(np.square(dt), 1), dtype=np.float64))
-            wtn = wtn / np.median(wtn)
-            wtn = wtn.reshape(-1)
-            wye = r.reshape(-1)
-            wte = wtn
-            if self.fun == "Fair":
-                wte = Fair(wtn, self.probctx_)
-                wye = Fair(wye, self.probcty_)
-            if self.fun == "Huber":
-                wte = Huber(wtn, self.probctx_)
-                wye = Huber(wye, self.probcty_)
-            if self.fun == "Hampel":
-                self.probctx_ = chi2.ppf(self.probp1, self.n_components)
-                self.hampelbx_ = chi2.ppf(self.probp2, self.n_components)
-                self.hampelrx_ = chi2.ppf(self.probp3, self.n_components)
-                wte = Hampel(wtn, self.probctx_,
-                             self.hampelbx_, self.hampelrx_)
-                wye = Hampel(wye, self.probcty_,
-                             self.hampelby_, self.hampelry_)
-            b2sum = np.sum(np.power(b, 2))
-            difference = abs(b2sum - rold) / rold
-            rold = b2sum
-            wte = np.array(wte).reshape(-1)
-            we = (wye * wte).astype("float64")
-            w0 = []
-            if any(we < 1e-06):
-                w0 = np.where(we < 1e-06)[0]
-                we[w0] = 1e-06
-                we = np.array(we, dtype=np.float64)
-            if len(w0) >= (n / 2):
-                break
-            WEmat = np.array([np.sqrt(we) for i in range(1, p + 1)], ndmin=1).T
-            Xw = np.multiply(Xs, WEmat).astype("float64")
-            yw = ys * np.sqrt(we)
-            loops += 1
-        if difference > self.maxit:
-            print(
-                "Warning: Method did not converge. The scaled difference between norms of the coefficient vectors is "
-                + str(round(difference, 4))
-            )
-        plotprec = False
-        if plotprec:
-            print(str(loops - 1))
-        w = we
-        w[w0] = 0
-        wt = wte
-        wt[w0] = 0
-        wy = wye
-        wy[w0] = 0
-        P = res_snipls.x_loadings_
-        W = res_snipls.x_weights_
-        R = res_snipls.x_Rweights_
-        Xrw = np.array(np.multiply(Xs, np.sqrt(WEmat)).astype("float64"))
-        scaling.set_params(scale="None")
-        Xrw = scaling.fit_transform(Xrw)
-        T = np.matmul(Xs, R)
-        if self.verbose:
-            print(
-                "Final Model: Variables retained for "
-                + str(self.n_components)
-                + " latent variables: \n"
-                + str(res_snipls.colret_)
-                + "\n"
-            )
-        b_rescaled = np.multiply(np.reshape(sy / sX, (p, 1)), b)
-        yp_rescaled = np.matmul(X, b_rescaled)
-        if self.centre == "mean":
-            intercept = np.mean(y - yp_rescaled, axis=0)
-        elif self.centre == "None":
-            intercept = 0
-        else:
-            intercept = np.median(y - yp_rescaled, axis=0)
-        # This median calculation produces slightly different result in R and Py
-        yfit = yp_rescaled + intercept
-        if self.scale != "None":
-            if self.centre == "mean":
-                b0 = np.mean(ys.astype("float64") -
-                             np.matmul(Xs.astype("float64"), b))
-            else:
-                b0 = np.median(
-                    np.array(ys.astype("float64") -
-                             np.matmul(Xs.astype("float64"), b))
-                )
-            # yfit2 = (np.matmul(Xrc.Xs.astype("float64"),b) + b0)*yrc.col_sca + yrc.col_loc
-            # already more generally included
-        else:
-            if self.centre == "mean":
-                b0 = np.mean(y - np.matmul(X, b))
-            elif self.centre == "None":
-                b0 = 0
-            else:
-                b0 = np.median(np.array(y - np.matmul(X, b)))
-            # yfit = np.matmul(X,b) + intercept
-        yfit = yfit
-        r = y - yfit
-        setattr(self, "x_weights_", W)
-        setattr(self, "x_loadings_", P)
-        setattr(self, "C_", res_snipls.C_)
-        setattr(self, "x_scores_", T)
-        setattr(self, "coef_", b_rescaled)
-        setattr(self, "intercept_", intercept)
-        setattr(self, "coef_scaled_", b)
-        setattr(self, "intercept_scaled_", b0)
-        setattr(self, "residuals_", r)
-        setattr(self, "x_ev_", res_snipls.x_ev_)
-        setattr(self, "y_ev_", res_snipls.y_ev_)
-        setattr(self, "fitted_", yfit)
-        setattr(self, "x_Rweights_", R)
-        setattr(self, "x_caseweights_", wte)
-        setattr(self, "y_caseweights_", wye)
-        setattr(self, "caseweights_", we)
-        setattr(self, "colret_", res_snipls.colret_)
-        setattr(self, "scaling_", scaling)
-        setattr(self, "scalingt_", scalingt)
-        return self
-        pass
-
-    def predict(self, Xn):
-        """
-        Predict using a  SPRM model. 
-
-        Parameters
-        ------------ 
-
-            Xn : numpy array or data frame 
-                Input data.
-        """
-        n, p, Xn = _predict_check_input(Xn)
-        if p != self.X.shape[1]:
-            raise (
-                ValueError(
-                    "New data must have seame number of columns as the ones the model has been trained with"
-                )
-            )
-        Xn = Xn[:, self.non_zero_scale_vars_]
-        return np.matmul(Xn, self.coef_) + self.intercept_
-
-    def transform(self, Xn):
-        """
-        Transform input data. 
-
-
-        Parameters
-        ------------ 
-
-            Xn : numpy array or data frame 
-                Input data.
-
-        """
-        n, p, Xn = _predict_check_input(Xn)
-        if p != self.X.shape[1]:
-            raise (
-                ValueError(
-                    "New data must have seame number of columns as the ones the model has been trained with"
-                )
-            )
-        Xn = Xn[:, self.non_zero_scale_vars_]
-        Xnc = scale_data(
-            Xn,
-            self.x_loc_[self.non_zero_scale_vars_],
-            self.x_sca_[self.non_zero_scale_vars_],
-        )
-        return np.matmul(Xnc, self.x_Rweights_)
-
-    def weightnewx(self, Xn):
-        """
-        Calculate case weights for new data based on the projection in the SPRM score space
-        """
-        n, p, Xn = _predict_check_input(Xn)
-        (n, p) = Xn.shape
-        if p != self.X.shape[1]:
-            raise (
-                ValueError(
-                    "New data must have seame number of columns as the ones the model has been trained with"
-                )
-            )
-        Tn = self.transform(Xn)
-        scaling = self.scalingt_
-        scalet = self.scale
-        if scalet == "None":
-            scaling.set_params(scale="mad")
-        if isinstance(Tn, np.matrix):
-            Tn = np.array(Tn)
-        dtn = scaling.fit_transform(Tn)
-        wtn = np.sqrt(np.array(np.sum(np.square(dtn), 1), dtype=np.float64))
-        wtn = wtn / np.median(wtn)
-        wtn = wtn.reshape(-1)
-        if self.fun == "Fair":
-            wtn = Fair(wtn, self.probctx_)
-        if self.fun == "Huber":
-            wtn = Huber(wtn, self.probctx_)
-        if self.fun == "Hampel":
-            wtn = Hampel(wtn, self.probctx_, self.hampelbx_, self.hampelrx_)
-        return wtn
-
-    def valscore(self, Xn, yn, scoring):
-        """
-        Specific score function for validation data
-        """
-        n, p, Xn = _predict_check_input(Xn)
-        (n, p) = Xn.shape
-        if p != self.X.shape[1]:
-            raise (
-                ValueError(
-                    "New data must have seame number of columns as the ones the model has been trained with"
-                )
-            )
-        if scoring == "weighted":
-            return RegressorMixin.score(self, Xn, yn, sample_weight=self.caseweights_)
-        elif scoring == "normal":
-            return RegressorMixin.score(self, Xn, yn)
-        else:
-            raise (ValueError('Scoring flag must be set to "weighted" or "normal".'))
+from __future__ import absolute_import, division, print_function
+from __future__ import unicode_literals
+from sklearn.base import RegressorMixin, BaseEstimator, TransformerMixin
+from sklearn.utils.metaestimators import _BaseComposition
+from scipy.stats import norm, chi2
+import copy
+import numpy as np
+import pandas as ps
+import warnings
+from ..preprocessing.robcent import VersatileScaler
+from .snipls import snipls
+from ..utils.utils import MyException, _predict_check_input, _check_input
+from ._m_support_functions import *
+from ..preprocessing._preproc_utilities import scale_data, mad
+
+
+class sprm(_BaseComposition, BaseEstimator, TransformerMixin, RegressorMixin):
+
+    """
+    SPRM Sparse Partial Robust M Regression 
+
+    Algorithm first outlined in: 
+        Sparse partial robust M regression, 
+        Irene Hoffmann, Sven Serneels, Peter Filzmoser, Christophe Croux, 
+        Chemometrics and Intelligent Laboratory Systems, 149 (2015), 50-59. 
+
+    Parameters
+    -----------
+
+    eta : float.
+          Sparsity parameter in [0,1)
+
+    n_components : int
+                     min 1. Note that if applied on data, n_components shall take a value <= min(x_data.shape)
+
+    fun : str
+         downweighting function. 'Hampel' (recommended), 'Fair' or  'Huber'
+
+    probp1 : float
+             probability cutoff for start of downweighting (e.g. 0.95)
+
+    probp2 : float
+            probability cutoff for start of steep downweighting (e.g. 0.975, only relevant if fun='Hampel')
+
+    probp3 : float
+            probability cutoff for start of outlier omission (e.g. 0.999, only relevant if fun='Hampel')
+
+    centre : str
+            type of centring (`'mean'`, `'median'`, `'l1median'`, or `'kstepLTS'`, 
+            the latter recommended statistically, if too slow, switch to `'median'`)
+
+    scale : str
+            type of scaling ('std','mad', 'scaleTau2' [recommended] or 'None')
+
+    verbose : booleans
+             specifying verbose mode
+
+    maxit : int
+            maximal number of iterations in M algorithm
+
+    tol : float
+         tolerance for convergence in M algorithm 
+
+    start_cutoff_mode : str,
+                    values:'specific' will set starting value cutoffs specific to X and y (preferred); any other value will set X and y stating cutoffs identically. The latter yields identical results to the SPRM R implementation available from CRAN.
+    start_X_init: str,
+                 values: 'pcapp' will include a PCA/broken stick projection to  calculate the staring weights, else just based on X; any other value will calculate the X starting values based on the X matrix itself. This is less stable for very flat data (p >> n), yet yields identical results to the SPRM R implementation available from CRAN.   
+
+    columns : (def false) Either boolean, list, numpy array or pandas Index
+                if False, no column names supplied; if True, if X data are supplied as a pandas data frame, will extract column names from the frame throws an error for other data input types if a list, array or Index (will only take length x_data.shape[1]), the column names of the x_data supplied in this list, will be printed in verbose mode. 
+
+    copy : (def True) boolean, whether to copy data
+
+    Attributes
+    ---------------
+    Attributes always provided 
+
+        -  `x_weights_`: X block PLS weighting vectors (usually denoted W)
+        -  `x_loadings_`: X block PLS loading vectors (usually denoted P)
+        -  `C_`: vector of inner relationship between response and latent variablesblock re
+        -  `x_scores_`: X block PLS score vectors (usually denoted T)
+        -  `coef_`: vector of regression coefficients 
+        -  `intercept_`: intercept
+        -  `coef_scaled_`: vector of scaled regression coeeficients (when scaling option used)
+        -  `intercept_scaled_`: scaled intercept
+        -  `residuals_`: vector of regression residuals
+        -  `x_ev_`: X block explained variance per component
+        -  `y_ev_`: y block explained variance 
+        -  `fitted_`: fitted response
+        -  `x_Rweights_`: X block SIMPLS style weighting vectors (usually denoted R)
+        -  `x_caseweights_`: X block case weights
+        -  `y_caseweights_`: y block case weights
+        -  `caseweights_`: combined case weights
+        -  `colret_`: names of variables retained in the sparse model
+        -  `x_loc_`: X block location estimate 
+        -  `y_loc_`: y location estimate
+        -  `x_sca_`: X block scale estimate
+        -  `y_sca_`: y scale estimate
+        -  `non_zero_scale_vars_`: indicator vector of variables in X with nonzero scale
+
+
+    """
+
+    def __init__(
+        self,
+        n_components=1,
+        eta=0.5,
+        fun="Hampel",
+        probp1=0.95,
+        probp2=0.975,
+        probp3=0.999,
+        centre="median",
+        scale="mad",
+        verbose=True,
+        maxit=100,
+        tol=0.01,
+        start_cutoff_mode="specific",
+        start_X_init="pcapp",
+        columns=False,
+        copy=True,
+    ):
+        self.n_components = int(n_components)
+        self.eta = float(eta)
+        self.fun = fun
+        self.probp1 = probp1
+        self.probp2 = probp2
+        self.probp3 = probp3
+        self.centre = centre
+        self.scale = scale
+        self.verbose = verbose
+        self.maxit = maxit
+        self.tol = tol
+        self.start_cutoff_mode = start_cutoff_mode
+        self.start_X_init = start_X_init
+        self.columns = columns
+        self.copy = copy
+        self.probctx_ = "irrelevant"
+        self.probcty_ = "irrelevant"
+        self.hampelbx_ = "irrelevant"
+        self.hampelby__ = "irrelevant"
+        self.hampelrx_ = "irrelevant"
+        self.hampelry_ = "irrelevant"
+        self.non_zero_scale_vars_ = None
+
+    def fit(self, X, y):
+        """
+        Fit a  SPRM model. 
+
+        Parameters
+        ------------ 
+
+            X : numpy array 
+                Input data.
+
+            y :   vector or 1D matrix
+                Response data
+
+        """
+        if self.copy:
+            self.X = copy.deepcopy(X)
+            self.y = copy.deepcopy(y)
+        (n, p) = X.shape
+        if not (type(self.n_components) == int) | (self.n_components <= 0):
+            raise MyException(
+                "Number of components has to be a positive integer")
+        if (self.n_components > n) | (self.n_components > p):
+            raise MyException("The number of components is too large.")
+        if self.n_components <= 0:
+            raise MyException("The number of components has to be positive.")
+        if not (type(self.eta) == float):
+            raise MyException(
+                "Sparsity parameter eta has to be a floating point number"
+            )
+        if (self.eta < 0) | (self.eta >= 1):
+            raise MyException("eta has to come from the interval [0,1)")
+        if not (self.fun in ("Hampel", "Huber", "Fair")):
+            raise MyException(
+                "Invalid weighting function. Choose Hampel, Huber or Fair for parameter fun."
+            )
+        if (self.probp1 > 1) | (self.probp1 <= 0):
+            raise MyException(
+                "probp1 is a probability. Choose a value between 0 and 1")
+        if self.fun == "Hampel":
+            if not (
+                (self.probp1 < self.probp2)
+                & (self.probp2 < self.probp3)
+                & (self.probp3 <= 1)
+            ):
+                raise MyException(
+                    "Wrong choise of parameters for Hampel function. Use 0<probp1<hampelp2<hampelp3<=1"
+                )
+        if type(self.columns) is list:
+            self.columns = np.array(self.columns)
+        elif type(self.columns) is bool:
+            if type(X) != ps.core.frame.DataFrame and self.columns:
+                raise (
+                    MyException(
+                        "Columns set to true can only extract column names for data frame input"
+                    )
+                )
+        if type(X) == ps.core.frame.DataFrame:
+            if type(self.columns) is bool and self.columns:
+                self.columns = X.columns
+        X = _check_input(X)
+        y = _check_input(y)
+        ny = y.shape[0]
+        if ny != n:
+            raise MyException("Number of cases in y and X must be identical.")
+
+        if self.scale == "scaleTau2":
+            if (mad(X) == 0).any():
+                # kstepLTS divides by MAD. Zero MAD leads to nan -> detect zero MAD, scale by MAD and remove zero MAD variable.
+                self.scale = "mad"
+                warnings.warn("Scale set to `mad`")
+
+        scaling = VersatileScaler(center=self.centre, scale=self.scale)
+        Xs = scaling.fit_transform(X).astype("float64")
+        mX = scaling.col_loc_
+        sX = scaling.col_sca_
+        ys = scaling.fit_transform(y).astype("float64")
+        my = scaling.col_loc_
+        sy = scaling.col_sca_
+        setattr(self, "x_loc_", mX)
+        setattr(self, "y_loc_", my)
+        setattr(self, "x_sca_", sX)
+        setattr(self, "y_sca_", sy)
+        ys = np.array(ys).reshape(-1)
+
+        zero_scale = np.where(sX < 1e-5)[0]
+        vars_to_keep = np.arange(0, p)
+        if len(zero_scale) > 0:
+            if type(self.columns) != bool:
+                warntext = (
+                    "Zero scale variables with indices "
+                    + str(self.columns[zero_scale])
+                    + " detected and removed"
+                )
+            else:
+                warntext = (
+                    "Zero scale variables with indices "
+                    + str(zero_scale)
+                    + " detected and removed"
+                )
+
+            warnings.warn(warntext)
+            vars_to_keep = np.setdiff1d(np.arange(0, p), zero_scale)
+            Xs = Xs[:, vars_to_keep]
+            X = X[:, vars_to_keep]
+            sX = sX[vars_to_keep]
+            if type(self.columns) != bool:
+                self.columns = self.columns[vars_to_keep]
+            p = len(vars_to_keep)
+            if self.n_components > p:
+                raise MyException(
+                    "Upon removal of zero scale variables, the number of components is too large. Please reduce to {p} maximally."
+                )
+
+        self.non_zero_scale_vars_ = vars_to_keep
+
+        if self.start_X_init == "pcapp":
+            U, S, V = np.linalg.svd(Xs)
+            spc = np.square(S)
+            spc /= np.sum(spc)
+            relcomp = max(
+                np.where(spc - brokenstick(min(p, n))[:, 0] <= 0)[0][0], 1)
+            Urc = np.array(U[:, 0:relcomp])
+            Us = scaling.fit_transform(Urc)
+        else:
+            Us = Xs
+        wx = np.sqrt(np.array(np.sum(np.square(Us), 1), dtype=np.float64))
+        wx = wx / np.median(wx)
+        if [self.centre, self.scale] == ["median", "mad"]:
+            wy = np.array(abs(ys), dtype=np.float64)
+        else:
+            wy = (y - np.median(y, axis=0)) / (
+                1.4826 * np.median(abs(y - np.median(y, axis=0)), axis=0)
+            )
+        self.probcty_ = norm.ppf(self.probp1)
+        if self.start_cutoff_mode == "specific":
+            self.probctx_ = chi2.ppf(self.probp1, relcomp)
+        else:
+            self.probctx_ = self.probcty_
+        if self.fun == "Fair":
+            wx = Fair(wx, self.probctx_)
+            wy = Fair(wy, self.probcty_)
+        if self.fun == "Huber":
+            wx = Huber(wx, self.probctx_)
+            wy = Huber(wy, self.probcty_)
+        if self.fun == "Hampel":
+            self.hampelby_ = norm.ppf(self.probp2)
+            self.hampelry_ = norm.ppf(self.probp3)
+            if self.start_cutoff_mode == "specific":
+                self.hampelbx_ = chi2.ppf(self.probp2, relcomp)
+                self.hampelrx_ = chi2.ppf(self.probp3, relcomp)
+            else:
+                self.hampelbx_ = self.hampelby_
+                self.hampelrx_ = self.hampelry_
+            wx = Hampel(wx, self.probctx_, self.hampelbx_, self.hampelrx_)
+            wy = Hampel(wy, self.probcty_, self.hampelby_, self.hampelry_)
+        wx = np.array(wx).reshape(-1)
+        wy = np.array(wy).reshape(-1)
+        w = (wx * wy).astype("float64")
+        if (w < 1e-06).any():
+            w0 = np.where(w < 1e-06)[0]
+            w[w0] = 1e-06
+            we = np.array(w, dtype=np.float64)
+        else:
+            we = np.array(w, dtype=np.float64)
+        wte = wx
+        wye = wy
+        WEmat = np.array([np.sqrt(we) for i in range(1, p + 1)], ndmin=1).T
+        Xw = np.multiply(Xs, WEmat).astype("float64")
+        yw = ys * np.sqrt(we)
+        scalingt = copy.deepcopy(scaling)
+        loops = 1
+        rold = 1e-5
+        difference = 1
+        # Begin at iteration
+        res_snipls = snipls(
+            self.eta,
+            self.n_components,
+            self.verbose,
+            self.columns,
+            "mean",
+            "None",
+            self.copy,
+        )
+        while (difference > self.tol) & (loops < self.maxit):
+            res_snipls.fit(Xw, yw)
+            T = np.divide(res_snipls.x_scores_,
+                          WEmat[:, 0: (self.n_components)])
+            b = res_snipls.coef_
+            yp = res_snipls.fitted_
+            r = ys - yp
+            if len(r) / 2 > np.sum(r == 0):
+                r = abs(r) / (1.4826 * np.median(abs(r)))
+            else:
+                r = abs(r) / (1.4826 * np.median(abs(r[r != 0])))
+            scalet = self.scale
+            if scalet == "None":
+                scalingt.set_params(scale="mad")
+            dt = scalingt.fit_transform(T)
+            wtn = np.sqrt(np.array(np.sum(np.square(dt), 1), dtype=np.float64))
+            wtn = wtn / np.median(wtn)
+            wtn = wtn.reshape(-1)
+            wye = r.reshape(-1)
+            wte = wtn
+            if self.fun == "Fair":
+                wte = Fair(wtn, self.probctx_)
+                wye = Fair(wye, self.probcty_)
+            if self.fun == "Huber":
+                wte = Huber(wtn, self.probctx_)
+                wye = Huber(wye, self.probcty_)
+            if self.fun == "Hampel":
+                self.probctx_ = chi2.ppf(self.probp1, self.n_components)
+                self.hampelbx_ = chi2.ppf(self.probp2, self.n_components)
+                self.hampelrx_ = chi2.ppf(self.probp3, self.n_components)
+                wte = Hampel(wtn, self.probctx_,
+                             self.hampelbx_, self.hampelrx_)
+                wye = Hampel(wye, self.probcty_,
+                             self.hampelby_, self.hampelry_)
+            b2sum = np.sum(np.power(b, 2))
+            difference = abs(b2sum - rold) / rold
+            rold = b2sum
+            wte = np.array(wte).reshape(-1)
+            we = (wye * wte).astype("float64")
+            w0 = []
+            if any(we < 1e-06):
+                w0 = np.where(we < 1e-06)[0]
+                we[w0] = 1e-06
+                we = np.array(we, dtype=np.float64)
+            if len(w0) >= (n / 2):
+                break
+            WEmat = np.array([np.sqrt(we) for i in range(1, p + 1)], ndmin=1).T
+            Xw = np.multiply(Xs, WEmat).astype("float64")
+            yw = ys * np.sqrt(we)
+            loops += 1
+        if difference > self.maxit:
+            print(
+                "Warning: Method did not converge. The scaled difference between norms of the coefficient vectors is "
+                + str(round(difference, 4))
+            )
+        plotprec = False
+        if plotprec:
+            print(str(loops - 1))
+        w = we
+        w[w0] = 0
+        wt = wte
+        wt[w0] = 0
+        wy = wye
+        wy[w0] = 0
+        P = res_snipls.x_loadings_
+        W = res_snipls.x_weights_
+        R = res_snipls.x_Rweights_
+        Xrw = np.array(np.multiply(Xs, np.sqrt(WEmat)).astype("float64"))
+        scaling.set_params(scale="None")
+        Xrw = scaling.fit_transform(Xrw)
+        T = np.matmul(Xs, R)
+        if self.verbose:
+            print(
+                "Final Model: Variables retained for "
+                + str(self.n_components)
+                + " latent variables: \n"
+                + str(res_snipls.colret_)
+                + "\n"
+            )
+        b_rescaled = np.multiply(np.reshape(sy / sX, (p, 1)), b)
+        yp_rescaled = np.matmul(X, b_rescaled)
+        if self.centre == "mean":
+            intercept = np.mean(y - yp_rescaled, axis=0)
+        elif self.centre == "None":
+            intercept = 0
+        else:
+            intercept = np.median(y - yp_rescaled, axis=0)
+        # This median calculation produces slightly different result in R and Py
+        yfit = yp_rescaled + intercept
+        if self.scale != "None":
+            if self.centre == "mean":
+                b0 = np.mean(ys.astype("float64") -
+                             np.matmul(Xs.astype("float64"), b))
+            else:
+                b0 = np.median(
+                    np.array(ys.astype("float64") -
+                             np.matmul(Xs.astype("float64"), b))
+                )
+            # yfit2 = (np.matmul(Xrc.Xs.astype("float64"),b) + b0)*yrc.col_sca + yrc.col_loc
+            # already more generally included
+        else:
+            if self.centre == "mean":
+                b0 = np.mean(y - np.matmul(X, b))
+            elif self.centre == "None":
+                b0 = 0
+            else:
+                b0 = np.median(np.array(y - np.matmul(X, b)))
+            # yfit = np.matmul(X,b) + intercept
+        yfit = yfit
+        r = y - yfit
+        setattr(self, "x_weights_", W)
+        setattr(self, "x_loadings_", P)
+        setattr(self, "C_", res_snipls.C_)
+        setattr(self, "x_scores_", T)
+        setattr(self, "coef_", b_rescaled)
+        setattr(self, "intercept_", intercept)
+        setattr(self, "coef_scaled_", b)
+        setattr(self, "intercept_scaled_", b0)
+        setattr(self, "residuals_", r)
+        setattr(self, "x_ev_", res_snipls.x_ev_)
+        setattr(self, "y_ev_", res_snipls.y_ev_)
+        setattr(self, "fitted_", yfit)
+        setattr(self, "x_Rweights_", R)
+        setattr(self, "x_caseweights_", wte)
+        setattr(self, "y_caseweights_", wye)
+        setattr(self, "caseweights_", we)
+        setattr(self, "colret_", res_snipls.colret_)
+        setattr(self, "scaling_", scaling)
+        setattr(self, "scalingt_", scalingt)
+        return self
+        pass
+
+    def predict(self, Xn):
+        """
+        Predict using a  SPRM model. 
+
+        Parameters
+        ------------ 
+
+            Xn : numpy array or data frame 
+                Input data.
+        """
+        n, p, Xn = _predict_check_input(Xn)
+        if p != self.X.shape[1]:
+            raise (
+                ValueError(
+                    "New data must have seame number of columns as the ones the model has been trained with"
+                )
+            )
+        Xn = Xn[:, self.non_zero_scale_vars_]
+        return np.matmul(Xn, self.coef_) + self.intercept_
+
+    def transform(self, Xn):
+        """
+        Transform input data. 
+
+
+        Parameters
+        ------------ 
+
+            Xn : numpy array or data frame 
+                Input data.
+
+        """
+        n, p, Xn = _predict_check_input(Xn)
+        if p != self.X.shape[1]:
+            raise (
+                ValueError(
+                    "New data must have seame number of columns as the ones the model has been trained with"
+                )
+            )
+        Xn = Xn[:, self.non_zero_scale_vars_]
+        Xnc = scale_data(
+            Xn,
+            self.x_loc_[self.non_zero_scale_vars_],
+            self.x_sca_[self.non_zero_scale_vars_],
+        )
+        return np.matmul(Xnc, self.x_Rweights_)
+
+    def weightnewx(self, Xn):
+        """
+        Calculate case weights for new data based on the projection in the SPRM score space
+        """
+        n, p, Xn = _predict_check_input(Xn)
+        (n, p) = Xn.shape
+        if p != self.X.shape[1]:
+            raise (
+                ValueError(
+                    "New data must have seame number of columns as the ones the model has been trained with"
+                )
+            )
+        Tn = self.transform(Xn)
+        scaling = self.scalingt_
+        scalet = self.scale
+        if scalet == "None":
+            scaling.set_params(scale="mad")
+        if isinstance(Tn, np.matrix):
+            Tn = np.array(Tn)
+        dtn = scaling.fit_transform(Tn)
+        wtn = np.sqrt(np.array(np.sum(np.square(dtn), 1), dtype=np.float64))
+        wtn = wtn / np.median(wtn)
+        wtn = wtn.reshape(-1)
+        if self.fun == "Fair":
+            wtn = Fair(wtn, self.probctx_)
+        if self.fun == "Huber":
+            wtn = Huber(wtn, self.probctx_)
+        if self.fun == "Hampel":
+            wtn = Hampel(wtn, self.probctx_, self.hampelbx_, self.hampelrx_)
+        return wtn
+
+    def valscore(self, Xn, yn, scoring):
+        """
+        Specific score function for validation data
+        """
+        n, p, Xn = _predict_check_input(Xn)
+        (n, p) = Xn.shape
+        if p != self.X.shape[1]:
+            raise (
+                ValueError(
+                    "New data must have seame number of columns as the ones the model has been trained with"
+                )
+            )
+        if scoring == "weighted":
+            return RegressorMixin.score(self, Xn, yn, sample_weight=self.caseweights_)
+        elif scoring == "normal":
+            return RegressorMixin.score(self, Xn, yn)
+        else:
+            raise (ValueError('Scoring flag must be set to "weighted" or "normal".'))
```

## direpack/sudire/_sudire_utils.py

 * *Ordering differences only*

```diff
@@ -1,506 +1,506 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Sat Apr 11 17:00:13 2020
-
-@author: Emmanuel Jordy Menvouta
-"""
-
-
-import numpy as np
-from ..dicomo._dicomo_utils import *
-import scipy.spatial as spp
-import pandas as pd
-import sympy
-from ..utils.utils import MyException
-
-
-# import Ball
-# uncomment this line if you want to use
-
-
-def mdd_trim(beta, *args):
-    """
-    This is the objective function to be optimised when using MDD-SDR
-    args is expected to be a tuple With the following elements : 
-        X : input Data as matrix
-        Y : input Data as 1d matrix 
-        h, int: Dimension of the central subspace
-        N2 : inverse of square root of the covariance matrix of X 
-        is_distance_matrix, bool : if true, X and Y represent distance matrices
-        trimming, float : trimming fraction to be applied for MDD
-        center, bool :  if true, X and Y will be centered when computing MDD
-        dmetric : 'the distance metric to be used in the computation of MDD
-        biascorr, bool : if True, an unbiased estimator of MDD is computed.
-    """
-    X = args[0]
-    Y = args[1]
-    h = args[2]
-    N2 = args[3]
-    is_data_matrix = args[4]
-    trimming = args[5]
-    center = args[6]
-    dmetric = args[7]
-    biascorr = args[8]
-    beta = np.reshape(beta, (-1, h), order="F")
-    if is_data_matrix:
-        X_coord = give_coords(X)
-        # Y_coord = give_coords(Y)
-        X_dat = np.matmul(X_coord, beta)
-        res = np.sqrt(
-            difference_divergence(
-                X_dat, Y, center=center, trimming=trimming, biascorr=biascorr
-            )
-        )
-        return -10 * res
-    else:
-        X_dat = np.matmul(X, beta)
-        res = np.sqrt(
-            difference_divergence(
-                X_dat, Y, center=center, trimming=trimming, biascorr=biascorr
-            )
-        )
-        return -10 * res
-
-
-def dcov_trim(beta, *args):
-    """
-    This is the objective function to be optimised when using DCOV-SDR
-    args is expected to be a tuple With the following elements : 
-        X : input Data as matrix
-        Y : input Data as 1d matrix 
-        h : Dimension of the central subspace
-        N2 : inverse of square root of the covariance matrix of X 
-        is_distance_matrix, bool : if true, X and Y represent distance matrices
-        center, bool :  if true, X and Y will be centered when computing MDD
-        dmetric : 'the distance metric to be used in the computation of MDD
-        biascorr, bool : if True, an unbiased estimator of MDD is computed.
-    """
-    X = args[0]
-    Y = args[1]
-    h = args[2]
-    N2 = args[3]
-    is_distance_matrix = args[4]
-    trimming = args[5]
-    center = args[6]
-    dmetric = args[7]
-    beta = np.reshape(beta, (-1, h), order="F")
-    if is_distance_matrix:
-        X_coord = give_coords(X)
-        Y_coord = give_coords(Y)
-        X_dat = np.matmul(X_coord, beta)
-        dmx, n1 = distance_matrix_centered(
-            X_dat, trimming=trimming, center=center, dmetric=dmetric
-        )
-        dmy, n2 = distance_matrix_centered(
-            Y_coord, trimming=trimming, center=center, dmetric=dmetric
-        )
-        res = distance_moment(
-            dmx, dmy, n1=n1, center=center, trimming=trimming, order=2
-        )
-        return -10 * res
-    else:
-        Y = Y.reshape(-1, 1)
-        X_dat = np.matmul(X, beta)
-        dmx, n1 = distance_matrix_centered(
-            X_dat, trimming=trimming, center=center, dmetric=dmetric
-        )
-        dmy, n2 = distance_matrix_centered(
-            Y, trimming=trimming, center=center, dmetric=dmetric
-        )
-        res = distance_moment(
-            dmx, dmy, n1=n1, center=center, trimming=trimming, order=2
-        )
-        return -10 * res
-
-
-def give_coords(distances):
-    """give coordinates of points for which distances given
-
-    coordinates are given relatively. 1st point on origin, 2nd on x-axis, 3rd 
-    x-y plane and so on. Maximum n-1 dimentions for which n is the number
-    of points
-     Args:
-        distanes (list): is a n x n, 2d array where distances[i][j] gives the distance 
-            from i to j assumed distances[i][j] == distances[j][i]
-
-     Returns:
-        numpy.ndarray: cordinates in list form n dim
-    """
-    distances = np.array(distances)
-
-    n = len(distances)
-    X = sympy.symarray("x", (n, n - 1))
-
-    for row in range(n):
-        X[row, row:] = [0] * (n - 1 - row)
-
-    for point2 in range(1, n):
-
-        expressions = []
-
-        for point1 in range(point2):
-            expression = np.sum((X[point1] - X[point2]) ** 2)
-            expression -= distances[point1, point2] ** 2
-            expressions.append(expression)
-
-        X[point2, :point2] = sympy.solve(expressions, list(X[point2, :point2]))[1]
-
-    return X
-
-
-def matpower(A, alpha):
-    """
-    computes the A to the power alpha using the eigen decomposition of A.
-    """
-    A = (A + A.T) / 2
-    eig_vals, eig_vecs = np.linalg.eigh(A)
-    res = np.matmul(np.matmul(eig_vecs, np.diag(eig_vals ** alpha)), eig_vecs.T)
-    return res
-
-
-def discretize(y, h):
-    """
-    This function is used to discretize a continuous variable y into h classes
-    that are ordered.
-    input :
-        y : input data as vector or 1D matrix.
-        h : the number of classes 
-        
-    output : 
-         vector containing the class to which each element of y belongs. 
-    """
-    n = len(y)
-    m = np.floor(n / h)
-    yord = np.sort(y)
-    divpt = []
-    for i in range(1, h):
-        divpt.append(yord[int((i * m))])
-    y1 = np.repeat(0, n)
-    y1[y < divpt[0]] = 1
-    y1[y >= divpt[h - 2]] = h
-    for i in range(1, h - 1):
-        y1[(y >= divpt[i - 1]) & (y < divpt[i])] = i + 1
-    return y1
-
-
-def SIR(x, y, n_slices, d, ytype="continuous", center_data=True, scale_data=True):
-    """
-    computes  the Sliced Inverse Regression  estimator of the central subspace.
-    The algortihm is inspired from :
-    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
-    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
-    
-    Input : 
-        X : input Data as Matrix 
-        Y : input data as vector or 1d matrix.
-        n_slices, int : number of slices 
-        d, int : dimension of the central subspace.
-        ytype, str : either discrete or continuous
-        center_data, bool, if true, the data is centered before SIR
-        scale_data, bool, if true, the data is sclaed before SIR
-        
-    Output :
-         The estimated basis of the central subspace.
-        
-    """
-    y = np.asarray(y).flatten()
-    n = x.shape[0]  ## zxception if d> p or x.shape[0] != y.shape[0]
-    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
-    #     xc = x - x.mean(axis=0)
-    #     xstd= np.matmul(xc,signsqrt)
-
-    if center_data:
-        x = x - x.mean(axis=0)
-    if scale_data:
-        x = np.matmul(x, signsqrt)
-    xstd = x
-    if ytype == "continuous":
-        ydis = discretize(y, n_slices)
-    else:
-        ydis = y
-    yless = ydis
-    ylabel = []
-    for i in range(n):
-        if np.var(yless) != 0:
-            ylabel.append(yless[0])
-            yless = yless[yless != yless[0]]
-    ylabel.append(yless[0])
-    prob = []
-    exy = []
-
-    for i in range(n_slices):
-        prob.append(len(ydis[ydis == ylabel[i]]) / n)
-        xres = np.apply_along_axis(np.mean, 0, xstd[ydis == ylabel[i], :])
-        exy.append(xres)
-
-    exy = np.vstack(exy)
-    sirmat = np.matmul(np.matmul(exy.T, np.diag(np.array(prob))), exy)
-    eig_vals, eig_vecs = np.linalg.eigh(sirmat)
-    idx = eig_vals.argsort()[::-1]
-    eig_vals = eig_vals[idx]
-    eig_vecs = eig_vecs[:, idx]
-    if scale_data:
-        return np.matmul(signsqrt, eig_vecs[:, 0:d])
-    else:
-        return eig_vecs[:, 0:d]
-
-
-def SAVE(x, y, n_slices, d, ytype="continuous", center_data=True, scale_data=True):
-    """
-    computes  the Sliced Average Variance Estimator of the central subspace.
-    The algortihm is inspired from :
-    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
-    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
-    
-    Input : 
-        X : input Data as Matrix 
-        Y : input data as vector or 1d matrix.
-        n_slices, int : number of slices 
-        d, int : dimension of the central subspace.
-        ytype, str : either discrete or continuous
-        center_data, bool, if true, the data is centered before SIR
-        scale_data, bool, if true, the data is sclaed before SIR
-        
-    Output :
-         The estimated basis of the central subspace.
-        
-    """
-    y = np.asarray(y).flatten()
-    p = x.shape[1]
-    n = x.shape[0]  ## zxception if d> p or x.shape[0] != y.shape[0]
-    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
-    #     xc = x - x.mean(axis=0)
-    #     xstd= np.matmul(xc,signsqrt)
-    if center_data:
-        x = x - x.mean(axis=0)
-    if scale_data:
-        x = np.matmul(x, signsqrt)
-
-    xstd = x
-
-    if ytype == "continuous":
-        ydis = discretize(y, n_slices)
-    else:
-        ydis = y
-    yless = ydis
-    ylabel = []
-    for i in range(n):
-        if np.var(yless) != 0:
-            ylabel.append(yless[0])
-            yless = yless[yless != yless[0]]
-    ylabel.append(yless[0])
-    prob = []
-    for i in range(n_slices):
-        prob.append(len(ydis[ydis == ylabel[i]]) / n)
-
-    vxy = np.zeros((n_slices, p, p))
-    for i in range(n_slices):
-        vxy[i, :, :] = np.cov(xstd[ydis == ylabel[i], :], rowvar=0)
-
-    savemat = np.zeros((p, p))
-    for i in range(n_slices):
-        savemat = savemat + prob[i] * np.matmul(
-            (vxy[i, :, :] - np.identity(p)), (vxy[i, :, :] - np.identity(p))
-        )
-
-    eig_vals, eig_vecs = np.linalg.eigh(savemat)
-    idx = eig_vals.argsort()[::-1]
-    eig_vals = eig_vals[idx]
-    eig_vecs = eig_vecs[:, idx]
-    if scale_data:
-        return np.matmul(signsqrt, eig_vecs[:, 0:d])
-    else:
-        return eig_vecs[:, 0:d]
-
-
-def DR(x, y, n_slices, d, ytype="continuous", center_data=True, scale_data=True):
-    """
-    computes  the Directional Regression Estimator of the central subspace.
-    The algortihm is inspired from :
-    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
-    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
-    
-    Input : 
-        X : input Data as Matrix 
-        Y : input data as vector or 1d matrix.
-        n_slices, int : number of slices 
-        d, int : dimension of the central subspace.
-        ytype, str : either discrete or continuous
-        center_data, bool, if true, the data is centered before SIR
-        scale_data, bool, if true, the data is sclaed before SIR
-        
-    Output :
-         The estimated basis of the central subspace.
-        
-    """
-    y = np.asarray(y).flatten()
-    p = x.shape[1]
-    n = x.shape[0]  ## zxception if d> p or x.shape[0] != y.shape[0]
-    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
-
-    if center_data:
-        x = x - x.mean(axis=0)
-    if scale_data:
-        x = np.matmul(x, signsqrt)
-
-    xstd = x
-    if ytype == "continuous":
-        ydis = discretize(y, n_slices)
-    else:
-        ydis = y
-    yless = ydis
-    ylabel = []
-    for i in range(n):
-        if np.var(yless) != 0:
-            ylabel.append(yless[0])
-            yless = yless[yless != yless[0]]
-    ylabel.append(yless[0])
-    prob = []
-    exy = []
-
-    for i in range(n_slices):
-        prob.append(len(ydis[ydis == ylabel[i]]) / n)
-
-    vxy = np.zeros((n_slices, p, p,))
-    exy = []
-    for i in range(n_slices):
-        vxy[i, :, :,] = np.cov(xstd[ydis == ylabel[i], :], rowvar=0)
-        xres = np.apply_along_axis(np.mean, 0, xstd[ydis == ylabel[i], :])
-        exy.append(xres)
-
-    exy = np.vstack(exy)
-    mat1 = np.zeros((p, p))
-    mat2 = np.zeros((p, p))
-
-    for i in range(n_slices):
-        mat1 = mat1 + prob[i] * np.matmul(
-            (
-                vxy[i, :, :]
-                + np.matmul(exy[i, :].reshape((-1, 1)), exy[i, :].reshape((-1, 1)).T)
-            ),
-            (
-                vxy[i, :, :]
-                + np.matmul(exy[i, :].reshape((-1, 1)), exy[i, :].reshape((-1, 1)).T)
-            ),
-        )
-        mat2 = mat2 + prob[i] * (
-            np.matmul(exy[i, :].reshape((-1, 1)), exy[i, :].reshape((-1, 1)).T)
-        )
-
-    out = (
-        2 * mat1
-        + 2 * np.matmul(mat2, mat2)
-        + 2 * np.sum(np.diag(mat2)) * mat2
-        - 2 * np.identity(p)
-    )
-    eig_vals, eig_vecs = np.linalg.eigh(out)
-    idx = eig_vals.argsort()[::-1]
-    eig_vals = eig_vals[idx]
-    eig_vecs = eig_vecs[:, idx]
-    if scale_data:
-        return np.matmul(signsqrt, eig_vecs[:, 0:d])
-    else:
-        return eig_vecs[:, 0:d]
-
-
-def PHD(x, y, d, center_data=True, scale_data=True):
-
-    """
-    computes  the Principal Hessian Dimension estimator of the central subspace.
-    The algortihm is inspired from :
-    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
-    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
-    
-    Input : 
-        X : input Data as Matrix 
-        Y : input data as vector or 1d matrix.
-        d, int : dimension of the central subspace.
-        center_data, bool, if true, the data is centered before SIR
-        scale_data, bool, if true, the data is sclaed before SIR
-        
-    Output :
-         The estimated basis of the central subspace.
-    
-    """
-    n = x.shape[0]
-    if len(y.shape) < 2:
-        y = y.reshape((-1, 1))
-    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
-    if center_data:
-        x = x - x.mean(axis=0)
-    if scale_data:
-        x = np.matmul(x, signsqrt)
-
-    yc = y - np.mean(y, axis=0)
-    out = np.matmul(np.multiply(x, yc).T, x) / n
-    eig_vals, eig_vecs = np.linalg.eigh(out)
-    idx = eig_vals.argsort()[::-1]
-    eig_vals = eig_vals[idx]
-    eig_vecs = eig_vecs[:, idx]
-
-    if scale_data:
-        return np.matmul(signsqrt, eig_vecs[:, 0:d])
-    else:
-        return eig_vecs[:, 0:d]
-
-
-def IHT(x, y, d, center_data=True, scale_data=True):
-    """
-    computes  the Iterative Hessian Transformation estimator of the central subspace.
-    The algortihm is inspired from :
-    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
-    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
-    
-    Input : 
-        X : input Data as Matrix 
-        Y : input data as vector or 1d matrix.
-        d, int : dimension of the central subspace.
-        center_data, bool, if true, the data is centered before SIR
-        scale_data, bool, if true, the data is sclaed before SIR
-        
-    Output :
-         The estimated basis of the central subspace.
-        
-    """
-
-    p = x.shape[1]
-    ## zxception if d> p or x.shape[0] != y.shape[0]
-    if len(y.shape) < 2:
-        y = y.reshape((-1, 1))
-    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
-
-    if center_data:
-        x = x - x.mean(axis=0)
-    if scale_data:
-        x = np.matmul(x, signsqrt)
-
-    covxy = np.cov(x, y, rowvar=0)[-1, :-1]
-    covx = np.cov(x, rowvar=0)
-    mat = covxy
-    for i in range(1, p):
-        mat = np.column_stack((mat, np.matmul(matpower(covx, i), covxy)))
-
-    out = np.matmul(mat, mat.T)
-    eig_vals, eig_vecs = np.linalg.eigh(out)
-    idx = eig_vals.argsort()[::-1]
-    eig_vals = eig_vals[idx]
-    eig_vecs = eig_vecs[:, idx]
-    if scale_data:
-        return np.matmul(signsqrt, eig_vecs[:, 0:d])
-    else:
-        return eig_vecs[:, 0:d]
-
-
-# def ballcov_func(beta, *args):
-#     """
-#     Objective function  for BCOV-SDR.
-#     Will only work after uncommenting the import Ball statement above
-#     """
-
-#     X= args[0]
-#     Y= args[1]
-#     h=args[2]
-#     beta = np.reshape(beta,(-1,h),order = 'F')
-#     X_dat = np.matmul(X, beta)
-#     res = Ball.bcov_test(X_dat,Y,num_permutations=0)[0]
-#     return(-10*res)
+# -*- coding: utf-8 -*-
+"""
+Created on Sat Apr 11 17:00:13 2020
+
+@author: Emmanuel Jordy Menvouta
+"""
+
+
+import numpy as np
+from ..dicomo._dicomo_utils import *
+import scipy.spatial as spp
+import pandas as pd
+import sympy
+from ..utils.utils import MyException
+
+
+# import Ball
+# uncomment this line if you want to use
+
+
+def mdd_trim(beta, *args):
+    """
+    This is the objective function to be optimised when using MDD-SDR
+    args is expected to be a tuple With the following elements : 
+        X : input Data as matrix
+        Y : input Data as 1d matrix 
+        h, int: Dimension of the central subspace
+        N2 : inverse of square root of the covariance matrix of X 
+        is_distance_matrix, bool : if true, X and Y represent distance matrices
+        trimming, float : trimming fraction to be applied for MDD
+        center, bool :  if true, X and Y will be centered when computing MDD
+        dmetric : 'the distance metric to be used in the computation of MDD
+        biascorr, bool : if True, an unbiased estimator of MDD is computed.
+    """
+    X = args[0]
+    Y = args[1]
+    h = args[2]
+    N2 = args[3]
+    is_data_matrix = args[4]
+    trimming = args[5]
+    center = args[6]
+    dmetric = args[7]
+    biascorr = args[8]
+    beta = np.reshape(beta, (-1, h), order="F")
+    if is_data_matrix:
+        X_coord = give_coords(X)
+        # Y_coord = give_coords(Y)
+        X_dat = np.matmul(X_coord, beta)
+        res = np.sqrt(
+            difference_divergence(
+                X_dat, Y, center=center, trimming=trimming, biascorr=biascorr
+            )
+        )
+        return -10 * res
+    else:
+        X_dat = np.matmul(X, beta)
+        res = np.sqrt(
+            difference_divergence(
+                X_dat, Y, center=center, trimming=trimming, biascorr=biascorr
+            )
+        )
+        return -10 * res
+
+
+def dcov_trim(beta, *args):
+    """
+    This is the objective function to be optimised when using DCOV-SDR
+    args is expected to be a tuple With the following elements : 
+        X : input Data as matrix
+        Y : input Data as 1d matrix 
+        h : Dimension of the central subspace
+        N2 : inverse of square root of the covariance matrix of X 
+        is_distance_matrix, bool : if true, X and Y represent distance matrices
+        center, bool :  if true, X and Y will be centered when computing MDD
+        dmetric : 'the distance metric to be used in the computation of MDD
+        biascorr, bool : if True, an unbiased estimator of MDD is computed.
+    """
+    X = args[0]
+    Y = args[1]
+    h = args[2]
+    N2 = args[3]
+    is_distance_matrix = args[4]
+    trimming = args[5]
+    center = args[6]
+    dmetric = args[7]
+    beta = np.reshape(beta, (-1, h), order="F")
+    if is_distance_matrix:
+        X_coord = give_coords(X)
+        Y_coord = give_coords(Y)
+        X_dat = np.matmul(X_coord, beta)
+        dmx, n1 = distance_matrix_centered(
+            X_dat, trimming=trimming, center=center, dmetric=dmetric
+        )
+        dmy, n2 = distance_matrix_centered(
+            Y_coord, trimming=trimming, center=center, dmetric=dmetric
+        )
+        res = distance_moment(
+            dmx, dmy, n1=n1, center=center, trimming=trimming, order=2
+        )
+        return -10 * res
+    else:
+        Y = Y.reshape(-1, 1)
+        X_dat = np.matmul(X, beta)
+        dmx, n1 = distance_matrix_centered(
+            X_dat, trimming=trimming, center=center, dmetric=dmetric
+        )
+        dmy, n2 = distance_matrix_centered(
+            Y, trimming=trimming, center=center, dmetric=dmetric
+        )
+        res = distance_moment(
+            dmx, dmy, n1=n1, center=center, trimming=trimming, order=2
+        )
+        return -10 * res
+
+
+def give_coords(distances):
+    """give coordinates of points for which distances given
+
+    coordinates are given relatively. 1st point on origin, 2nd on x-axis, 3rd 
+    x-y plane and so on. Maximum n-1 dimentions for which n is the number
+    of points
+     Args:
+        distanes (list): is a n x n, 2d array where distances[i][j] gives the distance 
+            from i to j assumed distances[i][j] == distances[j][i]
+
+     Returns:
+        numpy.ndarray: cordinates in list form n dim
+    """
+    distances = np.array(distances)
+
+    n = len(distances)
+    X = sympy.symarray("x", (n, n - 1))
+
+    for row in range(n):
+        X[row, row:] = [0] * (n - 1 - row)
+
+    for point2 in range(1, n):
+
+        expressions = []
+
+        for point1 in range(point2):
+            expression = np.sum((X[point1] - X[point2]) ** 2)
+            expression -= distances[point1, point2] ** 2
+            expressions.append(expression)
+
+        X[point2, :point2] = sympy.solve(expressions, list(X[point2, :point2]))[1]
+
+    return X
+
+
+def matpower(A, alpha):
+    """
+    computes the A to the power alpha using the eigen decomposition of A.
+    """
+    A = (A + A.T) / 2
+    eig_vals, eig_vecs = np.linalg.eigh(A)
+    res = np.matmul(np.matmul(eig_vecs, np.diag(eig_vals ** alpha)), eig_vecs.T)
+    return res
+
+
+def discretize(y, h):
+    """
+    This function is used to discretize a continuous variable y into h classes
+    that are ordered.
+    input :
+        y : input data as vector or 1D matrix.
+        h : the number of classes 
+        
+    output : 
+         vector containing the class to which each element of y belongs. 
+    """
+    n = len(y)
+    m = np.floor(n / h)
+    yord = np.sort(y)
+    divpt = []
+    for i in range(1, h):
+        divpt.append(yord[int((i * m))])
+    y1 = np.repeat(0, n)
+    y1[y < divpt[0]] = 1
+    y1[y >= divpt[h - 2]] = h
+    for i in range(1, h - 1):
+        y1[(y >= divpt[i - 1]) & (y < divpt[i])] = i + 1
+    return y1
+
+
+def SIR(x, y, n_slices, d, ytype="continuous", center_data=True, scale_data=True):
+    """
+    computes  the Sliced Inverse Regression  estimator of the central subspace.
+    The algortihm is inspired from :
+    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
+    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
+    
+    Input : 
+        X : input Data as Matrix 
+        Y : input data as vector or 1d matrix.
+        n_slices, int : number of slices 
+        d, int : dimension of the central subspace.
+        ytype, str : either discrete or continuous
+        center_data, bool, if true, the data is centered before SIR
+        scale_data, bool, if true, the data is sclaed before SIR
+        
+    Output :
+         The estimated basis of the central subspace.
+        
+    """
+    y = np.asarray(y).flatten()
+    n = x.shape[0]  ## zxception if d> p or x.shape[0] != y.shape[0]
+    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
+    #     xc = x - x.mean(axis=0)
+    #     xstd= np.matmul(xc,signsqrt)
+
+    if center_data:
+        x = x - x.mean(axis=0)
+    if scale_data:
+        x = np.matmul(x, signsqrt)
+    xstd = x
+    if ytype == "continuous":
+        ydis = discretize(y, n_slices)
+    else:
+        ydis = y
+    yless = ydis
+    ylabel = []
+    for i in range(n):
+        if np.var(yless) != 0:
+            ylabel.append(yless[0])
+            yless = yless[yless != yless[0]]
+    ylabel.append(yless[0])
+    prob = []
+    exy = []
+
+    for i in range(n_slices):
+        prob.append(len(ydis[ydis == ylabel[i]]) / n)
+        xres = np.apply_along_axis(np.mean, 0, xstd[ydis == ylabel[i], :])
+        exy.append(xres)
+
+    exy = np.vstack(exy)
+    sirmat = np.matmul(np.matmul(exy.T, np.diag(np.array(prob))), exy)
+    eig_vals, eig_vecs = np.linalg.eigh(sirmat)
+    idx = eig_vals.argsort()[::-1]
+    eig_vals = eig_vals[idx]
+    eig_vecs = eig_vecs[:, idx]
+    if scale_data:
+        return np.matmul(signsqrt, eig_vecs[:, 0:d])
+    else:
+        return eig_vecs[:, 0:d]
+
+
+def SAVE(x, y, n_slices, d, ytype="continuous", center_data=True, scale_data=True):
+    """
+    computes  the Sliced Average Variance Estimator of the central subspace.
+    The algortihm is inspired from :
+    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
+    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
+    
+    Input : 
+        X : input Data as Matrix 
+        Y : input data as vector or 1d matrix.
+        n_slices, int : number of slices 
+        d, int : dimension of the central subspace.
+        ytype, str : either discrete or continuous
+        center_data, bool, if true, the data is centered before SIR
+        scale_data, bool, if true, the data is sclaed before SIR
+        
+    Output :
+         The estimated basis of the central subspace.
+        
+    """
+    y = np.asarray(y).flatten()
+    p = x.shape[1]
+    n = x.shape[0]  ## zxception if d> p or x.shape[0] != y.shape[0]
+    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
+    #     xc = x - x.mean(axis=0)
+    #     xstd= np.matmul(xc,signsqrt)
+    if center_data:
+        x = x - x.mean(axis=0)
+    if scale_data:
+        x = np.matmul(x, signsqrt)
+
+    xstd = x
+
+    if ytype == "continuous":
+        ydis = discretize(y, n_slices)
+    else:
+        ydis = y
+    yless = ydis
+    ylabel = []
+    for i in range(n):
+        if np.var(yless) != 0:
+            ylabel.append(yless[0])
+            yless = yless[yless != yless[0]]
+    ylabel.append(yless[0])
+    prob = []
+    for i in range(n_slices):
+        prob.append(len(ydis[ydis == ylabel[i]]) / n)
+
+    vxy = np.zeros((n_slices, p, p))
+    for i in range(n_slices):
+        vxy[i, :, :] = np.cov(xstd[ydis == ylabel[i], :], rowvar=0)
+
+    savemat = np.zeros((p, p))
+    for i in range(n_slices):
+        savemat = savemat + prob[i] * np.matmul(
+            (vxy[i, :, :] - np.identity(p)), (vxy[i, :, :] - np.identity(p))
+        )
+
+    eig_vals, eig_vecs = np.linalg.eigh(savemat)
+    idx = eig_vals.argsort()[::-1]
+    eig_vals = eig_vals[idx]
+    eig_vecs = eig_vecs[:, idx]
+    if scale_data:
+        return np.matmul(signsqrt, eig_vecs[:, 0:d])
+    else:
+        return eig_vecs[:, 0:d]
+
+
+def DR(x, y, n_slices, d, ytype="continuous", center_data=True, scale_data=True):
+    """
+    computes  the Directional Regression Estimator of the central subspace.
+    The algortihm is inspired from :
+    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
+    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
+    
+    Input : 
+        X : input Data as Matrix 
+        Y : input data as vector or 1d matrix.
+        n_slices, int : number of slices 
+        d, int : dimension of the central subspace.
+        ytype, str : either discrete or continuous
+        center_data, bool, if true, the data is centered before SIR
+        scale_data, bool, if true, the data is sclaed before SIR
+        
+    Output :
+         The estimated basis of the central subspace.
+        
+    """
+    y = np.asarray(y).flatten()
+    p = x.shape[1]
+    n = x.shape[0]  ## zxception if d> p or x.shape[0] != y.shape[0]
+    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
+
+    if center_data:
+        x = x - x.mean(axis=0)
+    if scale_data:
+        x = np.matmul(x, signsqrt)
+
+    xstd = x
+    if ytype == "continuous":
+        ydis = discretize(y, n_slices)
+    else:
+        ydis = y
+    yless = ydis
+    ylabel = []
+    for i in range(n):
+        if np.var(yless) != 0:
+            ylabel.append(yless[0])
+            yless = yless[yless != yless[0]]
+    ylabel.append(yless[0])
+    prob = []
+    exy = []
+
+    for i in range(n_slices):
+        prob.append(len(ydis[ydis == ylabel[i]]) / n)
+
+    vxy = np.zeros((n_slices, p, p,))
+    exy = []
+    for i in range(n_slices):
+        vxy[i, :, :,] = np.cov(xstd[ydis == ylabel[i], :], rowvar=0)
+        xres = np.apply_along_axis(np.mean, 0, xstd[ydis == ylabel[i], :])
+        exy.append(xres)
+
+    exy = np.vstack(exy)
+    mat1 = np.zeros((p, p))
+    mat2 = np.zeros((p, p))
+
+    for i in range(n_slices):
+        mat1 = mat1 + prob[i] * np.matmul(
+            (
+                vxy[i, :, :]
+                + np.matmul(exy[i, :].reshape((-1, 1)), exy[i, :].reshape((-1, 1)).T)
+            ),
+            (
+                vxy[i, :, :]
+                + np.matmul(exy[i, :].reshape((-1, 1)), exy[i, :].reshape((-1, 1)).T)
+            ),
+        )
+        mat2 = mat2 + prob[i] * (
+            np.matmul(exy[i, :].reshape((-1, 1)), exy[i, :].reshape((-1, 1)).T)
+        )
+
+    out = (
+        2 * mat1
+        + 2 * np.matmul(mat2, mat2)
+        + 2 * np.sum(np.diag(mat2)) * mat2
+        - 2 * np.identity(p)
+    )
+    eig_vals, eig_vecs = np.linalg.eigh(out)
+    idx = eig_vals.argsort()[::-1]
+    eig_vals = eig_vals[idx]
+    eig_vecs = eig_vecs[:, idx]
+    if scale_data:
+        return np.matmul(signsqrt, eig_vecs[:, 0:d])
+    else:
+        return eig_vecs[:, 0:d]
+
+
+def PHD(x, y, d, center_data=True, scale_data=True):
+
+    """
+    computes  the Principal Hessian Dimension estimator of the central subspace.
+    The algortihm is inspired from :
+    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
+    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
+    
+    Input : 
+        X : input Data as Matrix 
+        Y : input data as vector or 1d matrix.
+        d, int : dimension of the central subspace.
+        center_data, bool, if true, the data is centered before SIR
+        scale_data, bool, if true, the data is sclaed before SIR
+        
+    Output :
+         The estimated basis of the central subspace.
+    
+    """
+    n = x.shape[0]
+    if len(y.shape) < 2:
+        y = y.reshape((-1, 1))
+    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
+    if center_data:
+        x = x - x.mean(axis=0)
+    if scale_data:
+        x = np.matmul(x, signsqrt)
+
+    yc = y - np.mean(y, axis=0)
+    out = np.matmul(np.multiply(x, yc).T, x) / n
+    eig_vals, eig_vecs = np.linalg.eigh(out)
+    idx = eig_vals.argsort()[::-1]
+    eig_vals = eig_vals[idx]
+    eig_vecs = eig_vecs[:, idx]
+
+    if scale_data:
+        return np.matmul(signsqrt, eig_vecs[:, 0:d])
+    else:
+        return eig_vecs[:, 0:d]
+
+
+def IHT(x, y, d, center_data=True, scale_data=True):
+    """
+    computes  the Iterative Hessian Transformation estimator of the central subspace.
+    The algortihm is inspired from :
+    "Bing, Li .(2018). Sufficient Dimension Reduction: Methods and Applications with R.
+    Chapman & Hall/CRC Monographs on Statistics and Applied Probability."
+    
+    Input : 
+        X : input Data as Matrix 
+        Y : input data as vector or 1d matrix.
+        d, int : dimension of the central subspace.
+        center_data, bool, if true, the data is centered before SIR
+        scale_data, bool, if true, the data is sclaed before SIR
+        
+    Output :
+         The estimated basis of the central subspace.
+        
+    """
+
+    p = x.shape[1]
+    ## zxception if d> p or x.shape[0] != y.shape[0]
+    if len(y.shape) < 2:
+        y = y.reshape((-1, 1))
+    signsqrt = matpower(np.cov(x, rowvar=0), -0.5)
+
+    if center_data:
+        x = x - x.mean(axis=0)
+    if scale_data:
+        x = np.matmul(x, signsqrt)
+
+    covxy = np.cov(x, y, rowvar=0)[-1, :-1]
+    covx = np.cov(x, rowvar=0)
+    mat = covxy
+    for i in range(1, p):
+        mat = np.column_stack((mat, np.matmul(matpower(covx, i), covxy)))
+
+    out = np.matmul(mat, mat.T)
+    eig_vals, eig_vecs = np.linalg.eigh(out)
+    idx = eig_vals.argsort()[::-1]
+    eig_vals = eig_vals[idx]
+    eig_vecs = eig_vecs[:, idx]
+    if scale_data:
+        return np.matmul(signsqrt, eig_vecs[:, 0:d])
+    else:
+        return eig_vecs[:, 0:d]
+
+
+# def ballcov_func(beta, *args):
+#     """
+#     Objective function  for BCOV-SDR.
+#     Will only work after uncommenting the import Ball statement above
+#     """
+
+#     X= args[0]
+#     Y= args[1]
+#     h=args[2]
+#     beta = np.reshape(beta,(-1,h),order = 'F')
+#     X_dat = np.matmul(X, beta)
+#     res = Ball.bcov_test(X_dat,Y,num_permutations=0)[0]
+#     return(-10*res)
```

## direpack/test/__init__.py

```diff
@@ -1,5 +1,5 @@
-__name__ = "test"
-__author__ = "Emmanuel Jordy and Sven"
-__license__ = "MIT"
-__version__ = "0.0.3"
-__date__ = "2024-02-23"
+__name__ = "test"
+__author__ = "Emmanuel Jordy and Sven"
+__license__ = "MIT"
+__version__ = "0.0.4"
+__date__ = "2024-05-23"
```

## direpack/test/test_dicomo.py

```diff
@@ -26,15 +26,15 @@
     def tearDownClass(cls):
         print('...teardownClass')
         
         
     @classmethod    
     def setUp(self):
         self.data=ps.read_csv("./data/Returns_shares.csv")
-        self.datav = np.matrix(self.data.values[:,2:8].astype('float64'))
+        self.datav = np.array(self.data.values[:,2:8].astype('float64'))
         self.est = dicomo()
         self.x = self.datav[:,1]
         self.y = self.datav[:,0]
         self.n=self.data.shape[0]
         self.p = self.data.shape[1]
         
         
@@ -51,20 +51,20 @@
     
     def test_mom(self):
         """ Tests functions to compute moments"""
         
         self.assertAlmostEquals(self.est.fit(self.x,biascorr=False),np.var(self.x))# biased var
         self.assertAlmostEquals(self.est.fit(self.x,biascorr=True),np.var(self.x)*self.n/(self.n-1))#unbiased var
         self.est.set_params(center='median')
-        self.assertAlmostEquals(self.est.fit(self.x),srs.mad(self.x)[0],places=4)
+        self.assertAlmostEquals(self.est.fit(self.x),srs.mad(self.x),places=4)
         self.est.set_params(center='mean')
-        self.assertAlmostEquals(self.est.fit(self.x,biascorr=False,order=3),sps.moment(self.x,3)[0])#third moment
+        self.assertAlmostEquals(self.est.fit(self.x,biascorr=False,order=3),sps.moment(self.x,3))#third moment
         self.est.set_params(mode='skew')
-        self.assertAlmostEquals(self.est.fit(self.x,biascorr=False),sps.skew(self.x)[0])# skew without small sample corr
-        self.assertAlmostEquals(self.est.fit(self.x,biascorr=True),sps.skew(self.x,bias=False)[0])
+        self.assertAlmostEquals(self.est.fit(self.x,biascorr=False),sps.skew(self.x))# skew without small sample corr
+        self.assertAlmostEquals(self.est.fit(self.x,biascorr=True),sps.skew(self.x,bias=False))
         
         
         
         
         
         
     def test_como(self):
```

## direpack/utils/utils.py

 * *Ordering differences only*

```diff
@@ -1,108 +1,108 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Apr 13 16:08:22 2020
-
-@author: sven
-"""
-
-import pandas as ps
-import numpy as np
-
-
-class MyException(Exception):
-    pass
-
-
-def convert_X_input(X):
-
-    if type(X) == ps.core.frame.DataFrame:
-        X = X.to_numpy().astype('float64')
-    return(X)
-
-
-def convert_y_input(y):
-
-    if type(y) in [ps.core.frame.DataFrame, ps.core.series.Series]:
-        y = y.to_numpy().T.astype('float64')
-    return(y)
-
-
-def const_xscale(beta, *args):
-    X = args[0]
-    h = args[1]
-    i = args[2]
-    j = args[3]
-    beta = np.reshape(beta, (-1, h), order='F')
-    covx = np.cov(X, rowvar=False)
-    ans = np.matmul(np.matmul(beta.T, covx), beta) - np.identity(h)
-    return(ans[i, j])
-
-
-def const_zscale(beta, *args):
-    X = args[0]
-    h = args[1]
-    i = args[2]
-    j = args[3]
-    beta = np.reshape(beta, (-1, h), order='F')
-    covx = np.identity(X.shape[1])
-    ans = np.matmul(np.matmul(beta.T, covx), beta) - np.identity(h)
-    return(ans[i, j])
-
-
-def _predict_check_input(Xn):
-    if type(Xn) == ps.core.series.Series:
-        Xn = Xn.to_numpy()
-    if Xn.ndim == 1:
-        Xn = Xn.reshape((1, -1))
-    if type(Xn) == ps.core.frame.DataFrame:
-        Xn = Xn.to_numpy()
-    n, p = Xn.shape
-    return (n, p, Xn)
-
-
-def _check_input(X):
-
-    if(type(X) in (np.matrix, ps.core.frame.DataFrame, ps.core.series.Series)):
-        X = np.array(X)
-
-    if (X.dtype == np.dtype('O')):
-        X = X.astype('float64')
-
-    if X.ndim == 1:
-        X = X.reshape((1, -1))
-
-    n, p = X.shape
-
-    if n == 1:
-        if p >= 2:
-            X = X.reshape((-1, 1))
-    return(X)
-
-
-def nandot(X, y):
-
-    p, n = X.shape
-    assert n == len(y), "Number of rows in X and y needs to agree"
-    if len(y.shape) > 1:
-        y = y.reshape(-1)
-    product = [np.nansum(np.multiply(X[i, :], y)) for i in range(p)]
-
-    return np.array(product).reshape((-1, 1))
-
-
-def nanmatdot(X, Y):
-
-    p, n = X.shape
-    if len(Y.shape) == 1:
-        return nandot(X, Y)
-    else:
-        m, q = Y.shape
-        assert n == m, "Matrix diomensions need to agree"
-        if q == 1:
-            return nandot(X, Y)
-        else:
-            product = [[np.nansum(np.multiply(X[i, :], Y[:, j]))
-                        for i in range(p)] for j in range(q)]
-
-            return np.array(product)
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Apr 13 16:08:22 2020
+
+@author: sven
+"""
+
+import pandas as ps
+import numpy as np
+
+
+class MyException(Exception):
+    pass
+
+
+def convert_X_input(X):
+
+    if type(X) == ps.core.frame.DataFrame:
+        X = X.to_numpy().astype('float64')
+    return(X)
+
+
+def convert_y_input(y):
+
+    if type(y) in [ps.core.frame.DataFrame, ps.core.series.Series]:
+        y = y.to_numpy().T.astype('float64')
+    return(y)
+
+
+def const_xscale(beta, *args):
+    X = args[0]
+    h = args[1]
+    i = args[2]
+    j = args[3]
+    beta = np.reshape(beta, (-1, h), order='F')
+    covx = np.cov(X, rowvar=False)
+    ans = np.matmul(np.matmul(beta.T, covx), beta) - np.identity(h)
+    return(ans[i, j])
+
+
+def const_zscale(beta, *args):
+    X = args[0]
+    h = args[1]
+    i = args[2]
+    j = args[3]
+    beta = np.reshape(beta, (-1, h), order='F')
+    covx = np.identity(X.shape[1])
+    ans = np.matmul(np.matmul(beta.T, covx), beta) - np.identity(h)
+    return(ans[i, j])
+
+
+def _predict_check_input(Xn):
+    if type(Xn) == ps.core.series.Series:
+        Xn = Xn.to_numpy()
+    if Xn.ndim == 1:
+        Xn = Xn.reshape((1, -1))
+    if type(Xn) == ps.core.frame.DataFrame:
+        Xn = Xn.to_numpy()
+    n, p = Xn.shape
+    return (n, p, Xn)
+
+
+def _check_input(X):
+
+    if(type(X) in (np.matrix, ps.core.frame.DataFrame, ps.core.series.Series)):
+        X = np.array(X)
+
+    if (X.dtype == np.dtype('O')):
+        X = X.astype('float64')
+
+    if X.ndim == 1:
+        X = X.reshape((1, -1))
+
+    n, p = X.shape
+
+    if n == 1:
+        if p >= 2:
+            X = X.reshape((-1, 1))
+    return(X)
+
+
+def nandot(X, y):
+
+    p, n = X.shape
+    assert n == len(y), "Number of rows in X and y needs to agree"
+    if len(y.shape) > 1:
+        y = y.reshape(-1)
+    product = [np.nansum(np.multiply(X[i, :], y)) for i in range(p)]
+
+    return np.array(product).reshape((-1, 1))
+
+
+def nanmatdot(X, Y):
+
+    p, n = X.shape
+    if len(Y.shape) == 1:
+        return nandot(X, Y)
+    else:
+        m, q = Y.shape
+        assert n == m, "Matrix diomensions need to agree"
+        if q == 1:
+            return nandot(X, Y)
+        else:
+            product = [[np.nansum(np.multiply(X[i, :], Y[:, j]))
+                        for i in range(p)] for j in range(q)]
+
+            return np.array(product)
```

## Comparing `direpack-1.1.2.dist-info/LICENSE` & `direpack-1.1.3.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-MIT License
-
-Copyright (c) 2020 Sven Serneels
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
+MIT License
+
+Copyright (c) 2020 Sven Serneels
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
```

## Comparing `direpack-1.1.2.dist-info/METADATA` & `direpack-1.1.3.dist-info/METADATA`

 * *Files 7% similar despite different names*

```diff
@@ -1,120 +1,117 @@
-Metadata-Version: 2.1
-Name: direpack
-Version: 1.1.2
-Summary: A Python 3 Library for State-of-the-Art Statistical Dimension Reduction Methods
-Home-page: https://github.com/SvenSerneels/direpack
-Author: Emmanuel Jordy Menvouta, Sven Serneels, Tim Verdonck
-Author-email: svenserneels@gmail.com
-License: UNKNOWN
-Platform: UNKNOWN
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: numpy (>=1.5.0)
-Requires-Dist: scipy (>=0.8.0)
-Requires-Dist: matplotlib (>=2.2.0)
-Requires-Dist: scikit-learn (>=0.18.0)
-Requires-Dist: pandas (>=0.19.0)
-Requires-Dist: statsmodels (>=0.8.0)
-Requires-Dist: dcor (>=0.3)
-
-`direpack`: a Python 3 library for state-of-the-art statistical dimension reduction techniques
-==============================================================================================
-
-This package delivers a `scikit-learn` compatible Python 3 package for sundry state-of-the art multivariate statistical methods, with 
-a focus on dimension reduction. 
-
-The categories of methods delivered in this package, are: 
-- Projection pursuit dimension reduction (`ppdire`) 
-- Sufficient dimension reduction (`sudire`)
-- Robust M-estimators for dimension reduction (`sprm`)
-each of which are presented as `scikit-learn` compatible objects in the corresponding folders.
-
-We hope that this package leads to scientific success. If it does so, we kindly ask to cite the [official `direpack` publication](https://www.sciencedirect.com/science/article/pii/S235271102200200X) \[0\], as well as the original publication of the corresponding method.  
-
-The package also contains a set of tools for pre- and postprocessing: 
-- The `preprocessing` folder provides classical and robust centring and scaling, as well as spatial sign transforms \[4\] and the robbustness inducing wrapping transformation \[15\].  
-- The `dicomo` folder contains a versatile class to access a wide variety of moment and co-moment statistics, and statistics derived from those. Check out the [dicomo Documentation file](https://github.com/SvenSerneels/direpack/blob/master/docs/dicomo.md) and the [dicomo Examples Notebook](https://github.com/SvenSerneels/direpack/blob/master/examples/dicomo_example.ipynb).
-- Plotting utilities in the `plot` folder 
-- Cross-validation utilities in the `cross-validation` folder  
-
- ![AIG sprm score space](https://github.com/SvenSerneels/direpack/blob/master/img/AIG_T12.png "AIG SPRM score space")
-
-
-Methods in the `sprm` folder
-----------------------------
-- The estimator (`sprm.py`) \[1\]
-- The Sparse NIPALS (SNIPLS) estimator \[3\](`snipls.py`)
-- Robust M regression estimator (`rm.py`)
-- Ancillary functions for M-estimation (`_m_support_functions.py`)
-
-Methods in the `ppdire` folder
-------------------------------
-The `ppdire` class will give access to a wide range of projection pursuit dimension reduction techniques.
-These include slower approximate estimates for well-established methods such as PCA, PLS and continuum regression. 
-However, the class provides unique access to a set of robust options, such as robust continuum regression (RCR) \[5\], through its native `grid` optimization algorithm, first 
-published for RCR as well \[6\]. Moreover, `ppdire` is also a great gateway to calculate generalized betas, using the CAPI projection index \[7\]. 
-
-The code is orghanized in 
-- `ppdire.py` - the main PP dimension reduction class 
-- `capi.py` - the co-moment analysis projection index.      
-
-Methods in the `sudire` folder
-------------------------------
-The `sudire` folder gives access to an extensive set of methods that resort under the umbrella of sufficient dimension reduction. 
-These range from meanwhile long-standing, well-accepted approaches, such as sliced inverse regression (SIR) and the closely related SAVE \[8,9\], 
-through methods such as directional regression \[10\] and principal Hessian directions \[11\], and more. However, the package also contains some 
-of the most recently developed, state-of-the-art sufficient dimension reduction techniques, that require no distributional assumptions. 
-The options provided in this category are based on energy statistics (distance covariance \[12\] or martingale difference divergence \[13\]) and 
-ball statistics (ball covariance) \[14\]. All of these options can be called by setting the corresponding parameters in the `sudire` class, cf. [the docs](https://github.com/SvenSerneels/direpack/blob/master/docs/sudire.md). 
-Note: the ball covariance option will require some lines to be uncommented as indicated. We decided not to make that option generally available, 
-since it depends on the `Ball` package that seems to be difficult to install on certain architectures. 
-
-How to install
---------------
-The package is distributed through PyPI, so install through: 
-        
-        pip install direpack
-        
-Note that some of the key methods in the `sudire` subpackage rely on the IPOPT 
-optimization package, which according to their recommendation, can best be installed
-directly as: 
-
-        conda install -c conda-forge cyipopt
-        
-Documentation
-=============
-
-- Detailed documentation can be found in the [ReadTheDocs page](https://direpack.readthedocs.io/en/latest/index.html). 
-- A more extensive description on the background is presented in the [official `direpack` publication](https://www.sciencedirect.com/science/article/pii/S235271102200200X). 
-- Examples on how to use each of the `dicomo`, `ppdire`, `sprm` and `sudire` classes are presented as Jupyter notebooks in the [examples](https://github.com/SvenSerneels/direpack/blob/master/examples) folder
-- Furthemore, the [docs](https://github.com/SvenSerneels/direpack/blob/master/docs) folder contains a few markdown files on usage of the classes. 
-
-  
-        
-References
-==========
-0. [`direpack`: A Python 3 package for state-of-the-art statistical dimensionality reduction methods](https://www.sciencedirect.com/science/article/pii/S235271102200200X), Emmanuel Jordy Menvouta, Sven Serneels, Tim Verdonck, SoftwareX, 21 (2023), 101282.
-1. [Sparse partial robust M regression](https://www.sciencedirect.com/science/article/abs/pii/S0169743915002440), Irene Hoffmann, Sven Serneels, Peter Filzmoser, Christophe Croux, Chemometrics and Intelligent Laboratory Systems, 149 (2015), 50-59.
-2. [Partial robust M regression](https://doi.org/10.1016/j.chemolab.2005.04.007), Sven Serneels, Christophe Croux, Peter Filzmoser, Pierre J. Van Espen, Chemometrics and Intelligent Laboratory Systems, 79 (2005), 55-64.
-3. [Sparse and robust PLS for binary classification](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.2775), I. Hoffmann, P. Filzmoser, S. Serneels, K. Varmuza, Journal of Chemometrics, 30 (2016), 153-162.
-4. [Spatial Sign Preprocessing:  A Simple Way To Impart Moderate Robustness to Multivariate Estimators](https://pubs.acs.org/doi/abs/10.1021/ci050498u), Sven Serneels, Evert De Nolf, Pierre J. Van Espen, Journal of Chemical Information and Modeling, 46 (2006), 1402-1409.
-5. [Robust Continuum Regression](https://www.sciencedirect.com/science/article/abs/pii/S0169743904002667), Sven Serneels, Peter Filzmoser, Christophe Croux, Pierre J. Van Espen, Chemometrics and Intelligent Laboratory Systems, 76 (2005), 197-204.
-6. [Robust Multivariate Methods: The Projection Pursuit Approach](https://link.springer.com/chapter/10.1007/3-540-31314-1_32), Peter Filzmoser, Sven Serneels, Christophe Croux and Pierre J. Van Espen, in: From Data and Information Analysis to Knowledge Engineering, Spiliopoulou, M., Kruse, R., Borgelt, C., Nuernberger, A. and Gaul, W., eds., Springer Verlag, Berlin, Germany, 2006, pages 270--277.
-7. [Projection pursuit based generalized betas accounting for higher order co-moment effects in financial market analysis](https://arxiv.org/pdf/1908.00141.pdf), Sven Serneels, in: JSM Proceedings, Business and Economic Statistics Section. Alexandria, VA: American Statistical Association, 2019, 3009-3035.
-8. [Sliced Inverse Regression for Dimension Reduction](https://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475035) Li K-C,  Journal of the American Statistical Association (1991), 86, 316-327.
-9. [Sliced Inverse Regression for Dimension Reduction: Comment](https://www.jstor.org/stable/2290564?seq=1#metadata_info_tab_contents),  R.D. Cook, and Sanford Weisberg, Journal of the American Statistical Association (1991), 86, 328-332.
-10. [On directional regression for dimension reduction](https://doi.org/10.1198/016214507000000536) ,  B. Li and S.Wang, Journal of the American Statistical Association (2007), 102:997–1008.
-11. [On principal hessian directions for data visualization and dimension reduction:Another application of stein’s lemma](https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10476258), K.-C. Li. , Journal of the American Statistical Association(1992)., 87,1025–1039.
-12. [Sufficient Dimension Reduction via Distance Covariance](https://doi.org/10.1080/10618600.2015.1026601), Wenhui Sheng and Xiangrong Yin in: Journal of Computational and Graphical Statistics (2016),  25, issue 1, pages 91-104.
-13. [A martingale-difference-divergence-based estimation of central mean subspace](https://dx.doi.org/10.4310/19-SII562), Yu Zhang, Jicai Liu, Yuesong Wu and Xiangzhong Fang, in: Statistics and Its Interface (2019),  12, number 3, pages 489-501.
-14. [Robust Sufficient Dimension Reduction Via Ball Covariance](https://www.sciencedirect.com/science/article/pii/S0167947319301380) Jia Zhang and Xin Chen, Computational Statistics and Data Analysis 140 (2019) 144–154 
-15. [Fast Robust Correlation for High-Dimensional Data](https://www.tandfonline.com/doi/full/10.1080/00401706.2019.1677270) Jakob Raymaekers and Peter J. Rousseeuw, Technometrics, 63 (2021), 184-198. 
- 
-        
-[Release Notes](https://github.com/SvenSerneels/direpack/blob/master/direpack_Release_Notes.md) can be checked out in the repository.  
-
-[A list of possible topics for further development](https://github.com/SvenSerneels/direpack/blob/master/direpack_Future_Dev.md) is provided as well. Additions and comments are welcome!
-
+Metadata-Version: 2.1
+Name: direpack
+Version: 1.1.3
+Summary: A Python 3 Library for State-of-the-Art Statistical Dimension Reduction Methods
+Home-page: https://github.com/SvenSerneels/direpack
+Author: Emmanuel Jordy Menvouta, Sven Serneels, Tim Verdonck
+Author-email: svenserneels@gmail.com
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: numpy (>=1.5.0)
+Requires-Dist: scipy (>=0.8.0)
+Requires-Dist: matplotlib (>=2.2.0)
+Requires-Dist: scikit-learn (>=0.18.0)
+Requires-Dist: pandas (>=0.19.0)
+Requires-Dist: statsmodels (>=0.8.0)
+Requires-Dist: dcor (>=0.3)
+
+`direpack`: a Python 3 library for state-of-the-art statistical dimension reduction techniques
+==============================================================================================
+
+This package delivers a `scikit-learn` compatible Python 3 package for sundry state-of-the art multivariate statistical methods, with 
+a focus on dimension reduction. 
+
+The categories of methods delivered in this package, are: 
+- Projection pursuit dimension reduction (`ppdire`) 
+- Sufficient dimension reduction (`sudire`)
+- Robust M-estimators for dimension reduction (`sprm`)
+each of which are presented as `scikit-learn` compatible objects in the corresponding folders.
+
+We hope that this package leads to scientific success. If it does so, we kindly ask to cite the [official `direpack` publication](https://www.sciencedirect.com/science/article/pii/S235271102200200X) \[0\], as well as the original publication of the corresponding method.  
+
+The package also contains a set of tools for pre- and postprocessing: 
+- The `preprocessing` folder provides classical and robust centring and scaling, as well as spatial sign transforms \[4\] and the robbustness inducing wrapping transformation \[15\].  
+- The `dicomo` folder contains a versatile class to access a wide variety of moment and co-moment statistics, and statistics derived from those. Check out the [dicomo Documentation file](https://github.com/SvenSerneels/direpack/blob/master/docs/dicomo.md) and the [dicomo Examples Notebook](https://github.com/SvenSerneels/direpack/blob/master/examples/dicomo_example.ipynb).
+- Plotting utilities in the `plot` folder 
+- Cross-validation utilities in the `cross-validation` folder  
+
+ ![AIG sprm score space](https://github.com/SvenSerneels/direpack/blob/master/img/AIG_T12.png "AIG SPRM score space")
+
+
+Methods in the `sprm` folder
+----------------------------
+- The estimator (`sprm.py`) \[1\]
+- The Sparse NIPALS (SNIPLS) estimator \[3\](`snipls.py`)
+- Robust M regression estimator (`rm.py`)
+- Ancillary functions for M-estimation (`_m_support_functions.py`)
+
+Methods in the `ppdire` folder
+------------------------------
+The `ppdire` class will give access to a wide range of projection pursuit dimension reduction techniques.
+These include slower approximate estimates for well-established methods such as PCA, PLS and continuum regression. 
+However, the class provides unique access to a set of robust options, such as robust continuum regression (RCR) \[5\], through its native `grid` optimization algorithm, first 
+published for RCR as well \[6\]. Moreover, `ppdire` is also a great gateway to calculate generalized betas, using the CAPI projection index \[7\]. 
+
+The code is orghanized in 
+- `ppdire.py` - the main PP dimension reduction class 
+- `capi.py` - the co-moment analysis projection index.      
+
+Methods in the `sudire` folder
+------------------------------
+The `sudire` folder gives access to an extensive set of methods that resort under the umbrella of sufficient dimension reduction. 
+These range from meanwhile long-standing, well-accepted approaches, such as sliced inverse regression (SIR) and the closely related SAVE \[8,9\], 
+through methods such as directional regression \[10\] and principal Hessian directions \[11\], and more. However, the package also contains some 
+of the most recently developed, state-of-the-art sufficient dimension reduction techniques, that require no distributional assumptions. 
+The options provided in this category are based on energy statistics (distance covariance \[12\] or martingale difference divergence \[13\]) and 
+ball statistics (ball covariance) \[14\]. All of these options can be called by setting the corresponding parameters in the `sudire` class, cf. [the docs](https://github.com/SvenSerneels/direpack/blob/master/docs/sudire.md). 
+Note: the ball covariance option will require some lines to be uncommented as indicated. We decided not to make that option generally available, 
+since it depends on the `Ball` package that seems to be difficult to install on certain architectures. 
+
+How to install
+--------------
+The package is distributed through PyPI, so install through: 
+        
+        pip install direpack
+        
+Note that some of the key methods in the `sudire` subpackage rely on the IPOPT 
+optimization package, which according to their recommendation, can best be installed
+directly as: 
+
+        conda install -c conda-forge cyipopt
+        
+Documentation
+=============
+
+- Detailed documentation can be found in the [ReadTheDocs page](https://direpack.readthedocs.io/en/latest/index.html). 
+- A more extensive description on the background is presented in the [official `direpack` publication](https://www.sciencedirect.com/science/article/pii/S235271102200200X). 
+- Examples on how to use each of the `dicomo`, `ppdire`, `sprm` and `sudire` classes are presented as Jupyter notebooks in the [examples](https://github.com/SvenSerneels/direpack/blob/master/examples) folder
+- Furthemore, the [docs](https://github.com/SvenSerneels/direpack/blob/master/docs) folder contains a few markdown files on usage of the classes. 
+
+  
+        
+References
+==========
+0. [`direpack`: A Python 3 package for state-of-the-art statistical dimensionality reduction methods](https://www.sciencedirect.com/science/article/pii/S235271102200200X), Emmanuel Jordy Menvouta, Sven Serneels, Tim Verdonck, SoftwareX, 21 (2023), 101282.
+1. [Sparse partial robust M regression](https://www.sciencedirect.com/science/article/abs/pii/S0169743915002440), Irene Hoffmann, Sven Serneels, Peter Filzmoser, Christophe Croux, Chemometrics and Intelligent Laboratory Systems, 149 (2015), 50-59.
+2. [Partial robust M regression](https://doi.org/10.1016/j.chemolab.2005.04.007), Sven Serneels, Christophe Croux, Peter Filzmoser, Pierre J. Van Espen, Chemometrics and Intelligent Laboratory Systems, 79 (2005), 55-64.
+3. [Sparse and robust PLS for binary classification](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.2775), I. Hoffmann, P. Filzmoser, S. Serneels, K. Varmuza, Journal of Chemometrics, 30 (2016), 153-162.
+4. [Spatial Sign Preprocessing:â€‰ A Simple Way To Impart Moderate Robustness to Multivariate Estimators](https://pubs.acs.org/doi/abs/10.1021/ci050498u), Sven Serneels, Evert De Nolf, Pierre J. Van Espen, Journal of Chemical Information and Modeling, 46 (2006), 1402-1409.
+5. [Robust Continuum Regression](https://www.sciencedirect.com/science/article/abs/pii/S0169743904002667), Sven Serneels, Peter Filzmoser, Christophe Croux, Pierre J. Van Espen, Chemometrics and Intelligent Laboratory Systems, 76 (2005), 197-204.
+6. [Robust Multivariate Methods: The Projection Pursuit Approach](https://link.springer.com/chapter/10.1007/3-540-31314-1_32), Peter Filzmoser, Sven Serneels, Christophe Croux and Pierre J. Van Espen, in: From Data and Information Analysis to Knowledge Engineering, Spiliopoulou, M., Kruse, R., Borgelt, C., Nuernberger, A. and Gaul, W., eds., Springer Verlag, Berlin, Germany, 2006, pages 270--277.
+7. [Projection pursuit based generalized betas accounting for higher order co-moment effects in financial market analysis](https://arxiv.org/pdf/1908.00141.pdf), Sven Serneels, in: JSM Proceedings, Business and Economic Statistics Section. Alexandria, VA: American Statistical Association, 2019, 3009-3035.
+8. [Sliced Inverse Regression for Dimension Reduction](https://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475035) Li K-C,  Journal of the American Statistical Association (1991), 86, 316-327.
+9. [Sliced Inverse Regression for Dimension Reduction: Comment](https://www.jstor.org/stable/2290564?seq=1#metadata_info_tab_contents),  R.D. Cook, and Sanford Weisberg, Journal of the American Statistical Association (1991), 86, 328-332.
+10. [On directional regression for dimension reduction](https://doi.org/10.1198/016214507000000536) ,  B. Li and S.Wang, Journal of the American Statistical Association (2007), 102:997â€“1008.
+11. [On principal hessian directions for data visualization and dimension reduction:Another application of steinâ€™s lemma](https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10476258), K.-C. Li. , Journal of the American Statistical Association(1992)., 87,1025â€“1039.
+12. [Sufficient Dimension Reduction via Distance Covariance](https://doi.org/10.1080/10618600.2015.1026601), Wenhui Sheng and Xiangrong Yin in: Journal of Computational and Graphical Statistics (2016),  25, issue 1, pages 91-104.
+13. [A martingale-difference-divergence-based estimation of central mean subspace](https://dx.doi.org/10.4310/19-SII562), Yu Zhang, Jicai Liu, Yuesong Wu and Xiangzhong Fang, in: Statistics and Its Interface (2019),  12, number 3, pages 489-501.
+14. [Robust Sufficient Dimension Reduction Via Ball Covariance](https://www.sciencedirect.com/science/article/pii/S0167947319301380) Jia Zhang and Xin Chen, Computational Statistics and Data Analysis 140 (2019) 144â€“154 
+15. [Fast Robust Correlation for High-Dimensional Data](https://www.tandfonline.com/doi/full/10.1080/00401706.2019.1677270) Jakob Raymaekers and Peter J. Rousseeuw, Technometrics, 63 (2021), 184-198. 
+ 
+        
+[Release Notes](https://github.com/SvenSerneels/direpack/blob/master/direpack_Release_Notes.md) can be checked out in the repository.  
+
+[A list of possible topics for further development](https://github.com/SvenSerneels/direpack/blob/master/direpack_Future_Dev.md) is provided as well. Additions and comments are welcome!
```

## Comparing `direpack-1.1.2.dist-info/RECORD` & `direpack-1.1.3.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-direpack/__init__.py,sha256=2cnjn3zF0Rpb_fi9tpxPnVJ3mDcjRcqMF3P65ESMPwc,1106
-direpack/cross_validation/__init__.py,sha256=o0X5y7n0tsa7EKuEEvRmrCeQxk6t7C0NJ0ZCB28OqOw,255
-direpack/cross_validation/_cv_support_functions.py,sha256=7AmDtxpTwXZAMSKXsVSDJUWRmkDFTvWfSgD-Yz9NTcA,2110
-direpack/dicomo/__init__.py,sha256=IWSFLPGm1RGtQdBX-azbr08pihT987oThtbkaFIc79Y,248
-direpack/dicomo/_dicomo_utils.py,sha256=a0zA42_fPva3IyJIi_Fg0-5y10bLjUgPt_Lg4bfw8JI,13152
-direpack/dicomo/dicomo.py,sha256=YmxI1UKBTjx1wbJcH1LZUa4ULXWlqd-46M98qU68-ec,18162
-direpack/ipopt_temp/__init__.py,sha256=EOGW5Ug1uGsCRCT9aDIrFbeNYHFOLblZNzel1YogwEU,397
-direpack/ipopt_temp/ipopt_wrapper.py,sha256=y_qNhXCkDp1BK9LztFHYW5iTWvB3BllhtpuCnO841Hs,8058
-direpack/ipopt_temp/jacobian.py,sha256=mwNeX1aOU9ckgEJzQfgiJJUFwKF1vT6lYgM0Fr44QV4,2191
-direpack/plot/__init__.py,sha256=5apkCSbaFBI3W7t1x-HgJyLJgvNpNl_Q0v4MeUz6C4I,245
+direpack/__init__.py,sha256=uT5BF80mQ18McG4pC1LXc-DgfiXdjAEopL0rb2ZGovM,1146
+direpack/cross_validation/__init__.py,sha256=sc76f-V_eaSFS_SpUaT0AJY8AmOyBWFhann96UXYut8,270
+direpack/cross_validation/_cv_support_functions.py,sha256=0fk02C4mttsfbe87X9Wh7dFtk7scPZq7xJxqzCWJa9c,2167
+direpack/dicomo/__init__.py,sha256=BQIuq0BywoiyJjt7CHYqTUAkehtbFJxlUCjsYpL4k1I,267
+direpack/dicomo/_dicomo_utils.py,sha256=WFFj8RDTVQ2jmy3x_1p0hfy2qqL5lFUZdO9XbYr3q2k,13616
+direpack/dicomo/dicomo.py,sha256=YByV3RNX55gOeet-RjjjIw79Mto2pmqJaxwM6vN8MSw,18614
+direpack/ipopt_temp/__init__.py,sha256=dTiCgkCaN1TBXWFsaZCgt7Zw-MTCYdiAvCnax30jvp8,416
+direpack/ipopt_temp/ipopt_wrapper.py,sha256=F3bKFHVtOqyd3ezXvRjtOi4FA3gDQ3BWJuHSOInCV_U,8301
+direpack/ipopt_temp/jacobian.py,sha256=vlGIymd5pjU1hRV_R04y8_Ofzt-ZwfI97IH80UenDmo,2256
+direpack/plot/__init__.py,sha256=BzWceOB5cdgKecispH8nGQIJKZjtrnlxOUkPj8BO71g,262
 direpack/plot/ppdire_plot.py,sha256=XYggHStkxS7BsLlTEgUs79eN99OvnabZrLpUMfuikkI,6468
-direpack/plot/sprm_plot.py,sha256=HOG_ragY56pRrkVrHlISBKHJntmAsHQcn0K6Z1bC97k,17165
+direpack/plot/sprm_plot.py,sha256=0XxKuH9pWLmCzHinGgEy6pcEz88WvLFBuMZpEX4g7hY,17544
 direpack/plot/sudire_plot.py,sha256=KXfhLFyN-tH2C2-c6KhNckEpQXZemwxAdljzR0Bz6FE,6321
-direpack/ppdire/__init__.py,sha256=qnsFn98cABWrc-P5ng5Iqaczl_tiSBbukRYSX9MHmno,243
-direpack/ppdire/_ppdire_utils.py,sha256=cM0aFHpWt4CAfuZvMYrKe3WN9-qQkZbHtIP_KsTHcpA,6170
-direpack/ppdire/capi.py,sha256=Ow2weR92JCOG-Poc35BEkCu_82gF86i45NpFhD5kWrw,6928
-direpack/ppdire/ppdire.py,sha256=0-xP000GoGy9lkEby58x61lFeg70enR0z47s42_UwK0,34828
-direpack/preprocessing/__init__.py,sha256=VHtg4aqw4BObPxuGGUeAS7k3QiHF0e86kwepNY16vbw,250
-direpack/preprocessing/_gsspp_utils.py,sha256=Hnc-quzhIHx63oW5Dy_JJ8Dz3nDvN89W9NYPSQs8BY8,3867
-direpack/preprocessing/_preproc_utilities.py,sha256=FH78_3j3ZXe1MHJe-aF_xuKCAOFTRckbBmgQMtGT-V0,8644
-direpack/preprocessing/gsspp.py,sha256=VsLTNtHmN017kCEk3czhcdVV-uRuLSKzxi4OyPbC7Ww,4934
-direpack/preprocessing/robcent.py,sha256=8uEdL9i1dF99EFVgo8axaNXl_2dumEWq_MHepMx0mvU,9187
-direpack/sprm/__init__.py,sha256=r3I15vnce_njIAICMegNWguSt0NeKv1Cib0iLX4fjAo,241
-direpack/sprm/_m_support_functions.py,sha256=nHCXc7Avo126QGv6nIPH9NCQHK4GmvEJbXUjZWHnAjA,1142
-direpack/sprm/rm.py,sha256=3xJ7uL-JIWRtVxtCf7LfpC_3v8B2K7oAL4B8R-6HSBk,10180
-direpack/sprm/snipls.py,sha256=5_kUhaeg37cR-C9_6vaoIK6CJIHlUzZXJ0IM-7C-MgI,11309
-direpack/sprm/sprm.py,sha256=pe0VvjSw-52galPbWa2T7Am3nf9O_fC0jJEH5q5RIiM,21320
+direpack/ppdire/__init__.py,sha256=Ppd3-nRcjg98VSQP0cbM_Svs4zIdgnT9nZcHkhs794U,256
+direpack/ppdire/_ppdire_utils.py,sha256=-bjxP7PA5FDbGToL5R28VZjPGfqAWNQaN5z7Mno-i4Q,6358
+direpack/ppdire/capi.py,sha256=lFent8qo7QKYs6ZOOZ5ND5fahzYKxodQob4HMUaLapc,7123
+direpack/ppdire/ppdire.py,sha256=c46mBtsg5-V161g3xh5WIVr6SmxhwkzCRy6BH9GIyig,35720
+direpack/preprocessing/__init__.py,sha256=7CtAoQ5_hbtJVEVXtQTtTRm_DDw-biTIUr8EFierOzY,263
+direpack/preprocessing/_gsspp_utils.py,sha256=kJHKcK__xuGltZsYp3ujfOLnetOzdFlsNepX3AtB4kA,4020
+direpack/preprocessing/_preproc_utilities.py,sha256=-Ql-zDZwaHPgd8sFAFRU6TLAalieyUWrbGHZP1OB3UM,9012
+direpack/preprocessing/gsspp.py,sha256=KHkOMh2CSIoY85j7o7mOiR-I_8NeMZJMD20ck8aUoU4,5094
+direpack/preprocessing/robcent.py,sha256=TeOBFIfCZmpEgN7l-Z7mYMNUPV26KUsvflCuN54tSNE,9512
+direpack/sprm/__init__.py,sha256=TZQ1Z6kgcI_74U0iMWoDgvZvgCJbDnBltP5HjWweDQ8,254
+direpack/sprm/_m_support_functions.py,sha256=NDr_I39MmykRlRMxIQj0q-9ouh7k9OBiTvm0-z3mcAg,1185
+direpack/sprm/rm.py,sha256=BXcfFpkqxRx1J46AXBmSJPt7JI9eevFXjITz33a79bY,10451
+direpack/sprm/snipls.py,sha256=CBFHDvGe7aldJ0K0b_MWFGit95hjPrRCZ2eheO0itWs,11637
+direpack/sprm/sprm.py,sha256=7fBg0anDpLXC6n-bm5qAFSfBeF3b20SvIPEoY-8I5do,21875
 direpack/sudire/__init__.py,sha256=rPR3r963mvCq5O8fLjl_TEvhwRxDKpmjAEQsxLntoEc,268
-direpack/sudire/_sudire_utils.py,sha256=wvhzoMgEm_OrukF9kwYLDwApf4I_3soGSy9azt9SK_s,15804
+direpack/sudire/_sudire_utils.py,sha256=1zLp1wVnqXgHtskERspmuMQAsAsdBM7_kz6E0YrVI-g,16310
 direpack/sudire/sudire.py,sha256=mkbP_CRYOeUYKF_acGx3UxKixLJgG4ZfHr8Cj0WyOGY,30240
-direpack/test/__init__.py,sha256=54hyYlmXqZ3P8WORozbNiAP-CspYLPlbtVOndq18lUk,123
-direpack/test/test_dicomo.py,sha256=-O3MYLmwpVGLMI7rShg0baucJk1M5NANUFOhdQl1MGU,3752
+direpack/test/__init__.py,sha256=Ex_vAkTteSo7z5ldR0C6w33WQA9szNQqw91fAYwgrF8,128
+direpack/test/test_dicomo.py,sha256=rTCmd7APvfTFPAf6C_dmR7iczqrYcMEutRFpC2NKhEI,3739
 direpack/test/test_ppdire.py,sha256=XA6OsULmBqGFvtfbnGeH_6mkbT-g3wYaGSqlT3TNnPg,3148
 direpack/test/test_sprm.py,sha256=36kTP5qa1Iyu-Yun5bhJxwCmSZPJS5u93gPdi6YbgLA,3091
 direpack/test/test_sudire.py,sha256=5xpjWAlFpLKokiMedSiztuQZx-Hl2-3on_0N3ClgDzA,5122
 direpack/utils/__init__.py,sha256=yDasx_hKfy3ziJPetYR7cBRymsisr7sm2HlTkh_lv1g,259
-direpack/utils/utils.py,sha256=kJJIlTOEbE9HdyQVe0uJVL8PleDAHknmFHcwizbI7zI,2320
-direpack-1.1.2.dist-info/LICENSE,sha256=eCNK5cLDeU5NZZNvQGT8aYdq5hz0d2Kyw3gqi0E0kDA,1070
-direpack-1.1.2.dist-info/METADATA,sha256=SymlQPGiVxn92PQxqNowPoXvHc2EcrpJHOrO8Udu3DM,10221
-direpack-1.1.2.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-direpack-1.1.2.dist-info/top_level.txt,sha256=bkJ0tMR7_5tvCUjLWbVXWIChYbW_O8SAAfVZSthcwCw,9
-direpack-1.1.2.dist-info/RECORD,,
+direpack/utils/utils.py,sha256=MBxqubkjlOnt0_d8CEKVCq18qnKMx76zmj8MC2HU9Dk,2428
+direpack-1.1.3.dist-info/LICENSE,sha256=O_brXXvCt-tcWL-jfM0kJI6YpvoJ611_BhDpqDLR0BM,1091
+direpack-1.1.3.dist-info/METADATA,sha256=xfvQBnSVamDjeGb9KrNenrElgdhJamRQkiDKCvf7yNY,10327
+direpack-1.1.3.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+direpack-1.1.3.dist-info/top_level.txt,sha256=bkJ0tMR7_5tvCUjLWbVXWIChYbW_O8SAAfVZSthcwCw,9
+direpack-1.1.3.dist-info/RECORD,,
```

