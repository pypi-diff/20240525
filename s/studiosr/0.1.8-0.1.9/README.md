# Comparing `tmp/studiosr-0.1.8-py3-none-any.whl.zip` & `tmp/studiosr-0.1.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,34 @@
-Zip file size: 42156 bytes, number of entries: 32
+Zip file size: 42114 bytes, number of entries: 32
 -rwxrwxrwx  2.0 unx       59 b- defN 24-Feb-09 06:58 studiosr/__init__.py
 -rwxrwxrwx  2.0 unx     1527 b- defN 24-Feb-10 05:32 studiosr/__main__.py
 -rwxrwxrwx  2.0 unx      136 b- defN 24-Feb-17 03:37 studiosr/data/__init__.py
 -rwxrwxrwx  2.0 unx     9648 b- defN 24-Feb-17 03:35 studiosr/data/dataset.py
 -rwxrwxrwx  2.0 unx     2945 b- defN 24-Mar-15 14:34 studiosr/data/handler.py
 -rwxrwxrwx  2.0 unx     2953 b- defN 24-Feb-10 08:22 studiosr/data/transforms.py
 -rwxrwxrwx  2.0 unx       92 b- defN 24-Feb-09 06:58 studiosr/engine/__init__.py
--rwxrwxrwx  2.0 unx     4609 b- defN 24-Mar-01 13:37 studiosr/engine/evaluator.py
--rwxrwxrwx  2.0 unx     6593 b- defN 24-Mar-23 04:09 studiosr/engine/trainer.py
+-rw-rw-r--  2.0 unx     4782 b- defN 24-Apr-10 13:39 studiosr/engine/evaluator.py
+-rw-rw-r--  2.0 unx     6594 b- defN 24-Apr-10 13:30 studiosr/engine/trainer.py
 -rwxrwxrwx  2.0 unx      438 b- defN 24-Mar-02 11:00 studiosr/models/__init__.py
--rw-rw-r--  2.0 unx     9195 b- defN 24-Mar-24 01:43 studiosr/models/common.py
--rwxrwxrwx  2.0 unx     2713 b- defN 24-Mar-13 13:11 studiosr/models/edsr.py
--rwxrwxrwx  2.0 unx     1733 b- defN 24-Mar-03 09:46 studiosr/models/espcn.py
--rwxrwxrwx  2.0 unx     4510 b- defN 24-Mar-13 13:11 studiosr/models/han.py
--rwxrwxrwx  2.0 unx    20703 b- defN 24-Mar-13 13:11 studiosr/models/hat.py
--rwxrwxrwx  2.0 unx    17668 b- defN 24-Mar-13 13:13 studiosr/models/maxsr.py
--rwxrwxrwx  2.0 unx     3187 b- defN 24-Mar-13 13:14 studiosr/models/rcan.py
--rwxrwxrwx  2.0 unx     2436 b- defN 24-Mar-04 13:04 studiosr/models/srcnn.py
--rwxrwxrwx  2.0 unx     3672 b- defN 24-Mar-02 10:58 studiosr/models/srresnet.py
+-rw-rw-r--  2.0 unx     9203 b- defN 24-Apr-10 04:54 studiosr/models/common.py
+-rw-rw-r--  2.0 unx     3113 b- defN 24-Apr-10 04:54 studiosr/models/edsr.py
+-rw-rw-r--  2.0 unx     1723 b- defN 24-Apr-10 04:48 studiosr/models/espcn.py
+-rw-rw-r--  2.0 unx     4500 b- defN 24-Apr-10 04:54 studiosr/models/han.py
+-rw-rw-r--  2.0 unx    20686 b- defN 24-Apr-10 04:54 studiosr/models/hat.py
+-rw-rw-r--  2.0 unx    17658 b- defN 24-Apr-10 04:50 studiosr/models/maxsr.py
+-rw-rw-r--  2.0 unx     3177 b- defN 24-Apr-10 04:54 studiosr/models/rcan.py
+-rw-rw-r--  2.0 unx     2426 b- defN 24-Apr-10 04:50 studiosr/models/srcnn.py
+-rw-rw-r--  2.0 unx     3662 b- defN 24-Apr-10 04:50 studiosr/models/srresnet.py
 -rwxrwxrwx  2.0 unx     4070 b- defN 24-Feb-09 06:58 studiosr/models/swinfir.py
--rwxrwxrwx  2.0 unx    14162 b- defN 24-Mar-13 13:11 studiosr/models/swinir.py
--rwxrwxrwx  2.0 unx     2465 b- defN 24-Mar-13 13:14 studiosr/models/vdsr.py
+-rw-rw-r--  2.0 unx    14152 b- defN 24-Apr-10 04:54 studiosr/models/swinir.py
+-rw-rw-r--  2.0 unx     2455 b- defN 24-Apr-10 04:51 studiosr/models/vdsr.py
 -rwxrwxrwx  2.0 unx      353 b- defN 24-Feb-29 14:43 studiosr/utils/__init__.py
 -rwxrwxrwx  2.0 unx     2663 b- defN 24-Feb-09 06:58 studiosr/utils/compare.py
 -rwxrwxrwx  2.0 unx     3292 b- defN 24-Feb-29 14:52 studiosr/utils/helpers.py
 -rwxrwxrwx  2.0 unx      413 b- defN 24-Feb-28 16:08 studiosr/utils/losses.py
 -rwxrwxrwx  2.0 unx     2088 b- defN 24-Feb-09 06:58 studiosr/utils/metrics.py
--rwxrwxrwx  2.0 unx     1069 b- defN 24-Mar-24 01:44 studiosr-0.1.8.dist-info/LICENSE
--rw-rw-r--  2.0 unx     5746 b- defN 24-Mar-24 01:44 studiosr-0.1.8.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 24-Mar-24 01:44 studiosr-0.1.8.dist-info/WHEEL
--rwxrwxrwx  2.0 unx        9 b- defN 24-Mar-24 01:44 studiosr-0.1.8.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2585 b- defN 24-Mar-24 01:44 studiosr-0.1.8.dist-info/RECORD
-32 files, 133824 bytes uncompressed, 38052 bytes compressed:  71.6%
+-rwxrwxrwx  2.0 unx     1069 b- defN 24-Apr-10 14:56 studiosr-0.1.9.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     4258 b- defN 24-Apr-10 14:56 studiosr-0.1.9.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 24-Apr-10 14:56 studiosr-0.1.9.dist-info/WHEEL
+-rwxrwxrwx  2.0 unx        9 b- defN 24-Apr-10 14:56 studiosr-0.1.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2585 b- defN 24-Apr-10 14:56 studiosr-0.1.9.dist-info/RECORD
+32 files, 132821 bytes uncompressed, 38010 bytes compressed:  71.4%
```

## zipnote {}

```diff
@@ -75,23 +75,23 @@
 
 Filename: studiosr/utils/losses.py
 Comment: 
 
 Filename: studiosr/utils/metrics.py
 Comment: 
 
-Filename: studiosr-0.1.8.dist-info/LICENSE
+Filename: studiosr-0.1.9.dist-info/LICENSE
 Comment: 
 
-Filename: studiosr-0.1.8.dist-info/METADATA
+Filename: studiosr-0.1.9.dist-info/METADATA
 Comment: 
 
-Filename: studiosr-0.1.8.dist-info/WHEEL
+Filename: studiosr-0.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: studiosr-0.1.8.dist-info/top_level.txt
+Filename: studiosr-0.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: studiosr-0.1.8.dist-info/RECORD
+Filename: studiosr-0.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## studiosr/engine/evaluator.py

```diff
@@ -9,27 +9,27 @@
 
 
 class Evaluator:
     """
     A class for evaluating the performance of super-resolution models on image datasets.
 
     Args:
-        dataset (str, optional): The name of the evaluation dataset (default is "Set5").
+        dataset (str, optional): The name of the evaluation dataset (default is "DIV2K_mini").
         scale (int, optional): The scaling factor for super-resolution (default is 4).
         root (str, optional): The root directory where evaluation dataset is located (default is "data").
 
     Note:
         This class is designed for evaluating the performance of super-resolution models. It loads the
         evaluation dataset, calculates PSNR and SSIM values for the model's output, and optionally visualizes
         the results. The class can be used for various evaluation datasets and scaling factors.
     """
 
     def __init__(
         self,
-        dataset: str = "Set5",
+        dataset: str = "DIV2K_mini",
         scale: int = 4,
         root: str = "dataset",
     ) -> None:
         self.dataset = dataset
         self.scale = scale
         self.root = root
         root = self.download_dataset(self.root, self.dataset)
@@ -82,35 +82,38 @@
     def download_dataset(root: str = "dataset", dataset: str = "Set5") -> str:
         dataset_id = {
             "Set5": "18bimJIcXV0nxYU9y64Liwo63afEZXlAY",
             "Set14": "1Wn8mJRFT7N4z0cGbqwGev4ltbLwi4Sg2",
             "BSD100": "1qoiBkwiUgv62MISQh4A4nibdmDfP5qzJ",
             "Urban100": "1YTYp0gVJj2gpIsL3N8NkEDKEPIZeyhnf",
             "Manga109": "1ZaUD3ZeaaI3zHlEI6HRSx0baBU2CeYe7",
+            "DIV2K": "1kUlppta5vEmXa76EHU_mb6_EoibNWlXw",
+            "DIV2K_mini": "1pDEDDuYzaRzmJb6ztZTafeui1xE6iCz9",
         }
         benchmark_path = os.path.join(root, dataset)
         if not os.path.exists(benchmark_path):
             os.makedirs(root, exist_ok=True)
             id = dataset_id[dataset]
             gdown_and_extract(id=id, save_dir=root)
         return benchmark_path
 
     @staticmethod
     def benchmark(
         func: Callable[[np.ndarray], np.ndarray],
         scale: int = 4,
         y_only: bool = True,
+        datasets: List[str] = ["Set5", "Set14", "BSD100", "Urban100", "Manga109"],
     ) -> Tuple[List[float], List[float]]:
         log_data = "| Metric |"
         log_line = "| ------ |"
         log_psnr = "|   PSNR |"
         log_ssim = "|   SSIM |"
 
         psnr_list, ssim_list = [], []
-        for dataset in ["Set5", "Set14", "BSD100", "Urban100", "Manga109"]:
+        for dataset in datasets:
             psnr, ssim = Evaluator(dataset, scale).run(func, y_only, logging=True)
             log_data += " %8s |" % dataset
             log_line += " -------- |"
             log_psnr += " %8.3f |" % psnr
             log_ssim += " %8.4f |" % ssim
             psnr_list.append(psnr)
             ssim_list.append(ssim)
```

## studiosr/engine/trainer.py

```diff
@@ -38,15 +38,15 @@
         beta1: float = 0.9,
         beta2: float = 0.99,
         weight_decay: float = 0.0,
         max_iters: int = 500000,
         gamma: float = 0.5,
         milestones: List[int] = [250000, 400000, 450000, 475000],
         loss_function: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = nn.L1Loss(),
-        eval_interval: int = 100,
+        eval_interval: int = 1000,
         ckpt_path: str = "checkpoints",
         bfloat16: bool = True,
         seed: int = 0,
     ) -> None:
         self.model = model
         self.dataset = train_dataset
         self.evaluator = evaluator
```

## studiosr/models/common.py

```diff
@@ -3,76 +3,74 @@
 
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 
-def diverge_images(image: np.ndarray) -> List[np.ndarray]:
+def diverge_images(image: torch.Tensor) -> List[torch.Tensor]:
     transformed_images = []
     for i in range(4):
-        rotated = np.rot90(image, k=i, axes=[0, 1])
-        flipped = np.fliplr(rotated)
+        rotated = torch.rot90(image, k=i, dims=[0, 1])
+        flipped = torch.fliplr(rotated)
         transformed_images.extend([rotated, flipped])
     return transformed_images
 
 
-def converge_images(images: List[np.ndarray]) -> np.ndarray:
+def converge_images(images: List[torch.Tensor]) -> torch.Tensor:
     transformed_images = []
     for i, image in enumerate(images):
-        image = np.fliplr(image) if i & 1 else image
-        image = np.rot90(image, k=i // 2, axes=[1, 0])
+        image = torch.fliplr(image) if i & 1 else image
+        image = torch.rot90(image, k=i // 2, dims=[1, 0])
         transformed_images.append(image)
-    image = np.mean(transformed_images, axis=0)
-    image = image.round().clip(0, 255).astype(np.uint8)
+    image = torch.mean(torch.stack(transformed_images), dim=0)
     return image
 
 
-class BaseModule(nn.Module):
+class Model(nn.Module):
     def __init__(self) -> None:
         super().__init__()
         self.img_range: float = 1.0
         self.scale: int = 4
 
     @torch.inference_mode()
     def inference(self, image: np.ndarray) -> np.ndarray:
         self.eval()
         scale = 255.0 if self.img_range == 1.0 else 1.0
         device = next(self.parameters()).get_device()
         device = torch.device("cpu") if device < 0 else device
-        x = image.transpose(2, 0, 1).astype(np.float32) / scale
-        x = torch.from_numpy(x).unsqueeze(0)
-        x = x.to(device)
-        output = self.forward(x) * scale
-        output = output.squeeze().cpu().numpy().transpose(1, 2, 0)
-        return output.round().clip(0, 255).astype(np.uint8)
+        image = torch.from_numpy(image).to(device)
+        x = image.permute(2, 0, 1).unsqueeze(0).to(torch.float32) / scale
+        output = self.forward(x)[0].permute(1, 2, 0) * scale
+        output = output.round().clip(0, 255).to(torch.uint8)
+        return output.cpu().numpy()
 
     @torch.inference_mode()
     def inference_with_self_ensemble(self, image: np.ndarray) -> np.ndarray:
         self.eval()
         scale = 255.0 if self.img_range == 1.0 else 1.0
         device = next(self.parameters()).get_device()
         device = torch.device("cpu") if device < 0 else device
+        image = torch.from_numpy(image).to(device)
         images = diverge_images(image)
         outputs = []
         for image in images:
-            x = image.transpose(2, 0, 1).astype(np.float32) / scale
-            x = torch.from_numpy(x).unsqueeze(0)
-            x = x.to(device)
-            output = self.forward(x) * scale
-            outputs.append(output.squeeze().cpu().numpy().transpose(1, 2, 0))
-        image = converge_images(outputs)
-        return image
+            x = image.permute(2, 0, 1).unsqueeze(0).to(torch.float32) / scale
+            output = self.forward(x)[0].permute(1, 2, 0)
+            outputs.append(output)
+        output = converge_images(outputs) * scale
+        output = output.round().clip(0, 255).to(torch.uint8)
+        return output.cpu().numpy()
 
     def get_training_config(self) -> Dict:
         training_config = dict()
         return training_config
 
     @classmethod
-    def from_pretrained(cls, scale: int = 4) -> "BaseModule":
+    def from_pretrained(cls, scale: int = 4) -> "Model":
         model = cls(scale=scale)
         return model
 
     def export(
         self,
         path: Optional[str] = None,
         input_shape: List[int] = [1, 3, 256, 256],
@@ -83,14 +81,17 @@
         if path is None:
             path = f"{self.__class__.__name__}x{self.scale}.{format}"
         x = torch.randn(input_shape)
         torch.onnx.export(self, x, path, input_names=["input"], output_names=["output"])
         return path
 
 
+BaseModule = Model
+
+
 def conv2d(in_channels: int, out_channels: int, kernel_size: int) -> nn.Module:
     return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size // 2))
 
 
 class MeanShift(nn.Conv2d):
     def __init__(
         self,
```

## studiosr/models/edsr.py

```diff
@@ -1,18 +1,19 @@
 import os
 from typing import Dict
 
+import gdown
 import torch
 import torch.nn as nn
 
-from studiosr.models.common import BaseModule, MeanShift, ResBlock, Upsampler, conv2d
+from studiosr.models.common import MeanShift, Model, ResBlock, Upsampler, conv2d
 from studiosr.utils import download
 
 
-class EDSR(BaseModule):
+class EDSR(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         img_range: float = 1.0,
         n_resblocks: int = 32,
         n_feats: int = 256,
@@ -50,27 +51,42 @@
             max_iters=1000000,
             gamma=0.5,
             milestones=[200000, 400000, 600000, 800000],
         )
         return training_config
 
     @classmethod
-    def from_pretrained(cls, scale: int = 4) -> "EDSR":
-        url = {
-            "r16f64x2.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_baseline_x2-1bc95232.pt",
-            "r16f64x3.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_baseline_x3-abf2a44e.pt",
-            "r16f64x4.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_baseline_x4-6b446fab.pt",
-            "r32f256x2.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_x2-0edfb8a3.pt",
-            "r32f256x3.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_x3-ea3ef2c6.pt",
-            "r32f256x4.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_x4-4f62e9ef.pt",
-        }
-        model = EDSR(scale=scale, img_range=255.0)
-        file_name = f"r32f256x{scale}.pth"
+    def from_pretrained(cls, scale: int = 4, dataset: str = "DIV2K") -> "EDSR":
+        assert scale in [2, 3, 4]
+        assert dataset in ["DIV2K", "DF2K"]
+
         model_dir = "pretrained"
         os.makedirs(model_dir, exist_ok=True)
-        link = url[file_name]
-        path = os.path.join(model_dir, file_name)
-        if not os.path.exists(path):
-            download(link, path)
+        if dataset == "DIV2K":
+            url = {
+                "r32f256x2.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_x2-0edfb8a3.pt",
+                "r32f256x3.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_x3-ea3ef2c6.pt",
+                "r32f256x4.pth": "https://cv.snu.ac.kr/research/EDSR/models/edsr_x4-4f62e9ef.pt",
+            }
+            model = EDSR(scale=scale, img_range=255.0)
+            file_name = f"r32f256x{scale}.pth"
+
+            link = url[file_name]
+            path = os.path.join(model_dir, file_name)
+            if not os.path.exists(path):
+                download(link, path)
+        elif dataset == "DF2K":
+            file_ids = {
+                2: "1XEqY_nkUMdIid4lM9zAW99rYDx5eftBT",
+                3: "1H1yFCFK14Z0DWAZHCtGXcWS6377fbkJE",
+                4: "1TeH67rKNSR3dXs56aLqsA-UvLL3TZL-g",
+            }
+            model = EDSR(scale=scale)
+            file_name = f"EDSRx{scale}.pth"
+            file_id = file_ids[scale]
+            path = os.path.join(model_dir, file_name)
+            if not os.path.exists(path):
+                gdown.download(id=file_id, output=path, quiet=False)
+
         pretrained = torch.load(path, map_location="cpu")
         model.load_state_dict(pretrained, strict=False)
         return model
```

## studiosr/models/espcn.py

```diff
@@ -1,16 +1,16 @@
 import math
 
 import torch
 from torch import nn
 
-from studiosr.models.common import BaseModule, Normalizer
+from studiosr.models.common import Model, Normalizer
 
 
-class ESPCN(BaseModule):
+class ESPCN(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         channels: int = 64,
         img_range: float = 1.0,
     ) -> None:
```

## studiosr/models/han.py

```diff
@@ -1,15 +1,15 @@
 import os
 from typing import Dict
 
 import gdown
 import torch
 import torch.nn as nn
 
-from studiosr.models.common import BaseModule, MeanShift, Upsampler, conv2d
+from studiosr.models.common import MeanShift, Model, Upsampler, conv2d
 from studiosr.models.rcan import ResidualGroup
 
 
 class LAM_Module(nn.Module):
     def __init__(self, in_dim: int) -> None:
         super().__init__()
         self.chanel_in = in_dim
@@ -48,15 +48,15 @@
 
         out = self.gamma * out
         out = out.view(m_batchsize, -1, height, width)
         x = x * out + x
         return x
 
 
-class HAN(BaseModule):
+class HAN(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         img_range: float = 1.0,
         n_resgroups: int = 10,
         n_resblocks: int = 20,
```

## studiosr/models/hat.py

```diff
@@ -2,19 +2,19 @@
 from itertools import repeat
 from typing import Dict, List, Optional
 
 import gdown
 import torch
 import torch.nn as nn
 from einops import rearrange
-from timm.models.layers import DropPath, trunc_normal_
+from timm.layers import DropPath, trunc_normal_
 
 from studiosr.models.common import (
-    BaseModule,
     Mlp,
+    Model,
     Normalizer,
     PatchEmbed,
     PatchUnEmbed,
     Upsampler,
     calculate_mask,
     check_image_size,
     window_partition,
@@ -381,15 +381,15 @@
         self.patch_embed = PatchEmbed(embed_dim=dim)
         self.patch_unembed = PatchUnEmbed(embed_dim=dim)
 
     def forward(self, x: torch.Tensor, x_size: List[int], params: Dict) -> torch.Tensor:
         return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size, params), x_size))) + x
 
 
-class HAT(BaseModule):
+class HAT(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         img_range: float = 1.0,
         embed_dim: int = 180,
         depths: List[int] = [6, 6, 6, 6, 6, 6],
```

## studiosr/models/maxsr.py

```diff
@@ -3,15 +3,15 @@
 
 import torch
 from einops import rearrange
 from einops.layers.torch import Rearrange, Reduce
 from torch import einsum, nn
 from torch.nn.functional import pad
 
-from studiosr.models.common import BaseModule, Normalizer
+from studiosr.models.common import Model, Normalizer
 
 # helpers
 
 
 def exists(val):
     return val is not None
 
@@ -371,15 +371,15 @@
             m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))
             m.append(nn.PixelShuffle(3))
         else:
             raise ValueError(f"scale {scale} is not supported. " "Supported scales: 2^n and 3.")
         super(Upsample, self).__init__(*m)
 
 
-class MaxSR(BaseModule):
+class MaxSR(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         img_range: float = 1.0,
         adaptive: bool = True,
         dim: int = 128,
```

## studiosr/models/rcan.py

```diff
@@ -1,14 +1,14 @@
 import os
 from typing import Dict
 
 import torch
 import torch.nn as nn
 
-from studiosr.models.common import BaseModule, ChannelAttention, MeanShift, Upsampler, conv2d
+from studiosr.models.common import ChannelAttention, MeanShift, Model, Upsampler, conv2d
 from studiosr.utils import gdown_and_extract
 
 
 class RCAB(nn.Module):
     def __init__(self, n_feat: int, kernel_size: int, reduction: int) -> None:
         super().__init__()
         self.body = nn.Sequential(
@@ -32,15 +32,15 @@
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         res = self.body(x)
         res += x
         return res
 
 
-class RCAN(BaseModule):
+class RCAN(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         img_range: float = 1.0,
         n_resgroups: int = 10,
         n_resblocks: int = 20,
```

## studiosr/models/srcnn.py

```diff
@@ -1,14 +1,14 @@
 import torch
 from torch import nn
 
-from studiosr.models.common import BaseModule, Normalizer
+from studiosr.models.common import Model, Normalizer
 
 
-class SRCNN(BaseModule):
+class SRCNN(Model):
     """Super-Resolution Convolutional Neural Network (SRCNN).
 
     Args:
         - scale (int): Upsampling scale factor. Default is 4.
         - n_colors (int): Number of input image channels. Default is 3.
         - img_range (float): Range of pixel values in the input image. Default is 1.0.
         - residual (bool): Flag indicating whether to use residual learning. Default is False.
```

## studiosr/models/srresnet.py

```diff
@@ -1,15 +1,15 @@
 import math
 from typing import Dict
 
 import torch
 import torch.nn as nn
 from torch import Tensor
 
-from studiosr.models.common import BaseModule, Normalizer
+from studiosr.models.common import Model, Normalizer
 
 
 class _ResidualConvBlock(nn.Module):
     def __init__(self, channels: int) -> None:
         super().__init__()
         self.rcb = nn.Sequential(
             nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1), bias=False),
@@ -36,15 +36,15 @@
         )
 
     def forward(self, x: Tensor) -> Tensor:
         x = self.upsample_block(x)
         return x
 
 
-class SRResNet(BaseModule):
+class SRResNet(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         img_range: float = 1.0,
         channels: int = 64,
         num_rcb: int = 16,
```

## studiosr/models/swinir.py

```diff
@@ -3,16 +3,16 @@
 from typing import Dict, List, Optional
 
 import torch
 import torch.nn as nn
 from timm.layers import DropPath, trunc_normal_
 
 from studiosr.models.common import (
-    BaseModule,
     Mlp,
+    Model,
     Normalizer,
     PatchEmbed,
     PatchUnEmbed,
     Upsampler,
     calculate_mask,
     check_image_size,
     window_partition,
@@ -234,15 +234,15 @@
         self.patch_embed = PatchEmbed(embed_dim=dim)
         self.patch_unembed = PatchUnEmbed(embed_dim=dim)
 
     def forward(self, x: torch.Tensor, x_size: List[int]) -> torch.Tensor:
         return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x
 
 
-class SwinIR(BaseModule):
+class SwinIR(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         img_range: float = 1.0,
         embed_dim: int = 180,
         depths: List[int] = [6, 6, 6, 6, 6, 6],
```

## studiosr/models/vdsr.py

```diff
@@ -2,18 +2,18 @@
 import os
 from typing import Dict
 
 import gdown
 import torch
 import torch.nn as nn
 
-from studiosr.models.common import BaseModule, Normalizer, conv2d
+from studiosr.models.common import Model, Normalizer, conv2d
 
 
-class VDSR(BaseModule):
+class VDSR(Model):
     def __init__(
         self,
         scale: int = 4,
         n_colors: int = 3,
         channels: int = 64,
         img_range: float = 1.0,
         n_layers: int = 18,
```

## Comparing `studiosr-0.1.8.dist-info/LICENSE` & `studiosr-0.1.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `studiosr-0.1.8.dist-info/RECORD` & `studiosr-0.1.9.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 studiosr/__init__.py,sha256=e04mneZqsm3l30aSxuGEt06CTdKgwGqrwVwascYK_XY,59
 studiosr/__main__.py,sha256=EDEGat6eMz00Kkyx5nc5VsZnj1RQAzk9iyjLPiKKooo,1527
 studiosr/data/__init__.py,sha256=-vqsrbV5FnXeCLq8eWVsNNZwCWD8z_ce7fGEnJYvMzk,136
 studiosr/data/dataset.py,sha256=oWpyP3AdmEFTzmFcN3HJN7YLLtGKXg-KiFM3wtS7E1g,9648
 studiosr/data/handler.py,sha256=ZBxD4sYjYFbG1vNTxvWWeD4Z3W7ZLAHoqHKfRI-chOc,2945
 studiosr/data/transforms.py,sha256=9HJl6asV3rKz7nzeW6-MLlVRJx-eeLC4mrI0-Rxs-_4,2953
 studiosr/engine/__init__.py,sha256=rsvpQdHdm8MqYyvl3EwrbUuR7IuA20R7GTyf4YNTJwQ,92
-studiosr/engine/evaluator.py,sha256=UmcYRz1WuPcIFtMB9SvaV_LzaSrPGLbcwl1-ws6JI8Y,4609
-studiosr/engine/trainer.py,sha256=U24baudzVZBy1Q5ZMjcCxIKM3Ki-GM10oPfyvMYkk6g,6593
+studiosr/engine/evaluator.py,sha256=aCT5ucmz-Buo9MQYRPc-3gDmUT3WMwuMhuocXrbipf4,4782
+studiosr/engine/trainer.py,sha256=hW0GqCQOWTTLELUFB-Xw0gafG7OO5JZUH06NCN8w2EQ,6594
 studiosr/models/__init__.py,sha256=u-raG6AYras29ozNLM9-53XKyZIZNCyX3YZc_dfU8mk,438
-studiosr/models/common.py,sha256=Aur09D1Hr-BihDjbv90iFGuGj-FN4bzGdKlKLpl-VhI,9195
-studiosr/models/edsr.py,sha256=3pUzOdP_QncifdD6N7zQ_OttWfzWGRHSNnE0DnIaXFE,2713
-studiosr/models/espcn.py,sha256=I54dbbNTQwjI1LRYOXQyDNw-_7ICpzrpN2Cd0nLzl9I,1733
-studiosr/models/han.py,sha256=-3SBPa3LzS8yh-aqh9K_wynpaDpkXxLBF5vN7MysbPc,4510
-studiosr/models/hat.py,sha256=xxi1k486VwzkUz3vYFtE51nTk43Bc8FPepYDCj8DLIM,20703
-studiosr/models/maxsr.py,sha256=YTlxS1yg-Vla3EkiRt7l8bNYLJo8S_U7L7CN-RlPR8M,17668
-studiosr/models/rcan.py,sha256=4OSjllGSTiDqS_iGguir_YNq1RxuSpVz_Ifq_H5_YO8,3187
-studiosr/models/srcnn.py,sha256=X3yrXD92bpzkfezF326IZxqIq8FkzAEkOLNUsxxLBA8,2436
-studiosr/models/srresnet.py,sha256=QqXxvm-6cnv2p7u53215_ylipomgX2fKXRUVRWVpv2k,3672
+studiosr/models/common.py,sha256=UiOpO_2C-fynNxrg6DUFeffkJzArg0yIXwLnvwyOWFM,9203
+studiosr/models/edsr.py,sha256=W9jIZjoJQyxqAKBXWn732J089zUUSqcfVxxjL2I6vF8,3113
+studiosr/models/espcn.py,sha256=ECXbxTtUHHq4EshDpjD2-4eDtnXz2tcqmGxWqfrdsaU,1723
+studiosr/models/han.py,sha256=Yc_bOMh5o4vLvL0hAvwloF1Bwdz2AtpNoQw2vD-23DI,4500
+studiosr/models/hat.py,sha256=wbPBy-20O3getTmsgEnN16ErgoGNYL-vLrQxEtayWLI,20686
+studiosr/models/maxsr.py,sha256=qICbnF1dXSrJRgEbEOJ56BofBnd9yHhi5P8mn0W6t3U,17658
+studiosr/models/rcan.py,sha256=W2dAer_g6OgcJoqUeOCDeqhXCj0lVMvcaO7WuA_pHp0,3177
+studiosr/models/srcnn.py,sha256=aApUMJlVLgRH7SyEZ4giuC6kCsdXpj4IG9FHkQK1NBE,2426
+studiosr/models/srresnet.py,sha256=F2gRHSnzF7AyE1njZCvbEn_40KEhrlTReuE-GbYEx9g,3662
 studiosr/models/swinfir.py,sha256=cgHFjk4bo8PEy0gSDvSx96SG2znOBtcmuY1lhfS8SbA,4070
-studiosr/models/swinir.py,sha256=bXOZaAxZg99IFdtcIwtTcW_9iPgZ85QECi3WaGz5P3Q,14162
-studiosr/models/vdsr.py,sha256=xvSKp6621T92EjtuLHsWEPpu_g1jAbnn55hEsVpxGD0,2465
+studiosr/models/swinir.py,sha256=SHlZ67TdGBYyIP1clGs6CbiGobslj6_nnIgSISiPZ9g,14152
+studiosr/models/vdsr.py,sha256=-XzFtclsNuR5ffjrgx76mACpt3eEKXrS6jdWrjW96Kk,2455
 studiosr/utils/__init__.py,sha256=zPS4pUVEKIxt7YbxQyI94K-Ng3Ho5iaw9F2YOUTN39E,353
 studiosr/utils/compare.py,sha256=6NvBgTCFxfIj2vnwL5Cge6u0Bl6g4z8lcqusx5qTxUA,2663
 studiosr/utils/helpers.py,sha256=ZWfRtIqAtzANLs68sp1xTX99-pRLPIVJaLKBXZr9P-8,3292
 studiosr/utils/losses.py,sha256=ziVo_Tt-B7DbxODn4Z3zcPGfpVO7O5EHan2QQwDf-Es,413
 studiosr/utils/metrics.py,sha256=3QMbITRu26_zveeKvwIFRy3ISGvMpflK6Q7YRS_mr0s,2088
-studiosr-0.1.8.dist-info/LICENSE,sha256=REasKXXF4I7O4BUavFfKVGLrAjdWNRrRWB4HH95oIec,1069
-studiosr-0.1.8.dist-info/METADATA,sha256=JSpzm-LyjIh5DzfdTMmeFlrvRVU9rArbqbLA7tIt7sI,5746
-studiosr-0.1.8.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-studiosr-0.1.8.dist-info/top_level.txt,sha256=ra7VlnwpSRfnOilJ7DRO_DQ-h-3L3cOUQ_rSbIfABfc,9
-studiosr-0.1.8.dist-info/RECORD,,
+studiosr-0.1.9.dist-info/LICENSE,sha256=REasKXXF4I7O4BUavFfKVGLrAjdWNRrRWB4HH95oIec,1069
+studiosr-0.1.9.dist-info/METADATA,sha256=kghwR3I71GsdaK7wrQcfW22IL3T-sEzu1IKBq_Xkw5Q,4258
+studiosr-0.1.9.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+studiosr-0.1.9.dist-info/top_level.txt,sha256=ra7VlnwpSRfnOilJ7DRO_DQ-h-3L3cOUQ_rSbIfABfc,9
+studiosr-0.1.9.dist-info/RECORD,,
```

