# Comparing `tmp/colossalai-nightly-2024.5.25.tar.gz` & `tmp/colossalai-nightly-2024.5.4.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "colossalai-nightly-2024.5.25.tar", last modified: Sat May 25 00:14:47 2024, max compression
+gzip compressed data, was "colossalai-nightly-2024.5.4.tar", last modified: Sat May  4 00:14:38 2024, max compression
```

## Comparing `colossalai-nightly-2024.5.25.tar` & `colossalai-nightly-2024.5.4.tar`

### file list

```diff
@@ -1,1281 +1,1175 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.529467 colossalai-nightly-2024.5.25/
--rw-r--r--   0 runner    (1001) docker     (127)    30134 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)      198 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)    36485 2024-05-25 00:14:47.529467 colossalai-nightly-2024.5.25/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    30954 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.365466 colossalai-nightly-2024.5.25/colossalai/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.365466 colossalai-nightly-2024.5.25/colossalai/_C/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_C/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      597 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.365466 colossalai-nightly-2024.5.25/colossalai/_analyzer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.365466 colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/
--rw-r--r--   0 runner    (1001) docker     (127)      165 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20868 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/_meta_registration.py
--rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/_monkey_patch.py
--rw-r--r--   0 runner    (1001) docker     (127)    18677 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/flop_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     7589 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/meta_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/envs.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.365466 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/
--rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18998 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/codegen.py
--rw-r--r--   0 runner    (1001) docker     (127)     9947 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/graph_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     8482 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/node_util.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.365466 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/passes/
--rw-r--r--   0 runner    (1001) docker     (127)      106 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12708 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/passes/graph_profile.py
--rw-r--r--   0 runner    (1001) docker     (127)     9939 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/passes/shape_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)      952 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/symbolic_profile.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.369466 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/
--rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5415 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/bias_addition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/custom_leaf_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/proxy.py
--rw-r--r--   0 runner    (1001) docker     (127)     5862 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/symbolic_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)    15712 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/tracer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.369466 colossalai-nightly-2024.5.25/colossalai/accelerator/
--rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/accelerator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/accelerator/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     9402 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/accelerator/base_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)    10154 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/accelerator/cpu_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/accelerator/cuda_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9453 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/accelerator/npu_accelerator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.369466 colossalai-nightly-2024.5.25/colossalai/amp/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.369466 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.369466 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/grad_scaler/
--rw-r--r--   0 runner    (1001) docker     (127)      222 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/grad_scaler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2119 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4977 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.369466 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_mixin/
--rw-r--r--   0 runner    (1001) docker     (127)      226 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_mixin/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2523 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_mixin/base.py
--rw-r--r--   0 runner    (1001) docker     (127)      504 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_mixin/bf16.py
--rw-r--r--   0 runner    (1001) docker     (127)     2695 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py
--rw-r--r--   0 runner    (1001) docker     (127)     7959 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.369466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.369466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/
--rw-r--r--   0 runner    (1001) docker     (127)      155 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      377 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/build_c_ext.py
--rw-r--r--   0 runner    (1001) docker     (127)     7711 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     3468 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
--rw-r--r--   0 runner    (1001) docker     (127)    18538 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4921 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/operation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.373466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/
--rw-r--r--   0 runner    (1001) docker     (127)       95 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.373466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/
--rw-r--r--   0 runner    (1001) docker     (127)      241 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3940 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2840 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     7571 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
--rw-r--r--   0 runner    (1001) docker     (127)     2512 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    24482 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     9318 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py
--rw-r--r--   0 runner    (1001) docker     (127)     7392 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)     3258 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     2827 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/where.py
--rw-r--r--   0 runner    (1001) docker     (127)      761 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     4748 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/shard_metainfo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.373466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6826 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/amp_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     3584 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/base_offload_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2072 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/mem_optimize.py
--rw-r--r--   0 runner    (1001) docker     (127)     5167 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/region.py
--rw-r--r--   0 runner    (1001) docker     (127)    20031 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/region_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     9909 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/runtime.py
--rw-r--r--   0 runner    (1001) docker     (127)    18503 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/solver.py
--rw-r--r--   0 runner    (1001) docker     (127)    17919 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/training_simulator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2793 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/util.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.377466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5111 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/comm_metainfo_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)      417 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6050 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/meta_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    11328 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/runtime_apply_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    22439 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/runtime_preparation_pass.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.377466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/pipeline_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/pipeline_shard/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.377466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)    16998 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.381466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/
--rw-r--r--   0 runner    (1001) docker     (127)     2062 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3731 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3070 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4988 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4805 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     5563 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    11414 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1316 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1901 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    13535 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    20347 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    16308 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1762 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2997 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1466 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)      744 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1980 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.385466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/
--rw-r--r--   0 runner    (1001) docker     (127)     2100 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14274 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     5162 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    24078 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    12070 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3615 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     7361 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9225 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    41664 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3636 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    18860 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4702 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    13053 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4965 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2278 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3981 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4158 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3053 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2330 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1774 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1964 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3553 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/options.py
--rw-r--r--   0 runner    (1001) docker     (127)    10812 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/sharding_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.385466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/
--rw-r--r--   0 runner    (1001) docker     (127)      238 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9987 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     5767 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)    20206 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/solver.py
--rw-r--r--   0 runner    (1001) docker     (127)     8474 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.385466 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5899 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/broadcast.py
--rw-r--r--   0 runner    (1001) docker     (127)     8346 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     3841 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (127)     9066 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/reshape.py
--rw-r--r--   0 runner    (1001) docker     (127)     4408 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/sharding.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.385466 colossalai-nightly-2024.5.25/colossalai/booster/
--rw-r--r--   0 runner    (1001) docker     (127)       93 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1481 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)    20278 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/booster.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.389466 colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (127)     1298 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      102 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/bf16.py
--rw-r--r--   0 runner    (1001) docker     (127)     3218 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/fp16_apex.py
--rw-r--r--   0 runner    (1001) docker     (127)      870 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/fp16_naive.py
--rw-r--r--   0 runner    (1001) docker     (127)     4824 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/fp16_torch.py
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/fp8.py
--rw-r--r--   0 runner    (1001) docker     (127)      565 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/mixed_precision_base.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.389466 colossalai-nightly-2024.5.25/colossalai/booster/plugin/
--rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/dp_plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    28226 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/gemini_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    65054 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/hybrid_parallel_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    20867 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/low_level_zero_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    20853 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)     2475 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)      559 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/pp_plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     9770 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/torch_ddp_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    15017 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/booster/plugin/torch_fsdp_plugin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.389466 colossalai-nightly-2024.5.25/colossalai/checkpoint_io/
--rw-r--r--   0 runner    (1001) docker     (127)      318 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/checkpoint_io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15804 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/checkpoint_io/checkpoint_io_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     8761 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/checkpoint_io/general_checkpoint_io.py
--rw-r--r--   0 runner    (1001) docker     (127)    43972 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py
--rw-r--r--   0 runner    (1001) docker     (127)     5758 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/checkpoint_io/index_file.py
--rw-r--r--   0 runner    (1001) docker     (127)    29632 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/checkpoint_io/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.389466 colossalai-nightly-2024.5.25/colossalai/cli/
--rw-r--r--   0 runner    (1001) docker     (127)       40 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cli/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.389466 colossalai-nightly-2024.5.25/colossalai/cli/check/
--rw-r--r--   0 runner    (1001) docker     (127)      396 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cli/check/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8454 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cli/check/check_installation.py
--rw-r--r--   0 runner    (1001) docker     (127)      310 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cli/cli.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.393466 colossalai-nightly-2024.5.25/colossalai/cli/launcher/
--rw-r--r--   0 runner    (1001) docker     (127)     3654 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cli/launcher/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3214 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cli/launcher/hostinfo.py
--rw-r--r--   0 runner    (1001) docker     (127)     4286 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cli/launcher/multinode_runner.py
--rw-r--r--   0 runner    (1001) docker     (127)    10530 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cli/launcher/run.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.393466 colossalai-nightly-2024.5.25/colossalai/cluster/
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4241 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cluster/device_mesh_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     7233 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cluster/dist_coordinator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2356 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cluster/process_group_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)    10963 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/cluster/process_group_mesh.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.393466 colossalai-nightly-2024.5.25/colossalai/context/
--rw-r--r--   0 runner    (1001) docker     (127)       96 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3153 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/context/config.py
--rw-r--r--   0 runner    (1001) docker     (127)      921 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/context/singleton_meta.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.393466 colossalai-nightly-2024.5.25/colossalai/device/
--rw-r--r--   0 runner    (1001) docker     (127)      139 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/device/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/device/alpha_beta_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     5634 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/device/calc_pipeline_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)    23559 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/device/device_mesh.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.393466 colossalai-nightly-2024.5.25/colossalai/fx/
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/_compatibility.py
--rw-r--r--   0 runner    (1001) docker     (127)    19328 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/_meta_regist_12.py
--rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/_meta_regist_13.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.393466 colossalai-nightly-2024.5.25/colossalai/fx/codegen/
--rw-r--r--   0 runner    (1001) docker     (127)       45 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/codegen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    45231 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/codegen/activation_checkpoint_codegen.py
--rw-r--r--   0 runner    (1001) docker     (127)     7438 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/graph_module.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.397466 colossalai-nightly-2024.5.25/colossalai/fx/passes/
--rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13496 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/passes/adding_split_node_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    12261 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/passes/concrete_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    14066 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/passes/meta_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    15886 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/passes/passes_for_gpt2_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6888 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/passes/shard_1d_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    13714 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/passes/split_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     6169 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/passes/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.397466 colossalai-nightly-2024.5.25/colossalai/fx/profiler/
--rw-r--r--   0 runner    (1001) docker     (127)      768 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      871 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6285 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/dataflow.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.397466 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)      282 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      782 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     7062 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.397466 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/
--rw-r--r--   0 runner    (1001) docker     (127)      211 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py
--rw-r--r--   0 runner    (1001) docker     (127)      632 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      438 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     2027 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1188 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)      402 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/python_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     2188 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.401466 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/
--rw-r--r--   0 runner    (1001) docker     (127)      252 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     2037 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     6708 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      424 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      495 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1582 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1013 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/rnn.py
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/torch_op.py
--rw-r--r--   0 runner    (1001) docker     (127)      603 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1050 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2232 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/memory_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    13214 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/opcount.py
--rw-r--r--   0 runner    (1001) docker     (127)    15377 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4235 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4990 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/profiler/tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4010 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/proxy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.401466 colossalai-nightly-2024.5.25/colossalai/fx/tracer/
--rw-r--r--   0 runner    (1001) docker     (127)      201 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4850 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/_meta_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)     2197 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/_symbolic_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)     1479 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/_tracer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.401466 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/
--rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.401466 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/
--rw-r--r--   0 runner    (1001) docker     (127)      193 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py
--rw-r--r--   0 runner    (1001) docker     (127)     2171 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py
--rw-r--r--   0 runner    (1001) docker     (127)     4445 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py
--rw-r--r--   0 runner    (1001) docker     (127)      745 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.405466 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/
--rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2370 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py
--rw-r--r--   0 runner    (1001) docker     (127)      503 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    26431 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/experimental.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.405466 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/
--rw-r--r--   0 runner    (1001) docker     (127)       62 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.405466 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/
--rw-r--r--   0 runner    (1001) docker     (127)      167 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     3200 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py
--rw-r--r--   0 runner    (1001) docker     (127)     5707 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      517 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     5622 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.405466 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/
--rw-r--r--   0 runner    (1001) docker     (127)      180 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      428 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     4752 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      254 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      400 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1129 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     6769 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)      644 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/rnn.py
--rw-r--r--   0 runner    (1001) docker     (127)      834 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)    24642 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/fx/tracer/tracer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.409466 colossalai-nightly-2024.5.25/colossalai/inference/
--rw-r--r--   0 runner    (1001) docker     (127)      120 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    24213 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/batch_bucket.py
--rw-r--r--   0 runner    (1001) docker     (127)    16668 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/config.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.409466 colossalai-nightly-2024.5.25/colossalai/inference/core/
--rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12749 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/core/async_engine.py
--rw-r--r--   0 runner    (1001) docker     (127)    33976 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/core/engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     5543 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/core/plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    16508 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/core/request_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    11954 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/core/rpc_engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/flash_decoding_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3802 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/graph_runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.409466 colossalai-nightly-2024.5.25/colossalai/inference/kv_cache/
--rw-r--r--   0 runner    (1001) docker     (127)      164 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/kv_cache/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2081 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/kv_cache/block_cache.py
--rw-r--r--   0 runner    (1001) docker     (127)    27863 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/kv_cache/kvcache_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     6685 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/logit_processors.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.409466 colossalai-nightly-2024.5.25/colossalai/inference/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.409466 colossalai-nightly-2024.5.25/colossalai/inference/modeling/layers/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13669 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/layers/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     1395 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/layers/baichuan_tp_linear.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.409466 colossalai-nightly-2024.5.25/colossalai/inference/modeling/models/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    19467 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/models/glide_llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    17728 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/models/nopadding_baichuan.py
--rw-r--r--   0 runner    (1001) docker     (127)    30542 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/models/nopadding_llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.409466 colossalai-nightly-2024.5.25/colossalai/inference/modeling/policy/
--rw-r--r--   0 runner    (1001) docker     (127)      501 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/policy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1575 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/policy/glide_llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     4583 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/policy/nopadding_baichuan.py
--rw-r--r--   0 runner    (1001) docker     (127)     4556 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/modeling/policy/nopadding_llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/sampler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.413466 colossalai-nightly-2024.5.25/colossalai/inference/server/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/server/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8381 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/server/api_server.py
--rw-r--r--   0 runner    (1001) docker     (127)     5876 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/server/chat_service.py
--rw-r--r--   0 runner    (1001) docker     (127)     1072 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/server/completion_service.py
--rw-r--r--   0 runner    (1001) docker     (127)      713 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/server/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.413466 colossalai-nightly-2024.5.25/colossalai/inference/spec/
--rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/spec/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4578 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/spec/drafter.py
--rw-r--r--   0 runner    (1001) docker     (127)     2150 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/spec/struct.py
--rw-r--r--   0 runner    (1001) docker     (127)     5269 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/struct.py
--rw-r--r--   0 runner    (1001) docker     (127)     4136 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/inference/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6088 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.413466 colossalai-nightly-2024.5.25/colossalai/interface/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/interface/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      851 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/interface/model.py
--rw-r--r--   0 runner    (1001) docker     (127)     5156 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/interface/optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)      333 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/interface/pretrained.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.413466 colossalai-nightly-2024.5.25/colossalai/kernel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.413466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/
--rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/base_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     4879 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/cpp_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.413466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.413466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/common/
--rw-r--r--   0 runner    (1001) docker     (127)      694 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/common/data_type.h
--rw-r--r--   0 runner    (1001) docker     (127)    14986 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/common/micros.h
--rw-r--r--   0 runner    (1001) docker     (127)      901 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/common/mp_type_traits.h
--rw-r--r--   0 runner    (1001) docker     (127)     2886 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/common/target.h
--rw-r--r--   0 runner    (1001) docker     (127)     2797 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/common/vec_type_traits.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.417466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/funcs/
--rw-r--r--   0 runner    (1001) docker     (127)    10622 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/funcs/binary_functor.h
--rw-r--r--   0 runner    (1001) docker     (127)    21630 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/funcs/cast_functor.h
--rw-r--r--   0 runner    (1001) docker     (127)     3932 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/funcs/reduce_function.h
--rw-r--r--   0 runner    (1001) docker     (127)     8393 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/funcs/ternary_functor.h
--rw-r--r--   0 runner    (1001) docker     (127)     2460 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/funcs/unary_functor.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.349466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.417466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/arm/
--rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.417466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)     2614 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/activation_kernel.cu
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.417466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/attention/
--rw-r--r--   0 runner    (1001) docker     (127)     6820 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/attention/attention_utils.h
--rw-r--r--   0 runner    (1001) docker     (127)    10773 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     3931 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    10013 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    41480 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    21861 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     9781 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    25833 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/layer_norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    25880 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/moe_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5207 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh
--rw-r--r--   0 runner    (1001) docker     (127)    15325 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    13119 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     4444 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     6486 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    14092 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    22113 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    23452 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     2538 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/utils/gpu_launch_config.h
--rw-r--r--   0 runner    (1001) docker     (127)      545 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/utils/micros.h
--rw-r--r--   0 runner    (1001) docker     (127)     1287 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/utils/nvgpu_dev_info.h
--rw-r--r--   0 runner    (1001) docker     (127)     2063 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/utils/vec_copy.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/x86/
--rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.h
--rw-r--r--   0 runner    (1001) docker     (127)     4131 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/cuda_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/cpu_adam/
--rw-r--r--   0 runner    (1001) docker     (127)      150 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/cpu_adam/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1090 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_arm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_x86.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/flash_attention/
--rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/flash_attention/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3761 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_dao_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1923 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_npu.py
--rw-r--r--   0 runner    (1001) docker     (127)     1800 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/inference/
--rw-r--r--   0 runner    (1001) docker     (127)       99 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5125 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/inference/inference.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     1272 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/inference/inference_ops_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/layernorm/
--rw-r--r--   0 runner    (1001) docker     (127)       89 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/layernorm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4885 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/layernorm/layer_norm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      923 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/layernorm/layernorm_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/moe/
--rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/moe/moe.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      898 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/moe/moe_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.421466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)      105 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1089 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/optimizer/fused_optimizer_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/optimizer/optimizer.cpp
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.425466 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2055 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     1002 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1690 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     1069 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/triton_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/extensions/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.425466 colossalai-nightly-2024.5.25/colossalai/kernel/jit/
--rw-r--r--   0 runner    (1001) docker     (127)      317 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/jit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      670 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/jit/bias_dropout_add.py
--rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/jit/bias_gelu.py
--rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/jit/option.py
--rw-r--r--   0 runner    (1001) docker     (127)     3791 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/kernel_loader.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.425466 colossalai-nightly-2024.5.25/colossalai/kernel/triton/
--rw-r--r--   0 runner    (1001) docker     (127)     1078 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26677 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/context_attn_unpad.py
--rw-r--r--   0 runner    (1001) docker     (127)    20403 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/flash_decoding.py
--rw-r--r--   0 runner    (1001) docker     (127)     4857 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/fused_rotary_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    10761 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/kvcache_copy.py
--rw-r--r--   0 runner    (1001) docker     (127)     6847 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/llama_act_combine_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)    20790 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/no_pad_rotary_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     4967 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/qkv_matmul_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)     4555 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/rms_layernorm.py
--rw-r--r--   0 runner    (1001) docker     (127)     4897 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/rotary_cache_copy.py
--rw-r--r--   0 runner    (1001) docker     (127)     3744 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/kernel/triton/softmax.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.425466 colossalai-nightly-2024.5.25/colossalai/lazy/
--rw-r--r--   0 runner    (1001) docker     (127)      107 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/lazy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2187 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/lazy/construction.py
--rw-r--r--   0 runner    (1001) docker     (127)    25011 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/lazy/lazy_init.py
--rw-r--r--   0 runner    (1001) docker     (127)    14790 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/lazy/pretrained.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.429466 colossalai-nightly-2024.5.25/colossalai/legacy/
--rw-r--r--   0 runner    (1001) docker     (127)      301 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.429466 colossalai-nightly-2024.5.25/colossalai/legacy/amp/
--rw-r--r--   0 runner    (1001) docker     (127)     2310 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      153 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/amp_type.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.429466 colossalai-nightly-2024.5.25/colossalai/legacy/amp/apex_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/apex_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/apex_amp/apex_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.429466 colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     2453 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13449 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6081 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/naive_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.429466 colossalai-nightly-2024.5.25/colossalai/legacy/amp/torch_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     1538 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/torch_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26771 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/torch_amp/_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3368 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/amp/torch_amp/torch_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.429466 colossalai-nightly-2024.5.25/colossalai/legacy/builder/
--rw-r--r--   0 runner    (1001) docker     (127)      166 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3025 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/builder/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.429466 colossalai-nightly-2024.5.25/colossalai/legacy/communication/
--rw-r--r--   0 runner    (1001) docker     (127)      868 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/communication/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11387 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/communication/collective.py
--rw-r--r--   0 runner    (1001) docker     (127)    17484 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/communication/p2p.py
--rw-r--r--   0 runner    (1001) docker     (127)     9047 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/communication/p2p_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)     1945 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/communication/ring.py
--rw-r--r--   0 runner    (1001) docker     (127)     5151 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/communication/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      978 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.429466 colossalai-nightly-2024.5.25/colossalai/legacy/context/
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    24229 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/parallel_context.py
--rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/parallel_mode.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.433467 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/
--rw-r--r--   0 runner    (1001) docker     (127)      763 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2144 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_1d.py
--rw-r--r--   0 runner    (1001) docker     (127)     6269 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)    12944 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)    13290 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_3d.py
--rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     2174 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     4170 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_sequence.py
--rw-r--r--   0 runner    (1001) docker     (127)     2051 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/process_group_initializer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.433467 colossalai-nightly-2024.5.25/colossalai/legacy/context/random/
--rw-r--r--   0 runner    (1001) docker     (127)      420 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/random/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5204 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/random/_helper.py
--rw-r--r--   0 runner    (1001) docker     (127)     3407 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/context/random/seed_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/core.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.433467 colossalai-nightly-2024.5.25/colossalai/legacy/engine/
--rw-r--r--   0 runner    (1001) docker     (127)       87 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7784 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/_base_engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.433467 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_accumulation/
--rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_accumulation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10265 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.433467 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/
--rw-r--r--   0 runner    (1001) docker     (127)      537 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      750 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1149 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2138 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2488 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1148 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)      749 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.437466 colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/
--rw-r--r--   0 runner    (1001) docker     (127)      315 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5816 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/_base_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     3808 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)    40085 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     7133 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)     1993 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/global_variables.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.437466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/async_engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     5938 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/async_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.437466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    13595 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/infer_batch.py
--rw-r--r--   0 runner    (1001) docker     (127)     5332 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/io_struct.py
--rw-r--r--   0 runner    (1001) docker     (127)     6304 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/ray_init_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     2810 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/req_queue.py
--rw-r--r--   0 runner    (1001) docker     (127)     3282 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/sampling_params.py
--rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/stats.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.437466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/
--rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6972 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.437466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)       80 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/modeling/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    21591 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/modeling/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.437466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/polices/
--rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/polices/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5477 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/polices/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    11609 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.441466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)       83 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9019 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/pipeline/microbatch_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.441466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)      123 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py
--rw-r--r--   0 runner    (1001) docker     (127)    21286 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     4580 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.441466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    23642 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    22757 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)    17826 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.441466 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/policies/
--rw-r--r--   0 runner    (1001) docker     (127)      209 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/policies/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     2987 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/policies/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    19842 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.441466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/
--rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.441466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/_ops/
--rw-r--r--   0 runner    (1001) docker     (127)       22 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/_ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8669 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/_ops/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.441466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/
--rw-r--r--   0 runner    (1001) docker     (127)      242 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2817 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/base_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.441466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/
--rw-r--r--   0 runner    (1001) docker     (127)      300 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      999 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)     6156 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     5224 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/normalization.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.445466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_1d/
--rw-r--r--   0 runner    (1001) docker     (127)      459 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_1d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3724 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_1d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     5063 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_1d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    44898 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_1d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.445466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2d/
--rw-r--r--   0 runner    (1001) docker     (127)      458 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    35857 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)      853 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49324 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.445466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2p5d/
--rw-r--r--   0 runner    (1001) docker     (127)      494 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2p5d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    39868 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1234 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49527 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2p5d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.445466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_3d/
--rw-r--r--   0 runner    (1001) docker     (127)      498 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_3d/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    22832 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_3d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3037 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_3d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49624 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_3d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.445466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_sequence/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_sequence/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6392 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_sequence/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)      483 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_sequence/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    10693 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_sequence/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.445466 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      445 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2411 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/utils/common.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.449467 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/vanilla/
--rw-r--r--   0 runner    (1001) docker     (127)      345 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/vanilla/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14693 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/vanilla/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.449467 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/wrapper/
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/wrapper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2170 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.449467 colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/
--rw-r--r--   0 runner    (1001) docker     (127)     1592 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/loss_1d.py
--rw-r--r--   0 runner    (1001) docker     (127)     5732 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/loss_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)     5540 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/loss_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)     6436 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/loss_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.449467 colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/
--rw-r--r--   0 runner    (1001) docker     (127)      686 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      148 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      787 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/accuracy_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)      801 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/accuracy_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)     1263 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/accuracy_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.449467 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/
--rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6469 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/data_parallel.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.449467 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/
--rw-r--r--   0 runner    (1001) docker     (127)      962 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.453467 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/
--rw-r--r--   0 runner    (1001) docker     (127)      742 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    27958 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     8710 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py
--rw-r--r--   0 runner    (1001) docker     (127)      807 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     5690 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     9962 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py
--rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py
--rw-r--r--   0 runner    (1001) docker     (127)     1954 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/colo_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     1150 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     4780 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/module_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3874 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/reducer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.453467 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)      163 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/layer_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.453467 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/middleware/
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/middleware/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.453467 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/middleware/adaptor/
--rw-r--r--   0 runner    (1001) docker     (127)       79 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/middleware/adaptor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6151 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/middleware/adaptor/fx.py
--rw-r--r--   0 runner    (1001) docker     (127)     6818 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/middleware/topo.py
--rw-r--r--   0 runner    (1001) docker     (127)    11447 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/pipelinable.py
--rw-r--r--   0 runner    (1001) docker     (127)     5759 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/pipeline_process_group.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.453467 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/rpc/
--rw-r--r--   0 runner    (1001) docker     (127)      237 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/rpc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    59091 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/rpc/_pipeline_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    14979 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     5382 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/rpc/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9017 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.453467 colossalai-nightly-2024.5.25/colossalai/legacy/registry/
--rw-r--r--   0 runner    (1001) docker     (127)      690 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/registry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3052 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/registry/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.457466 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      418 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      779 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/compute_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/const.py
--rw-r--r--   0 runner    (1001) docker     (127)     8692 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/dist_spec_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/distspec.py
--rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/op_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)    10505 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/process_group.py
--rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/tensor/tensor_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.457466 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/
--rw-r--r--   0 runner    (1001) docker     (127)       53 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14773 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.457466 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/
--rw-r--r--   0 runner    (1001) docker     (127)      648 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2712 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_base_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     3226 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_checkpoint_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_commons_.py
--rw-r--r--   0 runner    (1001) docker     (127)    13085 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_log_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     2091 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)    16242 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_metric_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.457466 colossalai-nightly-2024.5.25/colossalai/legacy/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     1438 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9872 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/activation_checkpoint.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.457466 colossalai-nightly-2024.5.25/colossalai/legacy/utils/checkpoint/
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/checkpoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5297 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/checkpoint/module_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/checkpoint/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    11338 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/checkpointing.py
--rw-r--r--   0 runner    (1001) docker     (127)    16595 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/common.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.457466 colossalai-nightly-2024.5.25/colossalai/legacy/utils/data_sampler/
--rw-r--r--   0 runner    (1001) docker     (127)      177 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/data_sampler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/data_sampler/base_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6704 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6416 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/memory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.461467 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/
--rw-r--r--   0 runner    (1001) docker     (127)       52 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      341 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/extention.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.461467 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/legacy/
--rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/legacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10461 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/legacy/comm_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4861 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3776 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/legacy/prof_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8459 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4036 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.461467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/
--rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.461467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/
--rw-r--r--   0 runner    (1001) docker     (127)      619 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7315 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/colo_init_context.py
--rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/gemini_context.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.461467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/
--rw-r--r--   0 runner    (1001) docker     (127)      118 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      774 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py
--rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py
--rw-r--r--   0 runner    (1001) docker     (127)     5081 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     4715 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.461467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/paramhooks/
--rw-r--r--   0 runner    (1001) docker     (127)       77 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/paramhooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1251 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     6905 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/stateful_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3965 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     6369 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/tensor_placement_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)     3807 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/tensor_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.461467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/init_ctx/
--rw-r--r--   0 runner    (1001) docker     (127)      171 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/init_ctx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11099 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/init_ctx/init_context.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.461467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/
--rw-r--r--   0 runner    (1001) docker     (127)      259 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      635 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/base_shard_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)     2296 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)      706 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/commons.py
--rw-r--r--   0 runner    (1001) docker     (127)     2751 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.465467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/
--rw-r--r--   0 runner    (1001) docker     (127)       75 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3003 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8289 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/reduce_scatter.py
--rw-r--r--   0 runner    (1001) docker     (127)    28757 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/sharded_model_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)      808 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4865 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/zero_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.465467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_optim/
--rw-r--r--   0 runner    (1001) docker     (127)       83 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_optim/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18982 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.465467 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_param/
--rw-r--r--   0 runner    (1001) docker     (127)      131 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_param/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3859 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_param/sharded_param.py
--rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_param/sharded_tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.465467 colossalai-nightly-2024.5.25/colossalai/logging/
--rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/logging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6110 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/logging/logger.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.465467 colossalai-nightly-2024.5.25/colossalai/moe/
--rw-r--r--   0 runner    (1001) docker     (127)      531 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12432 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)    36076 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     6249 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/experts.py
--rw-r--r--   0 runner    (1001) docker     (127)    16261 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)    18267 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/load_balance.py
--rw-r--r--   0 runner    (1001) docker     (127)     3075 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)     6138 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/manager.py
--rw-r--r--   0 runner    (1001) docker     (127)    20401 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/routers.py
--rw-r--r--   0 runner    (1001) docker     (127)     7403 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/moe/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.465467 colossalai-nightly-2024.5.25/colossalai/nn/
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9563 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/init.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.469467 colossalai-nightly-2024.5.25/colossalai/nn/layer/
--rw-r--r--   0 runner    (1001) docker     (127)       21 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2497 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/layer/layernorm.py
--rw-r--r--   0 runner    (1001) docker     (127)     6739 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/layer/scaled_softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)      449 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/layer/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.469467 colossalai-nightly-2024.5.25/colossalai/nn/loss/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/loss/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.469467 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/
--rw-r--r--   0 runner    (1001) docker     (127)      673 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5867 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/cosine.py
--rw-r--r--   0 runner    (1001) docker     (127)     7717 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/delayed.py
--rw-r--r--   0 runner    (1001) docker     (127)     1178 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     2672 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/multistep.py
--rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/onecycle.py
--rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/poly.py
--rw-r--r--   0 runner    (1001) docker     (127)     3516 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/torch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.473466 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)     1517 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7195 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/adafactor.py
--rw-r--r--   0 runner    (1001) docker     (127)     5955 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/came.py
--rw-r--r--   0 runner    (1001) docker     (127)     8611 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/cpu_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)    21082 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/distributed_adafactor.py
--rw-r--r--   0 runner    (1001) docker     (127)    28254 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/distributed_came.py
--rw-r--r--   0 runner    (1001) docker     (127)    12580 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/distributed_galore.py
--rw-r--r--   0 runner    (1001) docker     (127)     7665 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/distributed_lamb.py
--rw-r--r--   0 runner    (1001) docker     (127)     6478 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/fused_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)     9056 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/fused_lamb.py
--rw-r--r--   0 runner    (1001) docker     (127)     6012 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/fused_sgd.py
--rw-r--r--   0 runner    (1001) docker     (127)    12125 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/galore.py
--rw-r--r--   0 runner    (1001) docker     (127)     8004 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/hybrid_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)     4526 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/lamb.py
--rw-r--r--   0 runner    (1001) docker     (127)     3567 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/lars.py
--rw-r--r--   0 runner    (1001) docker     (127)     6768 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/nn/optimizer/nvme_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.473466 colossalai-nightly-2024.5.25/colossalai/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)      344 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26607 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/p2p.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.473466 colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/
--rw-r--r--   0 runner    (1001) docker     (127)      241 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5194 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1478 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/base.py
--rw-r--r--   0 runner    (1001) docker     (127)    20009 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/generate.py
--rw-r--r--   0 runner    (1001) docker     (127)    26918 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/interleaved_pp.py
--rw-r--r--   0 runner    (1001) docker     (127)    19591 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/one_f_one_b.py
--rw-r--r--   0 runner    (1001) docker     (127)     9552 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/pipeline/stage_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.473466 colossalai-nightly-2024.5.25/colossalai/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      144 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13038 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/quantization/bnb.py
--rw-r--r--   0 runner    (1001) docker     (127)     4399 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/quantization/bnb_config.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.473466 colossalai-nightly-2024.5.25/colossalai/shardformer/
--rw-r--r--   0 runner    (1001) docker     (127)      118 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3327 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.477467 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/
--rw-r--r--   0 runner    (1001) docker     (127)     1119 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    38285 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)    13198 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/attn.py
--rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)    15864 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    26710 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     5174 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)    11530 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)    17676 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/parallel_module.py
--rw-r--r--   0 runner    (1001) docker     (127)    31848 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/qkv_fused_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    10572 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/layer/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.477467 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    62664 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     5064 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)    53779 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    17629 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/chatglm2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.481467 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/chatglm2_6b/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/chatglm2_6b/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2249 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py
--rw-r--r--   0 runner    (1001) docker     (127)    54608 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py
--rw-r--r--   0 runner    (1001) docker     (127)    38507 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)    59934 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)    44054 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)     1053 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/jit.py
--rw-r--r--   0 runner    (1001) docker     (127)    46476 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    32013 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)    45316 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)    33866 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/qwen2.py
--rw-r--r--   0 runner    (1001) docker     (127)     8328 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)    37484 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)    16136 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)    54363 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.481467 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11302 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/auto_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)     8822 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/base_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)    27227 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)    15862 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)    17787 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    12232 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)    16924 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)    22241 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)    14001 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)    18920 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    14853 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)    14258 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)    14648 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/qwen2.py
--rw-r--r--   0 runner    (1001) docker     (127)     9682 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)    22174 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)    10928 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)    23410 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/policies/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.485467 colossalai-nightly-2024.5.25/colossalai/shardformer/shard/
--rw-r--r--   0 runner    (1001) docker     (127)      320 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3763 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/shard/grad_ckpt_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     6438 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/shard/shard_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     9749 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/shard/sharder.py
--rw-r--r--   0 runner    (1001) docker     (127)     1949 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/shard/shardformer.py
--rw-r--r--   0 runner    (1001) docker     (127)      550 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/shardformer/shard/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.485467 colossalai-nightly-2024.5.25/colossalai/tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      600 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3447 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/colo_parameter.py
--rw-r--r--   0 runner    (1001) docker     (127)     3349 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/colo_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)    21633 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/comm_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.485467 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/
--rw-r--r--   0 runner    (1001) docker     (127)     1249 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18592 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/api.py
--rw-r--r--   0 runner    (1001) docker     (127)    11144 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/comm_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/layout.py
--rw-r--r--   0 runner    (1001) docker     (127)    27564 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/layout_converter.py
--rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/misc.py
--rw-r--r--   0 runner    (1001) docker     (127)     9490 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/sharding_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     3263 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.485467 colossalai-nightly-2024.5.25/colossalai/tensor/moe_tensor/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/moe_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3778 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/moe_tensor/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     1413 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/moe_tensor/moe_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.485467 colossalai-nightly-2024.5.25/colossalai/tensor/padded_tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/padded_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3527 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/padded_tensor/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     5169 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/param_op_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)    35882 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/shape_consistency.py
--rw-r--r--   0 runner    (1001) docker     (127)    11626 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/sharding_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     8452 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/tensor/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.489467 colossalai-nightly-2024.5.25/colossalai/testing/
--rw-r--r--   0 runner    (1001) docker     (127)      865 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/testing/comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     1406 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/testing/pytest_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)      542 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/testing/random.py
--rw-r--r--   0 runner    (1001) docker     (127)     9201 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/testing/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.489467 colossalai-nightly-2024.5.25/colossalai/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      586 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1938 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/common.py
--rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/memory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.489467 colossalai-nightly-2024.5.25/colossalai/utils/model/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3905 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/model/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.489467 colossalai-nightly-2024.5.25/colossalai/utils/multi_tensor_apply/
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/multi_tensor_apply/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.489467 colossalai-nightly-2024.5.25/colossalai/utils/rank_recorder/
--rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/rank_recorder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5453 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/rank_recorder/rank_recorder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.489467 colossalai-nightly-2024.5.25/colossalai/utils/tensor_detector/
--rw-r--r--   0 runner    (1001) docker     (127)       44 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/tensor_detector/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8392 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/tensor_detector/tensor_detector.py
--rw-r--r--   0 runner    (1001) docker     (127)     4321 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/utils/timer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.489467 colossalai-nightly-2024.5.25/colossalai/zero/
--rw-r--r--   0 runner    (1001) docker     (127)      390 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.489467 colossalai-nightly-2024.5.25/colossalai/zero/gemini/
--rw-r--r--   0 runner    (1001) docker     (127)      490 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.493467 colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/
--rw-r--r--   0 runner    (1001) docker     (127)      342 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    25798 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/chunk.py
--rw-r--r--   0 runner    (1001) docker     (127)    12155 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     6291 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/search_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1529 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    45450 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/gemini_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/gemini_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     6243 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/gemini_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)    37815 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/gemini_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.493467 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/
--rw-r--r--   0 runner    (1001) docker     (127)      511 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1281 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     4063 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/memory_monitor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/memory_stats.py
--rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)      858 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/param_runtime_order.py
--rw-r--r--   0 runner    (1001) docker     (127)     3716 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py
--rw-r--r--   0 runner    (1001) docker     (127)     4126 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9569 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/placement_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)     4213 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/gemini/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.493467 colossalai-nightly-2024.5.25/colossalai/zero/low_level/
--rw-r--r--   0 runner    (1001) docker     (127)       88 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7283 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.493467 colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/
--rw-r--r--   0 runner    (1001) docker     (127)      242 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      442 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/base_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     5939 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/bucket_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     4537 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/gradient_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     1725 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/parameter_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     1494 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/tensor_bucket.py
--rw-r--r--   0 runner    (1001) docker     (127)    46468 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/low_level/low_level_optim.py
--rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/colossalai/zero/wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.497467 colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    36485 2024-05-25 00:14:47.000000 colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    53244 2024-05-25 00:14:47.000000 colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-25 00:14:47.000000 colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)       69 2024-05-25 00:14:47.000000 colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (127)      254 2024-05-25 00:14:47.000000 colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       37 2024-05-25 00:14:47.000000 colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.357466 colossalai-nightly-2024.5.25/examples/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.497467 colossalai-nightly-2024.5.25/examples/language/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/examples/language/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4227 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/examples/language/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      713 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/examples/language/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4385 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/examples/language/performance_evaluator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.497467 colossalai-nightly-2024.5.25/extensions/
--rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/base_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     4879 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/cpp_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.497467 colossalai-nightly-2024.5.25/extensions/csrc/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.497467 colossalai-nightly-2024.5.25/extensions/csrc/common/
--rw-r--r--   0 runner    (1001) docker     (127)      694 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/common/data_type.h
--rw-r--r--   0 runner    (1001) docker     (127)    14986 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/common/micros.h
--rw-r--r--   0 runner    (1001) docker     (127)      901 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/common/mp_type_traits.h
--rw-r--r--   0 runner    (1001) docker     (127)     2886 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/common/target.h
--rw-r--r--   0 runner    (1001) docker     (127)     2797 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/common/vec_type_traits.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.497467 colossalai-nightly-2024.5.25/extensions/csrc/funcs/
--rw-r--r--   0 runner    (1001) docker     (127)    10622 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/funcs/binary_functor.h
--rw-r--r--   0 runner    (1001) docker     (127)    21630 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/funcs/cast_functor.h
--rw-r--r--   0 runner    (1001) docker     (127)     3932 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/funcs/reduce_function.h
--rw-r--r--   0 runner    (1001) docker     (127)     8393 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/funcs/ternary_functor.h
--rw-r--r--   0 runner    (1001) docker     (127)     2460 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/funcs/unary_functor.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.361466 colossalai-nightly-2024.5.25/extensions/csrc/kernel/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.497467 colossalai-nightly-2024.5.25/extensions/csrc/kernel/arm/
--rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/arm/cpu_adam_arm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/arm/cpu_adam_arm.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.501467 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)     2614 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/activation_kernel.cu
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.501467 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/attention/
--rw-r--r--   0 runner    (1001) docker     (127)     6820 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/attention/attention_utils.h
--rw-r--r--   0 runner    (1001) docker     (127)    10773 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     3931 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    10013 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    41480 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    21861 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     9781 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    25833 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/layer_norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    25880 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/moe_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5207 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh
--rw-r--r--   0 runner    (1001) docker     (127)    15325 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    13119 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     4444 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     6486 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    14092 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    22113 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    23452 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.501467 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     2538 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/utils/gpu_launch_config.h
--rw-r--r--   0 runner    (1001) docker     (127)      545 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/utils/micros.h
--rw-r--r--   0 runner    (1001) docker     (127)     1287 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/utils/nvgpu_dev_info.h
--rw-r--r--   0 runner    (1001) docker     (127)     2063 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/utils/vec_copy.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.501467 colossalai-nightly-2024.5.25/extensions/csrc/kernel/x86/
--rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/x86/cpu_adam.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/csrc/kernel/x86/cpu_adam.h
--rw-r--r--   0 runner    (1001) docker     (127)     4131 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/cuda_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.501467 colossalai-nightly-2024.5.25/extensions/pybind/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/extensions/pybind/cpu_adam/
--rw-r--r--   0 runner    (1001) docker     (127)      150 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/cpu_adam/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1090 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/cpu_adam/cpu_adam_arm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/cpu_adam/cpu_adam_x86.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/extensions/pybind/flash_attention/
--rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/flash_attention/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3761 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/flash_attention/flash_attention_dao_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1923 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/flash_attention/flash_attention_npu.py
--rw-r--r--   0 runner    (1001) docker     (127)     1800 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/extensions/pybind/inference/
--rw-r--r--   0 runner    (1001) docker     (127)       99 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5125 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/inference/inference.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     1272 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/inference/inference_ops_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/extensions/pybind/layernorm/
--rw-r--r--   0 runner    (1001) docker     (127)       89 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/layernorm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4885 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/layernorm/layer_norm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      923 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/layernorm/layernorm_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/extensions/pybind/moe/
--rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/moe/moe.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      898 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/moe/moe_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/extensions/pybind/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)      105 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1089 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/optimizer/fused_optimizer_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/optimizer/optimizer.cpp
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/extensions/pybind/softmax/
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/softmax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2055 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/softmax/scaled_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     1002 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/softmax/scaled_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1690 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     1069 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/triton_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/extensions/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/requirements/
--rw-r--r--   0 runner    (1001) docker     (127)      520 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/requirements/requirements-test.txt
--rw-r--r--   0 runner    (1001) docker     (127)      254 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/requirements/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-25 00:14:47.529467 colossalai-nightly-2024.5.25/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     4353 2024-05-25 00:14:46.000000 colossalai-nightly-2024.5.25/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.361466 colossalai-nightly-2024.5.25/tests/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.505467 colossalai-nightly-2024.5.25/tests/kit/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.509467 colossalai-nightly-2024.5.25/tests/kit/model_zoo/
--rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.509467 colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/
--rw-r--r--   0 runner    (1001) docker     (127)      155 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      832 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/hanging_param_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/nested_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/repeated_computed_layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1707 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/simple_mlp.py
--rw-r--r--   0 runner    (1001) docker     (127)     1259 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/simple_net.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.509467 colossalai-nightly-2024.5.25/tests/kit/model_zoo/diffusers/
--rw-r--r--   0 runner    (1001) docker     (127)       25 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/diffusers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2534 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/diffusers/diffusers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1499 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/executor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3413 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.509467 colossalai-nightly-2024.5.25/tests/kit/model_zoo/timm/
--rw-r--r--   0 runner    (1001) docker     (127)       20 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/timm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5975 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/timm/timm.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.509467 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchaudio/
--rw-r--r--   0 runner    (1001) docker     (127)       26 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchaudio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4614 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchaudio/torchaudio.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.509467 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchrec/
--rw-r--r--   0 runner    (1001) docker     (127)       24 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchrec/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3899 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchrec/torchrec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.509467 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchvision/
--rw-r--r--   0 runner    (1001) docker     (127)       27 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchvision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4970 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchvision/torchvision.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.513467 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/
--rw-r--r--   0 runner    (1001) docker     (127)      524 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3768 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/albert.py
--rw-r--r--   0 runner    (1001) docker     (127)    12561 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     2289 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4487 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     2282 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4251 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)     6361 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/gpt.py
--rw-r--r--   0 runner    (1001) docker     (127)     3597 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)     3328 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     2837 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)     3063 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     3015 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/qwen2.py
--rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)     3061 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)     2214 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)     3717 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.513467 colossalai-nightly-2024.5.25/tests/test_analyzer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.513467 colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3684 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_bias_addition.py
--rw-r--r--   0 runner    (1001) docker     (127)     2139 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_mod_dir.py
--rw-r--r--   0 runner    (1001) docker     (127)     1750 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_nested_ckpt.py
--rw-r--r--   0 runner    (1001) docker     (127)     2127 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_shape_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_symbolic_profile.py
--rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/zoo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.513467 colossalai-nightly-2024.5.25/tests/test_analyzer/test_subclasses/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_subclasses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3714 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_subclasses/test_aten.py
--rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_subclasses/test_flop_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_analyzer/test_subclasses/test_meta_mode.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.513467 colossalai-nightly-2024.5.25/tests/test_auto_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.513467 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_pass/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_pass/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_pass/test_node_converting_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.517467 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2702 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
--rw-r--r--   0 runner    (1001) docker     (127)     2003 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
--rw-r--r--   0 runner    (1001) docker     (127)     2446 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     3383 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)     3733 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
--rw-r--r--   0 runner    (1001) docker     (127)     3674 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.517467 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_gpt/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10992 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
--rw-r--r--   0 runner    (1001) docker     (127)     7729 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
--rw-r--r--   0 runner    (1001) docker     (127)     3818 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2052 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.521467 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11120 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     7521 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4701 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6548 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py
--rw-r--r--   0 runner    (1001) docker     (127)     6034 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py
--rw-r--r--   0 runner    (1001) docker     (127)    10708 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8717 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12574 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3595 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    11426 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2942 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     7708 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4231 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12825 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8338 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2556 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    18264 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3056 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4168 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py
--rw-r--r--   0 runner    (1001) docker     (127)     8698 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12216 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12137 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2781 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3625 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    13847 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3556 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8505 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5283 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.525467 colossalai-nightly-2024.5.25/tests/test_infer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1210 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5233 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_batch_bucket.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1195 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_config_and_struct.py
--rw-r--r--   0 runner    (1001) docker     (127)     2347 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_continuous_batching.py
--rw-r--r--   0 runner    (1001) docker     (127)     2942 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_cuda_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     2607 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_drafter.py
--rw-r--r--   0 runner    (1001) docker     (127)     7403 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_inference_engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.525467 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.525467 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/test_convert_fp8.py
--rw-r--r--   0 runner    (1001) docker     (127)    11573 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     1792 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py
--rw-r--r--   0 runner    (1001) docker     (127)     5290 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py
--rw-r--r--   0 runner    (1001) docker     (127)     1586 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/test_rms_layernorm.py
--rw-r--r--   0 runner    (1001) docker     (127)     5544 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py
--rw-r--r--   0 runner    (1001) docker     (127)     1099 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/cuda/test_silu_and_mul.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.525467 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15777 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/kernel_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6808 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/test_context_attn_unpad.py
--rw-r--r--   0 runner    (1001) docker     (127)     7595 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/test_decoding_attn.py
--rw-r--r--   0 runner    (1001) docker     (127)     1647 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     6411 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/test_kvcache_copy.py
--rw-r--r--   0 runner    (1001) docker     (127)     1692 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py
--rw-r--r--   0 runner    (1001) docker     (127)     4011 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py
--rw-r--r--   0 runner    (1001) docker     (127)     2562 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kernels/triton/test_xine_copy.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6796 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_kvcache_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     2964 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_request_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4116 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_infer/test_rpc_engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.525467 colossalai-nightly-2024.5.25/tests/test_shardformer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5683 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_flash_attention.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:47.529467 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15543 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8340 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     3196 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)     7723 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     8331 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     6763 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)     8635 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)     7743 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)    12165 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     5625 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)     7402 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     7600 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_qwen2.py
--rw-r--r--   0 runner    (1001) docker     (127)     2557 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_sam.py
--rw-r--r--   0 runner    (1001) docker     (127)     7227 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_t5.py
--rw-r--r--   0 runner    (1001) docker     (127)     6528 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_vit.py
--rw-r--r--   0 runner    (1001) docker     (127)     7293 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_whisper.py
--rw-r--r--   0 runner    (1001) docker     (127)      833 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2623 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/tests/test_shardformer/test_with_torch_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)        6 2024-05-25 00:14:39.000000 colossalai-nightly-2024.5.25/version.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/
+-rw-r--r--   0 runner    (1001) docker     (127)    30134 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)      198 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)    36765 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    31195 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_C/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_C/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/
+-rw-r--r--   0 runner    (1001) docker     (127)      165 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_meta_registration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_monkey_patch.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18677 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/flop_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/meta_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/envs.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18998 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/codegen.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9947 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/graph_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8482 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/node_util.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)      106 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/graph_profile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/shape_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)      952 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/symbolic_profile.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5415 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/bias_addition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/custom_leaf_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/proxy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5862 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/symbolic_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15712 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/tracer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/accelerator/
+-rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/base_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10154 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/cpu_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/cuda_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/npu_accelerator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/
+-rw-r--r--   0 runner    (1001) docker     (127)      222 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2119 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4977 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/
+-rw-r--r--   0 runner    (1001) docker     (127)      226 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2523 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      504 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/bf16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2695 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (127)      155 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      377 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/build_c_ext.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7711 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3468 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4921 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/operation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)       95 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/
+-rw-r--r--   0 runner    (1001) docker     (127)      241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3940 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2840 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7571 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2512 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24482 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9318 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3258 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2827 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/where.py
+-rw-r--r--   0 runner    (1001) docker     (127)      761 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4748 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/shard_metainfo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6826 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/amp_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3584 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/base_offload_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2072 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/mem_optimize.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5167 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20031 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9909 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/runtime.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18503 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/solver.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17919 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/training_simulator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2793 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/util.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5111 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/comm_metainfo_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)      417 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/meta_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_apply_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22439 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_preparation_pass.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/pipeline_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/pipeline_shard/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16998 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.297213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)     2062 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3731 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4988 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4805 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5563 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11414 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1316 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1901 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13535 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20347 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16308 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1762 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2997 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1466 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      744 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1980 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/
+-rw-r--r--   0 runner    (1001) docker     (127)     2100 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14274 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5162 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24078 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3615 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7361 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41664 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3636 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18860 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4702 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2278 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3981 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4158 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2330 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1774 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1964 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3553 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/options.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10812 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/sharding_strategy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/
+-rw-r--r--   0 runner    (1001) docker     (127)      238 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9987 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5767 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/solver.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8474 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5899 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/broadcast.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8346 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3841 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/reshape.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/sharding.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/booster/
+-rw-r--r--   0 runner    (1001) docker     (127)       93 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1481 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20278 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/booster.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)     1298 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      102 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/bf16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3218 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_apex.py
+-rw-r--r--   0 runner    (1001) docker     (127)      870 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_naive.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4824 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_torch.py
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp8.py
+-rw-r--r--   0 runner    (1001) docker     (127)      565 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/mixed_precision_base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/booster/plugin/
+-rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/dp_plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28131 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/gemini_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    63839 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/hybrid_parallel_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19546 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/low_level_zero_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20853 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2475 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      559 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/pp_plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9770 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_ddp_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15017 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_fsdp_plugin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/
+-rw-r--r--   0 runner    (1001) docker     (127)      318 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15804 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/checkpoint_io_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8761 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/general_checkpoint_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)    43972 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5758 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/index_file.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29632 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/cli/
+-rw-r--r--   0 runner    (1001) docker     (127)       40 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/cli/check/
+-rw-r--r--   0 runner    (1001) docker     (127)      396 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/check/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8454 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/check/check_installation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      310 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/cli.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/cli/launcher/
+-rw-r--r--   0 runner    (1001) docker     (127)     3654 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/hostinfo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4286 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/multinode_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/run.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/cluster/
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/device_mesh_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7233 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/dist_coordinator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2356 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/process_group_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10848 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/process_group_mesh.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/context/
+-rw-r--r--   0 runner    (1001) docker     (127)       96 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3153 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)      921 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/singleton_meta.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/device/
+-rw-r--r--   0 runner    (1001) docker     (127)      139 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/alpha_beta_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5634 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/calc_pipeline_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23616 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/device_mesh.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/fx/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_12.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_13.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/fx/codegen/
+-rw-r--r--   0 runner    (1001) docker     (127)       45 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/codegen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/codegen/activation_checkpoint_codegen.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/graph_module.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13496 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/adding_split_node_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12261 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/concrete_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/meta_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15886 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/passes_for_gpt2_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6888 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/shard_1d_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/split_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6169 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)      768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      871 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6285 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/dataflow.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)      282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      782 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7062 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      211 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py
+-rw-r--r--   0 runner    (1001) docker     (127)      632 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1188 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)      402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/python_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2188 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/
+-rw-r--r--   0 runner    (1001) docker     (127)      252 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2037 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      424 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      495 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1582 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1013 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/rnn.py
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/torch_op.py
+-rw-r--r--   0 runner    (1001) docker     (127)      603 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2232 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/memory_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/opcount.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15377 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4235 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4990 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4010 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/proxy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)      201 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4850 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_meta_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2197 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_symbolic_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_tracer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/
+-rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      193 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2171 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4445 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)      745 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/
+-rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2370 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)      503 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/experimental.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/
+-rw-r--r--   0 runner    (1001) docker     (127)       62 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      167 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5707 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      517 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5622 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/
+-rw-r--r--   0 runner    (1001) docker     (127)      180 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      428 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4752 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      254 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      400 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6769 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)      644 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/rnn.py
+-rw-r--r--   0 runner    (1001) docker     (127)      834 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/tracer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/inference/
+-rw-r--r--   0 runner    (1001) docker     (127)      235 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/
+-rw-r--r--   0 runner    (1001) docker     (127)       67 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8949 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/microbatch_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21828 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)      360 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8485 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/
+-rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/batch_infer_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4582 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/kvcache_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/
+-rw-r--r--   0 runner    (1001) docker     (127)       61 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/
+-rw-r--r--   0 runner    (1001) docker     (127)      155 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/
+-rw-r--r--   0 runner    (1001) docker     (127)      372 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13893 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/cai_quant_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1629 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/gptq_op.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2423 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/gptq_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/
+-rw-r--r--   0 runner    (1001) docker     (127)      297 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/base_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6500 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35519 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10276 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/parallel_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6088 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/interface/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4120 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      333 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/pretrained.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/base_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4730 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpp_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/
+-rw-r--r--   0 runner    (1001) docker     (127)      150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_arm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/
+-rw-r--r--   0 runner    (1001) docker     (127)      350 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/
+-rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)      214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/compat.h
+-rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/include/
+-rw-r--r--   0 runner    (1001) docker     (127)     8585 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h
+-rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    25829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    25627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh
+-rw-r--r--   0 runner    (1001) docker     (127)    13279 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    13115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     6479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     2684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    21112 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     2066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    23530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    14851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/type_shim.h
+-rw-r--r--   0 runner    (1001) docker     (127)     6588 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/scaled_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cuda_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/
+-rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3760 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_npu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_sdpa_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/
+-rw-r--r--   0 runner    (1001) docker     (127)       89 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/layernorm_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/moe_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      105 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/triton_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/jit/
+-rw-r--r--   0 runner    (1001) docker     (127)      317 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      670 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_dropout_add.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_gelu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/option.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3649 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/kernel_loader.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/triton/
+-rw-r--r--   0 runner    (1001) docker     (127)     1096 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14981 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/context_attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2349 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/copy_kv_cache_dest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7022 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/custom_autotune.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1804 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/flash_decoding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/fused_layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18024 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/gptq_triton.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/int8_rotary_embedding_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6847 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/llama_act_combine_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4967 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/qkv_matmul_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5947 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/self_attention_nofusion.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/smooth_attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3744 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7896 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/token_attention_kernel.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/lazy/
+-rw-r--r--   0 runner    (1001) docker     (127)      107 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2187 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/construction.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25011 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/lazy_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13895 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/pretrained.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)      301 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     2310 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      153 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/amp_type.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/apex_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     2453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13449 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6081 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/naive_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     1538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26771 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3368 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/torch_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/builder/
+-rw-r--r--   0 runner    (1001) docker     (127)      166 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/builder/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/communication/
+-rw-r--r--   0 runner    (1001) docker     (127)      868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11387 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/collective.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17484 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9047 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1945 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/ring.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5151 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      978 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24229 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_context.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_mode.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/
+-rw-r--r--   0 runner    (1001) docker     (127)      763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_1d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6269 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12944 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13290 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_3d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2174 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4170 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_sequence.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2051 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/process_group_initializer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/
+-rw-r--r--   0 runner    (1001) docker     (127)      420 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5204 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/_helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3407 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/seed_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/core.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/
+-rw-r--r--   0 runner    (1001) docker     (127)       87 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7784 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/_base_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/
+-rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10265 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)      537 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      750 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2488 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1148 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      749 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/
+-rw-r--r--   0 runner    (1001) docker     (127)      315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5816 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_base_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3808 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40085 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7133 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1993 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/global_variables.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5935 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/infer_batch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5332 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/io_struct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6304 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_init_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2810 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/req_queue.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/sampling_params.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/stats.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/
+-rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6972 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)       80 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/
+-rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5477 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11605 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)       83 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9019 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/microbatch_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)      123 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21286 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4580 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22757 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17826 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)      209 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2987 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19842 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/
+-rw-r--r--   0 runner    (1001) docker     (127)       22 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8669 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)      242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2817 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/base_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/
+-rw-r--r--   0 runner    (1001) docker     (127)      300 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      999 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6156 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5224 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/normalization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/
+-rw-r--r--   0 runner    (1001) docker     (127)      459 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3724 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/
+-rw-r--r--   0 runner    (1001) docker     (127)      458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35857 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      853 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49324 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/
+-rw-r--r--   0 runner    (1001) docker     (127)      494 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1234 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49527 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/
+-rw-r--r--   0 runner    (1001) docker     (127)      498 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    22832 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3037 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10693 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      445 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2411 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/common.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/
+-rw-r--r--   0 runner    (1001) docker     (127)      345 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14693 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2170 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/
+-rw-r--r--   0 runner    (1001) docker     (127)     1592 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_1d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5732 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5540 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6436 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/
+-rw-r--r--   0 runner    (1001) docker     (127)      686 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      148 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      787 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)      801 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1263 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6469 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/data_parallel.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/
+-rw-r--r--   0 runner    (1001) docker     (127)      962 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/
+-rw-r--r--   0 runner    (1001) docker     (127)      742 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27958 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8710 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      807 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5690 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9962 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1954 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/colo_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4780 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/module_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3874 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/reducer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)      163 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/layer_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/
+-rw-r--r--   0 runner    (1001) docker     (127)       79 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6151 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/fx.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/topo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11447 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipelinable.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5759 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipeline_process_group.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      237 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    59091 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14979 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5382 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9017 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/registry/
+-rw-r--r--   0 runner    (1001) docker     (127)      690 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/registry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3052 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/registry/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      418 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      779 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/compute_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/const.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8692 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/dist_spec_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/distspec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/op_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10505 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/process_group.py
+-rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/tensor_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/
+-rw-r--r--   0 runner    (1001) docker     (127)       53 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/
+-rw-r--r--   0 runner    (1001) docker     (127)      648 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2712 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_base_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3226 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_commons_.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13085 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_log_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2091 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_metric_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9872 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/activation_checkpoint.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5297 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/module_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11338 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpointing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/common.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/
+-rw-r--r--   0 runner    (1001) docker     (127)      177 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/base_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6704 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6416 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)       52 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      341 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/extention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10461 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/comm_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4861 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3776 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/prof_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8459 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4036 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/
+-rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/
+-rw-r--r--   0 runner    (1001) docker     (127)      619 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/colo_init_context.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/gemini_context.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/
+-rw-r--r--   0 runner    (1001) docker     (127)      118 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      774 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5081 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4715 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/
+-rw-r--r--   0 runner    (1001) docker     (127)       77 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1251 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6905 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6369 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_placement_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3807 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/
+-rw-r--r--   0 runner    (1001) docker     (127)      171 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11099 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/init_context.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      259 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      635 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/base_shard_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)      706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/commons.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2751 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/
+-rw-r--r--   0 runner    (1001) docker     (127)       75 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3003 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8289 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/reduce_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28757 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/sharded_model_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)      808 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/zero_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/
+-rw-r--r--   0 runner    (1001) docker     (127)       83 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18982 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/
+-rw-r--r--   0 runner    (1001) docker     (127)      131 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3859 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_param.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/logging/
+-rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/logging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6110 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/logging/logger.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)      531 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12432 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36076 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6249 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/experts.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16261 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18267 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/load_balance.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3075 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20401 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/routers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7403 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9563 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/init.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)       21 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2497 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6739 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/scaled_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)      449 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/loss/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/loss/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/
+-rw-r--r--   0 runner    (1001) docker     (127)      673 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5867 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/cosine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/delayed.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1178 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2672 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/multistep.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/onecycle.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/poly.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3516 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/torch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      303 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8611 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/cpu_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6478 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9056 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_lamb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6012 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_sgd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8004 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/hybrid_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4423 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lamb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3567 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lars.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/nvme_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)      344 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26607 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/p2p.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/
+-rw-r--r--   0 runner    (1001) docker     (127)      241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5194 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1478 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20009 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/generate.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26918 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/interleaved_pp.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/one_f_one_b.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9552 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/stage_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13038 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/bnb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4399 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/bnb_config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/shardformer/
+-rw-r--r--   0 runner    (1001) docker     (127)      118 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3327 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)     1119 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38285 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13198 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/attn.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26710 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5018 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17676 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/parallel_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31848 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/qkv_fused_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10572 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.389213 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    62664 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5064 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49669 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17629 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.389213 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2249 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54608 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34314 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)    59838 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44054 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/jit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46388 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27319 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36942 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37484 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16136 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54363 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10821 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/auto_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/base_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15862 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17356 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12232 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16474 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14001 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18711 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14549 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9682 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22174 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10928 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23410 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/
+-rw-r--r--   0 runner    (1001) docker     (127)      320 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/grad_ckpt_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shard_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9749 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/sharder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shardformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      550 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      600 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3447 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/colo_parameter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3349 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/colo_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21633 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/comm_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)     1203 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18034 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/comm_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27564 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout_converter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/misc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9293 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/sharding_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3263 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1413 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/moe_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3527 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5169 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/param_op_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35882 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/shape_consistency.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11626 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/sharding_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8452 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/testing/
+-rw-r--r--   0 runner    (1001) docker     (127)      865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/comparison.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1406 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/pytest_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      542 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/random.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9201 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      586 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1938 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/model/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3905 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/model/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/
+-rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/rank_recorder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/
+-rw-r--r--   0 runner    (1001) docker     (127)       44 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/tensor_detector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4321 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/timer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/
+-rw-r--r--   0 runner    (1001) docker     (127)      390 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/
+-rw-r--r--   0 runner    (1001) docker     (127)      490 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/
+-rw-r--r--   0 runner    (1001) docker     (127)      342 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25030 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/chunk.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11791 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6291 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/search_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1428 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    42911 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6243 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)      511 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1281 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_monitor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)      858 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/param_runtime_order.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3716 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4126 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9569 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/placement_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4213 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/low_level/
+-rw-r--r--   0 runner    (1001) docker     (127)       88 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7283 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/
+-rw-r--r--   0 runner    (1001) docker     (127)      242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      409 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/base_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4545 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/bucket_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4362 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/gradient_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/parameter_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1494 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/tensor_bucket.py
+-rw-r--r--   0 runner    (1001) docker     (127)    43271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/low_level_optim.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    36765 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    48382 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       69 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      205 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       37 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.277213 colossalai-nightly-2024.5.4/examples/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/examples/language/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      713 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4385 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/performance_evaluator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/base_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4730 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpp_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/cpu_adam/
+-rw-r--r--   0 runner    (1001) docker     (127)      150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_arm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_x86.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/csrc/
+-rw-r--r--   0 runner    (1001) docker     (127)      350 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/arm/
+-rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/colossal_C_frontend.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)      214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/compat.h
+-rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/cuda/include/
+-rw-r--r--   0 runner    (1001) docker     (127)     8585 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/include/block_reduce.h
+-rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    25829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    25627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_adam.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_apply.cuh
+-rw-r--r--   0 runner    (1001) docker     (127)    13279 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    13115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_lamb.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     6479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     2684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    21112 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     2066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    23530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    14851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/type_shim.h
+-rw-r--r--   0 runner    (1001) docker     (127)     6588 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/scaled_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cuda_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/flash_attention/
+-rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3760 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_dao_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_npu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_sdpa_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/layernorm/
+-rw-r--r--   0 runner    (1001) docker     (127)       89 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/layernorm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/layernorm/layernorm_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/moe/moe_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      105 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/optimizer/fused_optimizer_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/softmax/
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/scaled_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/triton_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/requirements/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements-infer.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      557 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements-test.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      205 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     4379 2024-05-04 00:14:37.000000 colossalai-nightly-2024.5.4/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.277213 colossalai-nightly-2024.5.4/tests/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/
+-rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      832 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/hanging_param_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/nested_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/repeated_computed_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1259 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/simple_net.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/
+-rw-r--r--   0 runner    (1001) docker     (127)       25 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2534 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/diffusers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/executor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3413 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/
+-rw-r--r--   0 runner    (1001) docker     (127)       20 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5975 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/timm.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/
+-rw-r--r--   0 runner    (1001) docker     (127)       26 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4614 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/torchaudio.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/
+-rw-r--r--   0 runner    (1001) docker     (127)       24 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3899 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/torchrec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/
+-rw-r--r--   0 runner    (1001) docker     (127)       27 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4970 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/torchvision.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/
+-rw-r--r--   0 runner    (1001) docker     (127)      408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/albert.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2289 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4487 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4251 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6361 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gpt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2837 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3061 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/test_analyzer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_bias_addition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2139 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_mod_dir.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1750 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_nested_ckpt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_shape_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_symbolic_profile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/zoo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_aten.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_flop_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_meta_mode.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_node_converting_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2702 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2003 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2446 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3383 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3733 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3674 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10992 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7729 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2052 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.425213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11120 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7521 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4701 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6548 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6034 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12574 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11426 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2942 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12825 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8338 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2556 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18264 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3056 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4168 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8698 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12216 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12137 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2781 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3625 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13847 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3556 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8505 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5283 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/tests/test_shardformer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5683 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_flash_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12502 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8340 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3196 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7723 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8331 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8635 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7743 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12103 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5625 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2557 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6528 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7293 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_whisper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      833 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2623 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_with_torch_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)        6 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/version.txt
```

### Comparing `colossalai-nightly-2024.5.25/LICENSE` & `colossalai-nightly-2024.5.4/LICENSE`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/PKG-INFO` & `colossalai-nightly-2024.5.4/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: colossalai-nightly
-Version: 2024.5.25
+Version: 2024.5.4
 Summary: An integrated large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org
 License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io
@@ -32,15 +32,14 @@
         
         
            | [English](README.md) | [中文](docs/README-zh-Hans.md) |
         
         </div>
         
         ## Latest News
-        * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)
         * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
         * [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
         * [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
         * [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
         * [2024/03] [Open-Sora：Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
         * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
         * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
@@ -83,17 +82,19 @@
              <li><a href="#GPT-2-Single">GPT-2</a></li>
              <li><a href="#PaLM-Single">PaLM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Inference">Inference</a>
            <ul>
-             <li><a href="#Colossal-Inference">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>
              <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
              <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
+             <li><a href="#GPT-3-Inference">GPT-3</a></li>
+             <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
+             <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Installation">Installation</a>
            <ul>
              <li><a href="#PyPI">PyPI</a></li>
              <li><a href="#Install-From-Source">Install From Source</a></li>
@@ -383,52 +384,56 @@
         
         - 34x larger model size on the same hardware
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         
         ## Inference
-        ### Colossal-Inference
-        <p align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png" width=1000/>
-        </p>
-        
-        <p align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png" width=1000/>
-        </p>
-        
-         - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.
-        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)
-        [[blog]](https://hpc-ai.com/blog/colossal-inference)
-        
         ### Grok-1
         <p id="Grok-1" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
         </p>
         
          - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
         
         [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
         [[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
         [[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
         [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
         
-        ### SwiftInfer
         <p id="SwiftInfer" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
         </p>
         
         - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
         
+        <p id="GPT-3-Inference" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference_GPT-3.jpg" width=800/>
+        </p>
+        
+        - [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference acceleration on the same hardware
+        
+        <p id="OPT-Serving" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20serving.png" width=600/>
+        </p>
+        
+        - [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service): Try 175-billion-parameter OPT online services
+        
+        <p id="BLOOM-Inference" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
+        </p>
+        
+        - [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
+        
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Installation
         
         Requirements:
-        - PyTorch >= 2.1
+        - PyTorch >= 1.11 and PyTorch <= 2.1
         - Python >= 3.7
         - CUDA >= 11.0
         - [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
         - Linux OS
         
         If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.5.25 Summary: An
+Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.5.4 Summary: An
 integrated large-scale model training system with efficient parallelization
 techniques Home-page: https://www.colossalai.org License: Apache Software
 License 2.0 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/
 discussions Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/
 issues Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io Project-URL:
 Github, https://github.com/hpcaitech/ColossalAI Description: # Colossal-AI
@@ -22,47 +22,46 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 å¾®ä¿¡-å å¥-green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | [ä¸­æ]
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-
-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference) *
-[2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-
-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/
-open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-
-and-720p-resolution-in-open-source) * [2024/04] [Most cost-effective solutions
-for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://
-hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-
-pretraining-tailored-to-llama3-series) * [2024/03] [314 Billion Parameter Grok-
-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace
-version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-
-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-
-version-is-here) * [2024/03] [Open-Sora: Revealing Complete Model Parameters,
-Training Details, and Everything for Sora-like Video Generation Models](https:/
-/hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-Soraï¼Sora Replication
-Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million]
-(https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference Performance Improved
-by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round
-Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01]
-[Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI
-Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11]
-[Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More
-Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-
-moe-model-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of
-Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large
-Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://
-www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-
-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-
-free-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2
-Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-
-training) * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding]
-(https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-
-funding-to-fuel-team-expansion-and-business-growth) ## Table of Contents
+## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
+Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
+/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
+video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
+effective solutions for inference, fine-tuning and pretraining, tailored to
+LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
+inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
+[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
+Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
+billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
+use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
+Complete Model Parameters, Training Details, and Everything for Sora-like Video
+Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
+Soraï¼Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to
+Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
+Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
+LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
+SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
+USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
+colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
+Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
+enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
+efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
+Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
+Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
+of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
+large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
+[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
+(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
+Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
+tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
+business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
           o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
             _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
@@ -81,18 +80,20 @@
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
-          o _C_o_l_o_s_s_a_l_-_I_n_f_e_r_e_n_c_e_:_ _L_a_r_g_e_ _A_I_ _M_o_d_e_l_s_ _I_n_f_e_r_e_n_c_e_ _S_p_e_e_d_ _D_o_u_b_l_e_d
           o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
+          o _G_P_T_-_3
+          o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
+          o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
           o _I_n_s_t_a_l_l_ _F_r_o_m_ _S_o_u_r_c_e
     * _U_s_e_ _D_o_c_k_e_r
     * _C_o_m_m_u_n_i_t_y
     * _C_o_n_t_r_i_b_u_t_i_n_g
     * _C_i_t_e_ _U_s
@@ -282,66 +283,72 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference ### Colossal-Inference
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                    inference/colossal-inference-v1-1.png]
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                    inference/colossal-inference-v1-2.png]
-- Large AI models inference speed doubled, compared to the offline inference
-performance of vLLM in some cases. [[code]](https://github.com/hpcaitech/
-ColossalAI/tree/main/colossalai/inference) [[blog]](https://hpc-ai.com/blog/
-colossal-inference) ### Grok-1
+## Inference ### Grok-1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/grok-1-inference.jpg]
 - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
 Python + PyTorch + HuggingFace version for Inference. [[code]](https://
 github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
 (https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
 3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
 [HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
 grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
-models/colossalai/grok-1-pytorch/summary) ### SwiftInfer
+models/colossalai/grok-1-pytorch/summary)
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             inference_GPT-3.jpg]
+- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
+acceleration on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             BLOOM%20serving.png]
+- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
+Try 175-billion-parameter OPT online services
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                            BLOOM%20Inference.PNG]
+- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
+Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
+times.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Installation Requirements: - PyTorch >= 2.1 - Python >= 3.7 - CUDA >= 11.0 -
-[NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0
-(V100/RTX20 and higher) - Linux OS If you encounter any problem with
-installation, you may want to raise an [issue](https://github.com/hpcaitech/
-ColossalAI/issues/new/choose) in this repository. ### Install from PyPI You can
-easily install Colossal-AI with the following command. **By default, we do not
-build PyTorch extensions during installation.** ```bash pip install colossalai
-``` **Note: only Linux is supported for now.** However, if you want to build
-the PyTorch extensions during installation, you can set `BUILD_EXT=1`. ```bash
-BUILD_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
-during runtime when you actually need them.** We also keep releasing the
-nightly version to PyPI every week. This allows you to access the unreleased
-features and bug fixes in the main branch. Installation can be made via ```bash
-pip install colossalai-nightly ``` ### Download From Source > The version of
-Colossal-AI will be in line with the main branch of the repository. Feel free
-to raise an issue if you encounter any problems. :) ```shell git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
-install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
-build them during runtime. If you want to install and enable CUDA kernel fusion
-(compulsory installation when using fused optimizer): ```shell BUILD_EXT=1 pip
-install . ``` For Users with CUDA 10.2, you can still build ColossalAI from
-source. However, you need to manually download the cub library and copy it to
-the corresponding directory. ```bash # clone the repository git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # download the cub library
-wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip
-cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ #
-install BUILD_EXT=1 pip install . ```
+## Installation Requirements: - PyTorch >= 1.11 and PyTorch <= 2.1 - Python >=
+3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
+developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
+you encounter any problem with installation, you may want to raise an [issue]
+(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+### Install from PyPI You can easily install Colossal-AI with the following
+command. **By default, we do not build PyTorch extensions during
+installation.** ```bash pip install colossalai ``` **Note: only Linux is
+supported for now.** However, if you want to build the PyTorch extensions
+during installation, you can set `BUILD_EXT=1`. ```bash BUILD_EXT=1 pip install
+colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
+actually need them.** We also keep releasing the nightly version to PyPI every
+week. This allows you to access the unreleased features and bug fixes in the
+main branch. Installation can be made via ```bash pip install colossalai-
+nightly ``` ### Download From Source > The version of Colossal-AI will be in
+line with the main branch of the repository. Feel free to raise an issue if you
+encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
+ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
+we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
+If you want to install and enable CUDA kernel fusion (compulsory installation
+when using fused optimizer): ```shell BUILD_EXT=1 pip install . ``` For Users
+with CUDA 10.2, you can still build ColossalAI from source. However, you need
+to manually download the cub library and copy it to the corresponding
+directory. ```bash # clone the repository git clone https://github.com/
+hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
+github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
+1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
+BUILD_EXT=1 pip install . ```
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
```

### Comparing `colossalai-nightly-2024.5.25/README.md` & `colossalai-nightly-2024.5.4/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -21,15 +21,14 @@
 
 
    | [English](README.md) | [中文](docs/README-zh-Hans.md) |
 
 </div>
 
 ## Latest News
-* [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)
 * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
 * [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
 * [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
 * [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
 * [2024/03] [Open-Sora：Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
 * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
 * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
@@ -72,17 +71,19 @@
      <li><a href="#GPT-2-Single">GPT-2</a></li>
      <li><a href="#PaLM-Single">PaLM</a></li>
    </ul>
  </li>
  <li>
    <a href="#Inference">Inference</a>
    <ul>
-     <li><a href="#Colossal-Inference">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>
      <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
      <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
+     <li><a href="#GPT-3-Inference">GPT-3</a></li>
+     <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
+     <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
    </ul>
  </li>
  <li>
    <a href="#Installation">Installation</a>
    <ul>
      <li><a href="#PyPI">PyPI</a></li>
      <li><a href="#Install-From-Source">Install From Source</a></li>
@@ -372,52 +373,56 @@
 
 - 34x larger model size on the same hardware
 
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 
 ## Inference
-### Colossal-Inference
-<p align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png" width=1000/>
-</p>
-
-<p align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png" width=1000/>
-</p>
-
- - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.
-[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)
-[[blog]](https://hpc-ai.com/blog/colossal-inference)
-
 ### Grok-1
 <p id="Grok-1" align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
 </p>
 
  - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
 
 [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
 [[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
 [[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
 [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
 
-### SwiftInfer
 <p id="SwiftInfer" align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
 </p>
 
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
 
+<p id="GPT-3-Inference" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference_GPT-3.jpg" width=800/>
+</p>
+
+- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference acceleration on the same hardware
+
+<p id="OPT-Serving" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20serving.png" width=600/>
+</p>
+
+- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service): Try 175-billion-parameter OPT online services
+
+<p id="BLOOM-Inference" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
+</p>
+
+- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
+
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 ## Installation
 
 Requirements:
-- PyTorch >= 2.1
+- PyTorch >= 1.11 and PyTorch <= 2.1
 - Python >= 3.7
 - CUDA >= 11.0
 - [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
 - Linux OS
 
 If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
```

#### html2text {}

```diff
@@ -15,47 +15,46 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 å¾®ä¿¡-å å¥-green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | [ä¸­æ]
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-
-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference) *
-[2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-
-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/
-open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-
-and-720p-resolution-in-open-source) * [2024/04] [Most cost-effective solutions
-for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://
-hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-
-pretraining-tailored-to-llama3-series) * [2024/03] [314 Billion Parameter Grok-
-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace
-version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-
-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-
-version-is-here) * [2024/03] [Open-Sora: Revealing Complete Model Parameters,
-Training Details, and Everything for Sora-like Video Generation Models](https:/
-/hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-Soraï¼Sora Replication
-Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million]
-(https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference Performance Improved
-by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round
-Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01]
-[Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI
-Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11]
-[Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More
-Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-
-moe-model-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of
-Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large
-Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://
-www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-
-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-
-free-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2
-Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-
-training) * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding]
-(https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-
-funding-to-fuel-team-expansion-and-business-growth) ## Table of Contents
+## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
+Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
+/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
+video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
+effective solutions for inference, fine-tuning and pretraining, tailored to
+LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
+inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
+[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
+Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
+billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
+use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
+Complete Model Parameters, Training Details, and Everything for Sora-like Video
+Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
+Soraï¼Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to
+Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
+Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
+LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
+SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
+USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
+colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
+Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
+enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
+efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
+Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
+Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
+of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
+large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
+[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
+(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
+Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
+tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
+business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
           o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
             _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
@@ -74,18 +73,20 @@
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
-          o _C_o_l_o_s_s_a_l_-_I_n_f_e_r_e_n_c_e_:_ _L_a_r_g_e_ _A_I_ _M_o_d_e_l_s_ _I_n_f_e_r_e_n_c_e_ _S_p_e_e_d_ _D_o_u_b_l_e_d
           o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
+          o _G_P_T_-_3
+          o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
+          o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
           o _I_n_s_t_a_l_l_ _F_r_o_m_ _S_o_u_r_c_e
     * _U_s_e_ _D_o_c_k_e_r
     * _C_o_m_m_u_n_i_t_y
     * _C_o_n_t_r_i_b_u_t_i_n_g
     * _C_i_t_e_ _U_s
@@ -275,66 +276,72 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference ### Colossal-Inference
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                    inference/colossal-inference-v1-1.png]
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                    inference/colossal-inference-v1-2.png]
-- Large AI models inference speed doubled, compared to the offline inference
-performance of vLLM in some cases. [[code]](https://github.com/hpcaitech/
-ColossalAI/tree/main/colossalai/inference) [[blog]](https://hpc-ai.com/blog/
-colossal-inference) ### Grok-1
+## Inference ### Grok-1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/grok-1-inference.jpg]
 - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
 Python + PyTorch + HuggingFace version for Inference. [[code]](https://
 github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
 (https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
 3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
 [HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
 grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
-models/colossalai/grok-1-pytorch/summary) ### SwiftInfer
+models/colossalai/grok-1-pytorch/summary)
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             inference_GPT-3.jpg]
+- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
+acceleration on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             BLOOM%20serving.png]
+- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
+Try 175-billion-parameter OPT online services
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                            BLOOM%20Inference.PNG]
+- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
+Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
+times.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Installation Requirements: - PyTorch >= 2.1 - Python >= 3.7 - CUDA >= 11.0 -
-[NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0
-(V100/RTX20 and higher) - Linux OS If you encounter any problem with
-installation, you may want to raise an [issue](https://github.com/hpcaitech/
-ColossalAI/issues/new/choose) in this repository. ### Install from PyPI You can
-easily install Colossal-AI with the following command. **By default, we do not
-build PyTorch extensions during installation.** ```bash pip install colossalai
-``` **Note: only Linux is supported for now.** However, if you want to build
-the PyTorch extensions during installation, you can set `BUILD_EXT=1`. ```bash
-BUILD_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
-during runtime when you actually need them.** We also keep releasing the
-nightly version to PyPI every week. This allows you to access the unreleased
-features and bug fixes in the main branch. Installation can be made via ```bash
-pip install colossalai-nightly ``` ### Download From Source > The version of
-Colossal-AI will be in line with the main branch of the repository. Feel free
-to raise an issue if you encounter any problems. :) ```shell git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
-install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
-build them during runtime. If you want to install and enable CUDA kernel fusion
-(compulsory installation when using fused optimizer): ```shell BUILD_EXT=1 pip
-install . ``` For Users with CUDA 10.2, you can still build ColossalAI from
-source. However, you need to manually download the cub library and copy it to
-the corresponding directory. ```bash # clone the repository git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # download the cub library
-wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip
-cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ #
-install BUILD_EXT=1 pip install . ```
+## Installation Requirements: - PyTorch >= 1.11 and PyTorch <= 2.1 - Python >=
+3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
+developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
+you encounter any problem with installation, you may want to raise an [issue]
+(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+### Install from PyPI You can easily install Colossal-AI with the following
+command. **By default, we do not build PyTorch extensions during
+installation.** ```bash pip install colossalai ``` **Note: only Linux is
+supported for now.** However, if you want to build the PyTorch extensions
+during installation, you can set `BUILD_EXT=1`. ```bash BUILD_EXT=1 pip install
+colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
+actually need them.** We also keep releasing the nightly version to PyPI every
+week. This allows you to access the unreleased features and bug fixes in the
+main branch. Installation can be made via ```bash pip install colossalai-
+nightly ``` ### Download From Source > The version of Colossal-AI will be in
+line with the main branch of the repository. Feel free to raise an issue if you
+encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
+ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
+we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
+If you want to install and enable CUDA kernel fusion (compulsory installation
+when using fused optimizer): ```shell BUILD_EXT=1 pip install . ``` For Users
+with CUDA 10.2, you can still build ColossalAI from source. However, you need
+to manually download the cub library and copy it to the corresponding
+directory. ```bash # clone the repository git clone https://github.com/
+hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
+github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
+1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
+BUILD_EXT=1 pip install . ```
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/_meta_registration.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_meta_registration.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/_monkey_patch.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_monkey_patch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/flop_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/flop_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/_subclasses/meta_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/meta_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/codegen.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/codegen.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/graph_module.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/graph_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/node_util.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/node_util.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/passes/graph_profile.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/graph_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/passes/shape_prop.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/shape_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/symbolic_profile.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/symbolic_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/bias_addition.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/bias_addition.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/custom_leaf_module.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/custom_leaf_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/proxy.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/proxy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/symbolic_trace.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/symbolic_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/_analyzer/fx/tracer/tracer.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/accelerator/api.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/api.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/accelerator/base_accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/base_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/accelerator/cpu_accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/cpu_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/accelerator/cuda_accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/cuda_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/accelerator/npu_accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/npu_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_mixin/base.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/amp/naive_amp/mixed_precision_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/checkpoint/operation.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/meta_registry/where.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/where.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/registry.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/meta_profiler/shard_metainfo.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/shard_metainfo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/amp_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/amp_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/base_offload_module.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/base_offload_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/mem_optimize.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/mem_optimize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/region.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/region_manager.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/runtime.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/runtime.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/solver.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/solver.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/training_simulator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/training_simulator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/offload/util.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/util.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/comm_metainfo_pass.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/comm_metainfo_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/meta_info_prop.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/meta_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/runtime_apply_pass.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_apply_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/passes/runtime_preparation_pass.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_preparation_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/constants.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/initialize.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/initialize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/registry.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/options.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/options.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/sharding_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/sharding_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/solver.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/solver.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/broadcast.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/broadcast.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/factory.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/factory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/misc.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/misc.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/reshape.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/reshape.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/auto_parallel/tensor_shard/utils/sharding.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/sharding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/booster/accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/booster.py` & `colossalai-nightly-2024.5.4/colossalai/booster/booster.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/fp16_apex.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_apex.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/fp16_naive.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_naive.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/fp16_torch.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_torch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/mixed_precision/mixed_precision_base.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/mixed_precision_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/dp_plugin_base.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/dp_plugin_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/gemini_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/gemini_plugin.py`

 * *Files 1% similar despite different names*

```diff
@@ -357,15 +357,14 @@
         extra_dp_size: int = 1,
         enable_all_optimization: bool = False,
         enable_fused_normalization: bool = False,
         enable_flash_attention: bool = False,
         enable_sequence_parallelism: bool = False,
         enable_jit_fused: bool = False,
         enable_sequence_overlap: bool = False,
-        enable_async_reduce: bool = True,
         verbose: bool = False,
     ) -> None:
         super().__init__()
         assert precision in SUPPORTED_PRECISION, f"precision {precision} is not supported"
         if get_accelerator().name == "npu":
             assert placement_policy == "static", "NPU only supports static placement policy"
         self.gemini_config = dict(
@@ -383,15 +382,14 @@
             strict_ddp_mode=strict_ddp_mode,
             search_range_m=search_range_m,
             hidden_dim=hidden_dim,
             min_chunk_size_m=min_chunk_size_m,
             memstats=memstats,
             mixed_precision=PRECISION_STR_TO_DTYPE[precision],
             master_weights=master_weights,
-            enable_async_reduce=enable_async_reduce,
         )
         self.zero_optim_config = dict(
             gpu_margin_mem_ratio=gpu_margin_mem_ratio,
         )
         self.optim_kwargs = dict(
             initial_scale=initial_scale,
             growth_factor=growth_factor,
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/hybrid_parallel_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/hybrid_parallel_plugin.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,11 @@
 import ctypes
 import random
 import warnings
-from collections import defaultdict
 from contextlib import contextmanager
-from copy import deepcopy
 from functools import partial
 from types import MethodType
 from typing import Any, Callable, Dict, Iterator, List, Optional, OrderedDict, Tuple, Union
 
 import numpy as np
 import torch
 import torch.distributed as dist
@@ -22,16 +20,14 @@
 from torch.utils.data.distributed import DistributedSampler
 
 from colossalai.accelerator import get_accelerator
 from colossalai.amp.naive_amp.mixed_precision_optimizer import MixedPrecisionOptimizer
 from colossalai.checkpoint_io import CheckpointIO, HybridParallelCheckpointIO
 from colossalai.cluster import ProcessGroupMesh
 from colossalai.interface import AMPModelMixin, ModelWrapper, OptimizerWrapper
-from colossalai.interface.optimizer import DistributedOptim
-from colossalai.nn.optimizer import DistGaloreAwamW, cast_to_distributed
 from colossalai.pipeline.schedule import InterleavedSchedule, OneForwardOneBackwardSchedule
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer import GradientCheckpointConfig, ShardConfig, ShardFormer
 from colossalai.shardformer.layer.utils import SeqParallelUtils
 from colossalai.shardformer.policies.base_policy import Policy
 from colossalai.tensor.d_tensor.api import is_distributed_tensor
 from colossalai.zero.low_level import LowLevelZeroOptimizer
@@ -735,15 +731,15 @@
                 return grads_to_sync
             else:
                 return None
 
         # Get all working gradients and gradients to be synchronized.
         all_working_grads = _get_all_working_grads()
         grads_to_sync = _get_grads_to_sync(all_working_grads)
-        if self._grad_store.require_grad_sync and grads_to_sync is not None:
+        if self.require_grad_sync and grads_to_sync is not None:
             # Synchronize sequence parallelism gradients if required.
             SeqParallelUtils.allreduce_partial_data_grad(process_group=self.tp_pg, grads=grads_to_sync)
         else:
             return
 
     def backward(self, loss, retain_graph=False):
         """
@@ -759,15 +755,15 @@
 
         Returns:
             None
         """
         # Call the superclass backward method to compute gradients.
         super().backward(loss, retain_graph)
 
-        if self._grad_store.require_grad_sync and self.model.shard_config.enable_sequence_parallelism:
+        if self.require_grad_sync and self.model.shard_config.enable_sequence_parallelism:
             # If gradient synchronization is required, sync sequence parallelism gradients.
             self._sync_sp_grads()
         else:
             # If gradient synchronization is is not required, return.
             return
 
     def backward_by_grad(self, tensor, grad):
@@ -784,15 +780,15 @@
 
         Returns:
             None
         """
         # Call the superclass backward_by_grad method to compute gradients.
         super().backward_by_grad(tensor, grad)
 
-        if self._grad_store.require_grad_sync and self.model.shard_config.enable_sequence_parallelism:
+        if self.require_grad_sync and self.model.shard_config.enable_sequence_parallelism:
             # If gradient synchronization is required, sync sequence parallelism gradients.
             self._sync_sp_grads()
         else:
             # If gradient synchronization is is not required, return.
             return
 
     def _compute_grad_norm(self, gradients: List[Tensor], norm_type: int = 2) -> float:
@@ -1171,27 +1167,14 @@
         model: Module,
         optimizer: Optional[Optimizer] = None,
         criterion: Optional[Callable] = None,
         dataloader: Optional[DataLoader] = None,
         lr_scheduler: Optional[LRScheduler] = None,
     ) -> Tuple[Module, OptimizerWrapper, Callable, DataLoader, LRScheduler]:
         param_info = get_param_info(optimizer)
-
-        # TODO: Support Galore + ZeRO
-        zero_stage = self.zero_stage
-        zero_config = deepcopy(self.zero_config)
-
-        # Replace with distributed implementation if exists
-        optimizer = cast_to_distributed(optimizer)
-
-        if isinstance(optimizer, DistGaloreAwamW) and zero_stage > 0 and self.dp_size > 0:
-            warnings.warn("Galore is only supported for Tensor Parallel and vanilla Data Parallel yet. Disabling ZeRO.")
-            zero_config["partition_grad"] = False
-            zero_stage = 0
-
         if not isinstance(model, ModelWrapper):
             use_ddp = (self.dp_size > 1 and self.pp_size == 1 and self.zero_stage == 0) or (
                 self.dp_size == 1
                 and self.pp_size == 1
                 and self.enable_sequence_parallelism
                 and self.sequence_parallelism_mode == "all_to_all"
             )
@@ -1207,16 +1190,15 @@
                 tp_group=self.tp_group,
                 sp_group=self.sp_group,
                 use_ddp=use_ddp,
                 ddp_config=self.ddp_config,
                 custom_policy=self.custom_policy,
             )
         if optimizer is not None and not isinstance(optimizer, OptimizerWrapper):
-            if zero_stage == 0:
-                is_zero = False
+            if self.zero_stage == 0:
                 if self.precision in ["fp16", "bf16"]:
                     optimizer = HybridParallelAMPOptimizer(
                         optimizer,
                         model,
                         use_pipeline=self.enable_pipeline_parallelism,
                         param_info=param_info,
                         precision=self.precision,
@@ -1232,45 +1214,37 @@
                         use_pipeline=self.enable_pipeline_parallelism,
                         param_info=param_info,
                         max_norm=self.max_norm,
                         pp_process_group=self.pp_group,
                         tp_process_group=self.tp_group,
                     )
             else:
-                is_zero = self.dp_size > 1
-                if self.dp_size == 1:
+                zero_dp_size = dist.get_world_size(dp_group)
+                if zero_dp_size == 1:
                     warnings.warn(
                         "Use Zero Optimizer when data parallel size is 1 may introduce unnecessary overhead. "
-                        "If you do not intend to use cpu_offload, please consider set zero_stage=0."
+                        "If you are not intended to use cpu_offload, please consider set zero_stage=0."
                     )
 
                 assert self.precision != "fp32", "Please set precision to 'fp16' or 'bf16' when using ZeRO."
                 optimizer = HybridParallelZeroOptimizer(
                     optimizer,
                     model,
                     use_pipeline=self.enable_pipeline_parallelism,
                     param_info=param_info,
                     dp_process_group=dp_group,
                     tp_process_group=self.tp_group,
                     pp_process_group=self.pp_group,
                     verbose=True,
                     clip_grad_norm=self.max_norm,
-                    **zero_config,
+                    **self.zero_config,
                     **self.amp_config,
                 )
             # inject update_master_params
             model.update_master_params = MethodType(optimizer.update_master_params, model)
-
-            # Setup optimizers that require global states
-            optim = optimizer.optim
-            if isinstance(optim, DistributedOptim):
-                shard_to_param = optimizer.get_master_to_working_map() if is_zero else {}
-                padding_map = optimizer.get_param_padding_map() if is_zero else defaultdict(int)
-                optim.setup_distributed(self.tp_group, self.dp_group, shard_to_param, padding_map, is_zero)
-
         return model, optimizer, criterion, dataloader, lr_scheduler
 
     def execute_pipeline(
         self,
         data_iter: Iterator,
         model: HybridParallelModule,
         criterion: Callable[[Any, Any], torch.Tensor],
@@ -1294,15 +1268,15 @@
         with ctx:
             outputs = self.schedule.forward_backward_step(
                 model, data_iter, criterion, optimizer, return_loss, return_outputs
             )
 
         # run with gradients accumulation
         if model.require_grad_sync == False or (
-            isinstance(optimizer, HybridParallelZeroOptimizer) and optimizer._grad_store.require_grad_sync == False
+            isinstance(optimizer, HybridParallelZeroOptimizer) and optimizer.require_grad_sync == False
         ):
             return outputs
 
         # Synchronize the grads of shared parameters of the model.
         model.sync_shared_params()
         # Synchronize sequence parallelism gradients of the model.
         model.sync_sp_grads()
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/low_level_zero_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/low_level_zero_plugin.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,18 +4,15 @@
 import warnings
 from functools import partial
 from pathlib import Path
 from types import MethodType
 from typing import Callable, Dict, Iterator, List, Optional, Tuple
 
 import torch
-import torch.distributed
-import torch.distributed as dist
 import torch.nn as nn
-from torch.distributed.distributed_c10d import _get_default_group
 from torch.nn import Parameter
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils._pytree import tree_map
 from torch.utils.data import DataLoader
 
 from colossalai.accelerator import get_accelerator
@@ -27,16 +24,14 @@
     load_shard_state_dict,
     load_states_into_optimizer,
     save_param_groups,
     save_state_dict,
     sharded_optimizer_loading_epilogue,
 )
 from colossalai.interface import AMPModelMixin, ModelWrapper, OptimizerWrapper
-from colossalai.interface.optimizer import DistributedOptim
-from colossalai.nn.optimizer import DistGaloreAwamW, cast_to_distributed
 from colossalai.quantization import BnbQuantizationConfig, quantize_model
 from colossalai.zero import LowLevelZeroOptimizer
 
 from .dp_plugin_base import DPPluginBase
 from .torch_ddp_plugin import TorchDDPCheckpointIO
 
 __all__ = ["LowLevelZeroPlugin"]
@@ -429,43 +424,21 @@
             ), "The model should have been wrapped as a PeftModel when self.lora_enabled is True"
             if optimizer is not None:
                 self.add_lora_params_to_optimizer(model, optimizer)
 
         if not isinstance(model, ModelWrapper):
             model = LowLevelZeroModel(model, self.precision)
 
-        # TODO: Support Galore + ZeRO
-        zero_stage = self.stage
-        zero_optim_kwargs = {**self.zero_optim_kwargs}
-        dp_size = dist.get_world_size()
-
-        # Replace with the distributed implementation if exists
-        optimizer = cast_to_distributed(optimizer)
-
-        if isinstance(optimizer, DistGaloreAwamW) and zero_stage > 0 and dp_size > 0:
-            warnings.warn("Galore is only supported for Tensor Parallel and vanilla Data Parallel yet. Disabling ZeRO.")
-            zero_optim_kwargs["partition_grad"] = False
-            zero_stage = 0
-
         if optimizer is not None and not isinstance(optimizer, OptimizerWrapper):
             optimizer: LowLevelZeroOptimizer = LowLevelZeroOptimizer(
-                optimizer, **zero_optim_kwargs, verbose=self.verbose
+                optimizer, **self.zero_optim_kwargs, verbose=self.verbose
             )
             # inject update_master_params
             model.update_master_params = MethodType(optimizer.update_master_params, model)
 
-            # Setup optimizers that require global states
-            optim = optimizer.optim
-            is_zero = dp_size > 1 and zero_stage > 0
-            dp_group = _get_default_group()  # Use the whole world
-            if isinstance(optim, DistributedOptim):
-                shard_to_param = optimizer.get_master_to_working_map()
-                padding_map = optimizer.get_param_padding_map()
-                optim.setup_distributed(None, dp_group, shard_to_param, padding_map, is_zero)
-
         return model, optimizer, criterion, dataloader, lr_scheduler
 
     def control_checkpoint_io(self) -> bool:
         return True
 
     def get_checkpoint_io(self) -> CheckpointIO:
         return LowLevelZeroCheckpointIO()
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/plugin_base.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/plugin_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/pp_plugin_base.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/pp_plugin_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/torch_ddp_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_ddp_plugin.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/booster/plugin/torch_fsdp_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_fsdp_plugin.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/checkpoint_io/checkpoint_io_base.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/checkpoint_io_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/checkpoint_io/general_checkpoint_io.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/general_checkpoint_io.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/checkpoint_io/index_file.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/index_file.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/checkpoint_io/utils.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cli/check/check_installation.py` & `colossalai-nightly-2024.5.4/colossalai/cli/check/check_installation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cli/launcher/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/cli/launcher/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cli/launcher/hostinfo.py` & `colossalai-nightly-2024.5.4/colossalai/cli/launcher/hostinfo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cli/launcher/multinode_runner.py` & `colossalai-nightly-2024.5.4/colossalai/cli/launcher/multinode_runner.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cli/launcher/run.py` & `colossalai-nightly-2024.5.4/colossalai/cli/launcher/run.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cluster/device_mesh_manager.py` & `colossalai-nightly-2024.5.4/colossalai/cluster/device_mesh_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cluster/dist_coordinator.py` & `colossalai-nightly-2024.5.4/colossalai/cluster/dist_coordinator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cluster/process_group_manager.py` & `colossalai-nightly-2024.5.4/colossalai/cluster/process_group_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/cluster/process_group_mesh.py` & `colossalai-nightly-2024.5.4/colossalai/cluster/process_group_mesh.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,20 +34,15 @@
     Attributes:
         shape (Tuple[int, ...]): The shape of the process group mesh.
         rank (int): The rank of the current process.
     """
 
     def __init__(self, *size: int) -> None:
         assert dist.is_initialized(), "Please initialize torch.distributed first."
-        world_size = dist.get_world_size()
-        prod_size = prod(size)
-        assert (
-            prod_size == world_size
-        ), f"The product of the size({prod_size}) must be equal to the world size({world_size})."
-
+        assert prod(size) == dist.get_world_size(), "The product of the size must be equal to the world size."
         self._shape = size
         self._rank = dist.get_rank()
         self._coord = ProcessGroupMesh.unravel(self._rank, self._shape)
         self._ranks_to_group: Dict[Tuple[int, ...], ProcessGroup] = {}
         self._group_to_ranks: Dict[ProcessGroup, Tuple[int, ...]] = {}
 
     def destroy_mesh_process_groups(self):
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/context/config.py` & `colossalai-nightly-2024.5.4/colossalai/context/config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/context/singleton_meta.py` & `colossalai-nightly-2024.5.4/colossalai/context/singleton_meta.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/device/alpha_beta_profiler.py` & `colossalai-nightly-2024.5.4/colossalai/device/alpha_beta_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/device/calc_pipeline_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/device/calc_pipeline_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/device/device_mesh.py` & `colossalai-nightly-2024.5.4/colossalai/device/device_mesh.py`

 * *Files 0% similar despite different names*

```diff
@@ -302,16 +302,17 @@
             mapping (Dict): a dictionary that maps the global rank to the local rank in the logical device mesh.
                 The value is a list of integers and each integer represents the local rank in the indexed axis.
         """
         for index, inner_tensor in enumerate(tensor):
             # index means the local rank in the current axis
             # inner_tensor refers to the processes with the same local rank
 
-            if inner_tensor.dim() == 0:
-                # if the inner_tensor already reaches the last axis,
+            if inner_tensor.numel() == 1:
+                # if the inner_tensor only has one element, it means that
+                # it already reaches the last axis
                 # we append its local_rank in the last axis to the index_list
                 # and assign to the mapping
                 # the value of the mapping is the the local rank at the indexed axis of the device mesh
                 mapping[int(inner_tensor)] = index_list + [index]
             else:
                 # we recursively go into the function until we reach the last axis
                 # meanwhile, we should add the local rank in the current axis in the index_list
@@ -454,15 +455,14 @@
                     processes_in_the_same_process_group[dim] = []
 
                 # get the local rank corresponding to the global rank
                 process_coordinates = self._global_to_local_rank_mapping[global_rank].copy()
 
                 # replace the local rank in the given dimension with the
                 # local rank of the current process iterated
-
                 process_coordinates[dim] = _local_rank
                 processes_in_the_same_process_group[dim].append(process_coordinates)
 
         # =================================================================
         # Step 2
         # Use local rank combination to find its corresponding global rank
         # =================================================================
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/_compatibility.py` & `colossalai-nightly-2024.5.4/colossalai/fx/_compatibility.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/_meta_regist_12.py` & `colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_12.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/_meta_regist_13.py` & `colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_13.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/codegen/activation_checkpoint_codegen.py` & `colossalai-nightly-2024.5.4/colossalai/fx/codegen/activation_checkpoint_codegen.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/graph_module.py` & `colossalai-nightly-2024.5.4/colossalai/fx/graph_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/passes/adding_split_node_pass.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/adding_split_node_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/passes/concrete_info_prop.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/concrete_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/passes/meta_info_prop.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/meta_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/passes/passes_for_gpt2_test.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/passes_for_gpt2_test.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/passes/shard_1d_pass.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/shard_1d_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/passes/split_module.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/split_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/passes/utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/constants.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/dataflow.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/dataflow.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/constants.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/activation_function.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/activation_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/pooling.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/activation_function.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/activation_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/attention.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/attention.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/convolution.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/pooling.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/profiler_module/rnn.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/rnn.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/registry.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/experimental/shard_utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/memory_utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/memory_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/opcount.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/opcount.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/profiler.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/shard_utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/profiler/tensor.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/proxy.py` & `colossalai-nightly-2024.5.4/colossalai/fx/proxy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/_meta_trace.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_meta_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/_symbolic_trace.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_symbolic_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/_tracer_utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_tracer_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/experimental.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/experimental.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/convolution.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/convolution.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/pooling.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/meta_patch/patched_module/rnn.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/rnn.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/registry.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/fx/tracer/tracer.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/inference/config.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/layers.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,342 +1,360 @@
-"""
-Our config contains various options for inference optimization, it is a unified API that wraps all the configurations for inference.
-"""
-import logging
-from abc import ABC, abstractmethod
-from dataclasses import dataclass, fields
-from typing import Any, Dict, List, Optional, Union
+import math
+from typing import Callable
 
 import torch
-from transformers.generation import GenerationConfig
+import torch.nn.functional as F
+from torch import Tensor
+from torch import nn as nn
+from torch.nn.parameter import Parameter
+
+from colossalai.accelerator import get_accelerator
+from colossalai.legacy.context import seed
+from colossalai.legacy.registry import LAYERS
+from colossalai.nn import init as init
+
+from ..utils import to_2tuple
+
+
+def drop_path(x, drop_prob: float = 0.0, training: bool = False):
+    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
+
+    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
+    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
+    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
+    'survival rate' as the argument.
 
-from colossalai.inference.flash_decoding_utils import FDIntermTensors
+    Args:
+        drop_prob (float, optional): probability of dropping path, defaults 0.0.
+        training (bool, optional): whether in training progress, defaults False.
+    """
+    if drop_prob == 0.0 or not training:
+        return x
+    keep_prob = 1 - drop_prob
+    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
+    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
+    random_tensor.floor_()  # binarize
+    output = x.div(keep_prob) * random_tensor
+    return output
 
-GibiByte = 1024**3
 
-logger = logging.Logger(__name__)
+class DropPath(nn.Module):
+    """
+    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
+    Adapted from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py
 
-_DTYPE_MAPPING = {
-    "fp16": torch.float16,
-    "bf16": torch.bfloat16,
-    "fp32": torch.float32,
-}
+    Args:
+        drop_prob (float, optional): probability of dropping path, defaults None.
+    """
 
-_ALLOWED_DTYPES = [torch.float16, torch.bfloat16, torch.float32]
+    def __init__(self, drop_prob=None):
+        super(DropPath, self).__init__()
+        self.drop_prob = drop_prob
+
+    def forward(self, x):
+        return drop_path(x, self.drop_prob, self.training)
 
-_DEFAULT_PROMPT_TEMPLATES = {
-    "llama": "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{input_text}[/INST]",
-    "baichuan": " <reserved_106> {input_text} <reserved_107> ",
-    "vicuna": "A chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user input. USER: {input_text}\nASSISTANT: ",
-}
 
+class WrappedDropout(nn.Module):
+    r"""Same as torch.nn.Dropout. But it is wrapped with the context of seed manager. During training, randomly zeroes
+    some elements of the input tensor with probability p using samples from a Bernoulli distribution. Each
+    channel will be zeroed out independently on every forward call. Furthermore, the outputs are scaled by a factor of
+    1/(1-p) during training. This means that during evaluation the module simply computes an identity function.
 
-class RPC_PARAM(ABC):
+    Args:
+        p (float, optional): probability of an element to be zeroed, defaults 0.5.
+        inplace (bool, optional): whether to do dropout in-place, default to be False.
+        mode (:class:`colossalai.legacy.context.ParallelMode`): The chosen parallel mode.
+
+    Note:
+        The parallel_mode should be concluded in ``ParallelMode``. More details about ``ParallelMode`` could be found
+        in `parallel_mode <https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/context/parallel_mode.py>`_
     """
-    NOTE(lry89757) We use rpyc to transport param between client and server.
-    Rpyc only support the type of `POD` in python as the param, so we should take some smart ways to transport the data like tensor or some sophisticated classes.
-    Drawing on the logic of `__setstate__`, `__getstate__`, we will let some classes(will be rpc param later) inherit this base class, and rewrite the to_rpc_param and from_rpc_param. We will invoke `to_rpc_param` in client to pass the params and recover the param in server side by `from_rpc_param`.
+
+    def __init__(self, p: float = 0.5, inplace: bool = False, mode=None):
+        super().__init__()
+        if p < 0 or p > 1:
+            raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
+        self.p = p
+        self.inplace = inplace
+        if mode is None:
+            self.func = self.nonefunc
+        else:
+            self.func = self.normalfunc
+            self.mode = mode
+
+    def nonefunc(self, inputs):
+        return F.dropout(inputs, self.p, self.training, self.inplace)
+
+    def normalfunc(self, inputs):
+        with seed(self.mode):
+            return F.dropout(inputs, self.p, self.training, self.inplace)
+
+    def forward(self, inputs):
+        return self.func(inputs)
+
+
+class WrappedDropPath(nn.Module):
+    r"""Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
+    Here, it is wrapped with the context of seed manager.
+
+    Args:
+        p (float, optional): probability of dropping path, defaults 0.0.
+        mode (:class:`colossalai.legacy.context.ParallelMode`): The chosen parallel mode.
+
+    Note:
+        The parallel_mode should be concluded in ``ParallelMode``. More details about ``ParallelMode`` could be found
+        in `parallel_mode <https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/context/parallel_mode.py>`_
     """
 
-    @abstractmethod
-    def to_rpc_param(self):
-        return NotImplementedError
-
-    @staticmethod
-    @abstractmethod
-    def from_rpc_param():
-        return NotImplementedError
+    def __init__(self, p: float = 0.0, mode=None):
+        super().__init__()
+        self.p = p
+        self.mode = mode
+        if self.mode is None:
+            self.func = self.nonefunc
+        else:
+            self.func = self.normalfunc
+            self.mode = mode
+
+    def nonefunc(self, inputs):
+        return drop_path(inputs, self.p, self.training)
+
+    def normalfunc(self, inputs):
+        with seed(self.mode):
+            return drop_path(inputs, self.p, self.training)
+
+    def forward(self, inputs):
+        return self.func(inputs)
 
 
-@dataclass
-class InputMetaData(RPC_PARAM):
-    """The input info for a single step
+@LAYERS.register_module
+class VanillaPatchEmbedding(nn.Module):
+    r"""
+    2D Image to Patch Embedding
 
     Args:
-    block_tables (torch.Tensor, optional): Sequences' BlockTables Defaults to None.
-    sequence_lengths (torch.Tensor): A tensor containing sequence lengths.
-    fd_inter_tensor (torch.Tensor, optional): A tensor representing intermediate data for flash decoding. Defaults to None.
-    batch_size (int, optional): The current batch size. Defaults to 64.
-    is_prompts (bool, optional): Indicates whether prefill or decoding. Defaults to False(decoding).
-    use_cuda_kernel(bool): Whether to use cuda kernel, faster but lose some precision occasionally
-    use_cuda_graph (bool, optional): Indicates whether to use the CUDA graph. Defaults to False.
-    kv_seq_len (int, optional): Key-value sequence length. Defaults to 512.
-    head_dim (int, optional): Head dimension. Defaults to 32.
-    high_precision(bool, optional): Whether to use float32 for underlying calculations of float16 data to achieve higher precision, Defaults to False.
-    dtype (torch.dtype, optional): The computation type of tensor, Defaults to torch.float32.
-    use_spec_dec (bool): Indicate whether to use speculative decoding.
-    num_tokens_to_verify (int): The number of tokens to verify in speculative decoding. Only valid when `use_spec_dec` is set to True.
-    batch_token_ids (List[List[int]], optional): input_token_ids + output_token_ids of current batch. Only used for `repetition_penalty`, `no_repeat_ngram_size` in sampler process.
+        img_size (int): image size.
+        patch_size (int): patch size.
+        in_chans (int): number of channels of input image.
+        embed_size (int): size of embedding.
+        dtype (:class:`torch.dtype`, optional): The dtype of parameters, defaults to None.
+        flatten (bool, optional): whether to flatten output tensor, defaults to True.
+        weight_initializer (:class:`typing.Callable`, optional):
+            The initializer of weight, defaults to kaiming uniform initializer.
+        bias_initializer (:class:`typing.Callable`, optional):
+            The initializer of bias, defaults to xavier uniform initializer.
+        position_embed_initializer (:class:`typing.Callable`, optional):
+            The initializer of position embedding, defaults to zeros initializer.
+
+    More details about initializer please refer to
+    `init <https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/init.py>`_.
     """
 
-    block_tables: torch.Tensor = None
-    sequence_lengths: torch.Tensor = None
-    fd_inter_tensor: FDIntermTensors = None
-    batch_size: int = 64  # current_batch_size
-    is_prompts: bool = False
-    use_cuda_kernel: bool = False
-    use_cuda_graph: bool = False
-    kv_seq_len: int = 512
-    head_dim: int = 32
-    high_precision: bool = False
-    dtype: torch.dtype = torch.float32
-    use_spec_dec: bool = False
-    num_tokens_to_verify: int = 0
-    batch_token_ids: Optional[
-        List[List[int]]
-    ] = None  # for `repetition_penalty`, `no_repeat_ngram_size` in sampler process
-
-    def to_rpc_param(self) -> Dict[str, any]:
-        return {
-            "block_tables": self.block_tables.tolist(),
-            "sequence_lengths": self.sequence_lengths.tolist(),
-            "batch_size": self.batch_size,
-            "is_prompts": self.is_prompts,
-            "use_cuda_kernel": self.use_cuda_kernel,
-            "use_cuda_graph": self.use_cuda_graph,
-            "kv_seq_len": self.kv_seq_len,
-            "head_dim": self.head_dim,
-            "high_precision": self.high_precision,
-            "dtype": str(self.dtype).split(".")[-1],
-            "use_spec_dec": self.use_spec_dec,
-            "num_tokens_to_verify": self.num_tokens_to_verify,
-            "batch_token_ids": self.batch_token_ids,
-        }
-
-    @staticmethod
-    def from_rpc_param(rpc_dict: Dict[str, any]) -> "InputMetaData":
-        """
-        We intentionally don't use `dict.get` method to ensure we pass the right rpc param, or program will show error message
-        """
-        from colossalai.accelerator import get_accelerator
-
-        dtype = getattr(torch, rpc_dict["dtype"])
-        return InputMetaData(
-            block_tables=torch.tensor(
-                rpc_dict["block_tables"], dtype=torch.int, device=get_accelerator().get_current_device()
-            ),
-            sequence_lengths=torch.tensor(
-                rpc_dict["sequence_lengths"], dtype=torch.int, device=get_accelerator().get_current_device()
-            ),
-            batch_size=rpc_dict["batch_size"],
-            is_prompts=rpc_dict["is_prompts"],
-            use_cuda_kernel=rpc_dict["use_cuda_kernel"],
-            use_cuda_graph=rpc_dict["use_cuda_graph"],
-            kv_seq_len=rpc_dict["kv_seq_len"],
-            head_dim=rpc_dict["head_dim"],
-            high_precision=rpc_dict["high_precision"],
-            dtype=dtype,
-            use_spec_dec=rpc_dict["use_spec_dec"],
-            num_tokens_to_verify=rpc_dict["num_tokens_to_verify"],
-            batch_token_ids=rpc_dict["batch_token_ids"],
+    def __init__(
+        self,
+        img_size: int,
+        patch_size: int,
+        in_chans: int,
+        embed_size: int,
+        flatten: bool = True,
+        dtype: torch.dtype = None,
+        weight_initializer: Callable = init.kaiming_uniform_(a=math.sqrt(5)),
+        bias_initializer: Callable = init.xavier_uniform_(a=1, scale=1),
+        position_embed_initializer: Callable = init.zeros_(),
+    ):
+        super().__init__()
+        img_size = to_2tuple(img_size)
+        patch_size = to_2tuple(patch_size)
+        self.img_size = img_size
+        self.patch_size = patch_size
+        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
+        self.num_patches = self.grid_size[0] * self.grid_size[1]
+        self.flatten = flatten
+
+        self.weight = nn.Parameter(
+            torch.empty(
+                (embed_size, in_chans, *self.patch_size), device=get_accelerator().get_current_device(), dtype=dtype
+            )
         )
-
-    def __repr__(self) -> str:
-        return (
-            f"InputMetaData(block_tables={self.block_tables}, "
-            f"sequence_lengths={self.sequence_lengths}, "
-            f"fd_inter_tensor={self.fd_inter_tensor}, "
-            f"batch_size={self.batch_size}, "
-            f"is_prompts={self.is_prompts}, "
-            f"use_cuda_kernel={self.use_cuda_kernel}, "
-            f"use_cuda_graph={self.use_cuda_graph}, "
-            f"kv_seq_len={self.kv_seq_len}, "
-            f"use_spec_dec={self.use_spec_dec}, "
-            f"num_tokens_to_verify={self.num_tokens_to_verify})"
+        self.bias = nn.Parameter(torch.empty(embed_size, device=get_accelerator().get_current_device(), dtype=dtype))
+        self.cls_token = nn.Parameter(
+            torch.zeros((1, 1, embed_size), device=get_accelerator().get_current_device(), dtype=dtype)
+        )
+        self.pos_embed = nn.Parameter(
+            torch.zeros(
+                (1, self.num_patches + 1, embed_size), device=get_accelerator().get_current_device(), dtype=dtype
+            )
         )
 
+        self.reset_parameters(weight_initializer, bias_initializer, position_embed_initializer)
+
+    def reset_parameters(self, weight_initializer, bias_initializer, position_embed_initializer):
+        fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(self.weight)
+        weight_initializer(self.weight, fan_in=fan_in, fan_out=fan_out)
+        bias_initializer(self.bias, fan_in=fan_in)
+        position_embed_initializer(self.pos_embed)
+
+    def forward(self, input_: Tensor) -> Tensor:
+        B, C, H, W = input_.shape
+        assert (
+            H == self.img_size[0] and W == self.img_size[1]
+        ), f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
+        output = F.conv2d(input_, self.weight, self.bias, stride=self.patch_size)
+        if self.flatten:
+            output = output.flatten(2).transpose(1, 2)  # BCHW -> BNC
+
+        cls_token = self.cls_token.expand(output.shape[0], -1, -1)
+        output = torch.cat((cls_token, output), dim=1)
+        output = output + self.pos_embed
+        return output
+
 
-@dataclass
-class InferenceConfig(RPC_PARAM):
-    """The inference configuration.
+@LAYERS.register_module
+class VanillaClassifier(nn.Module):
+    r"""Dense linear classifier.
 
     Args:
-        max_batch_size (int): Maximum batch size, defaults to 8.
-        max_output_len (int): Maximum output length, defaults to 256.
-        max_input_len (int): Maximum input length, defaults to 256.
-        dtype (Union[str, torch.dtype]): The data type for weights and activations.
-        kv_cache_dtype (Optional[str]): The data type of kv_cache, defaults to None.
-        prompt_template (Optional[str]): The prompt template for generation, defaults to None.
-        do_sample (bool): Whether to use sampling for generation, defaults to False.
-        beam_width (int): The maximum beam width used to initialize KV Cache, defaults to 1.
-            During generation, the beam width provided as sampling parameter should be less than or equivalent to this value.
-        prefill_ratio (Optional[float]): A controling ratio for prefill and decoding in running list, defaults to 1.2. We will do a step of prefill
-            when the actual value exceeds this ratio.
-        pad_input: Whether to pad all inputs to the max length.
-        early_stopping (Optional[bool]): Whether to stop the generation when all beam hypotheses have finished or not, defaults to False.
-        top_k (Optional[int]): The number of highest probability vocabulary tokens to keep for top-k-filtering, defaults to None.
-        top_p (Optional[float]): The cumulative probability threshold for retaining tokens with a total probability above it, defaults to None.
-        temperature (Optional[float]): Randomness used to control randomization, defaults to 1.0.
-        repetition_penalty (Optional[float]): The parameter that influences the model's treatment of new tokens in relation to their appearance in the prompt and the generated text. Values greater than 1 incentivize the model to introduce new tokens, whereas values less than 1 incentivize token repetition., defaults to 1.0.
-        no_repeat_ngram_size (Optional[int]): If no_repeat_ngram_size > 0, the consecutive tokens of ngram size can only appear once in inference sentences.
-        n_spec_tokens (int): The maximum number of speculating tokens, defaults to None.
-        glimpse_large_kv (bool): Whether to use large KV in drafter model, defaults to False.
-        block_size (int): The number of blocks in a logical block, defaults to 16.
-        tp_size (int): Tensor parallel size, defaults to 1.
-        pp_size (int): Pipeline parallel size, defaults to 1.
-        micro_batch_size (int): the micro batch size, defaults to 1. Only useful when `pp_size` > 1.
-        micro_batch_buffer_size (int): the buffer size for micro batch. Normally, it should be the same as the number of pipeline stages.
-        use_cuda_kernel(bool): Whether to use cuda kernel, faster but lose some precision occasionally
-        use_cuda_graph (bool): Whether to enforce CUDA graph execution. If False, we will disable CUDA graph and always execute the model in eager mode. If True, we will use eager execution in hybrid.
-        max_context_len_to_capture (int): max context len that could be captured by CUDA Graph, per sequence
-        high_precision(Optional[bool]): Whether to use float32 for underlying calculations of float16 data to achieve higher precision, defaults to False.
-        ignore_eos(bool): Whether to ignore the EOS token and continue generating tokens when encountering the EOS token.
+        in_features (int): size of each input sample.
+        num_classes (int): number of classes.
+        weight (:class:`torch.nn.Parameter`, optional): weight of the classifier, defaults to None.
+        dtype (:class:`torch.dtype`, optional): The dtype of parameters, defaults to None.
+        flatten (bool, optional): whether to flatten output tensor, defaults to True.
+        weight_initializer (:class:`typing.Callable`, optional):
+            The initializer of weight, defaults to kaiming uniform initializer.
+        bias_initializer (:class:`typing.Callable`, optional):
+            The initializer of bias, defaults to xavier uniform initializer.
+
+    More details about initializer please refer to
+    `init <https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/init.py>`_.
     """
 
-    # NOTE: arrange configs according to their importance and frequency of usage
+    def __init__(
+        self,
+        in_features: int,
+        num_classes: int,
+        weight: nn.Parameter = None,
+        bias: bool = True,
+        dtype: torch.dtype = None,
+        weight_initializer: Callable = init.kaiming_uniform_(a=math.sqrt(5)),
+        bias_initializer: Callable = init.xavier_uniform_(a=1, scale=1),
+    ):
+        super().__init__()
+        self.in_features = in_features
+        self.num_classes = num_classes
+
+        if weight is not None:
+            self.weight = weight
+            self.has_weight = False
+        else:
+            self.weight = nn.Parameter(
+                torch.empty(
+                    self.num_classes, self.in_features, device=get_accelerator().get_current_device(), dtype=dtype
+                )
+            )
+            self.has_weight = True
+        if bias:
+            self.bias = nn.Parameter(
+                torch.zeros(self.num_classes, device=get_accelerator().get_current_device(), dtype=dtype)
+            )
+        else:
+            self.bias = None
 
-    # runtime limit
-    max_batch_size: int = 8
-    max_output_len: int = 256
-    max_input_len: int = 256
-
-    # general configs
-    dtype: Union[str, torch.dtype] = torch.float16  # use fp16 by default
-    kv_cache_dtype: Optional[str] = None
-
-    # generation configs
-    prompt_template: Optional[str] = None
-    do_sample: bool = False
-    beam_width: int = 1  # TODO: beam search is not support for now
-    prefill_ratio: Optional[
-        float
-    ] = 1.2  # the ratio of prefill sequences to decoding sequences, we do prefill step once the actual value exceeds ratio
-    pad_input: bool = False
-    early_stopping: Optional[bool] = False
-    top_k: Optional[int] = 50
-    top_p: Optional[float] = 1.0
-    temperature: Optional[float] = 1.0
-    no_repeat_ngram_size: Optional[int] = 0
-    repetition_penalty: Optional[float] = 1.0
-    forced_eos_token_id: int = None
-
-    # speculative decoding configs
-    max_n_spec_tokens: int = 5
-    glimpse_large_kv: bool = False
-
-    # paged attention configs
-    block_size: int = 16
-
-    # model parallelism configs
-    tp_size: int = 1
-    pp_size: int = 1
-    micro_batch_size: int = 1
-    micro_batch_buffer_size: int = None
-    high_precision: Optional[bool] = False
-
-    # cuda kernel option
-    use_cuda_kernel: bool = False
-
-    # cuda_graph
-    use_cuda_graph: bool = False  # NOTE only when we have the graph for specific decoding batch size can we use the cuda graph for inference
-    max_context_len_to_capture: int = 512
-    ignore_eos: bool = False
-
-    def __post_init__(self):
-        self.max_context_len_to_capture = self.max_input_len + self.max_output_len
-        self._verify_config()
-
-    def _verify_config(self) -> None:
-        """
-        Verify the input config
-        """
-        # check dtype
-        if isinstance(self.dtype, str):
-            # convert string dtype to torch dtype
-            assert (
-                self.dtype in _DTYPE_MAPPING
-            ), f"Expected the dtype string argument to be in {list(_DTYPE_MAPPING.keys())} but found an unknown dtype: {self.dtype}"
-            self.dtype = _DTYPE_MAPPING[self.dtype]
-        assert (
-            self.dtype in _ALLOWED_DTYPES
-        ), f"Expected dtype to be in {_ALLOWED_DTYPES} but found an unknown dtype: {self.dtype}"
+        self.reset_parameters(weight_initializer, bias_initializer)
+
+    def reset_parameters(self, weight_initializer, bias_initializer):
+        fan_in, fan_out = self.in_features, self.num_classes
+
+        if self.has_weight:
+            weight_initializer(self.weight, fan_in=fan_in, fan_out=fan_out)
+
+        if self.bias is not None:
+            bias_initializer(self.bias, fan_in=fan_in)
+
+    def forward(self, input_: Tensor) -> Tensor:
+        return F.linear(input_, self.weight, self.bias)
+
+
+@LAYERS.register_module
+class VanillaLayerNorm(nn.Module):
+    r"""
+    Layer Normalization for colossalai
+
+    Args:
+        normalized_shape (int): input shape from an expected input of size.
+            :math:`[* \times \text{normalized_shape}[0] \times \text{normalized_shape}[1]
+            \times \ldots \times \text{normalized_shape}[-1]]`
+            If a single integer is used, it is treated as a singleton list, and this module will
+            normalize over the last dimension which is expected to be of that specific size.
+        eps (float): a value added to the denominator for numerical stability, defaults to 1e-05.
+        bias (bool, optional): Whether to add a bias, defaults to ``True``.
+        dtype (:class:`torch.dtype`, optional): The dtype of parameters, defaults to None.
+    """
+
+    def __init__(self, normalized_shape: int, eps=1e-05, bias=True, dtype=None):
+        super().__init__()
 
-        if self.kv_cache_dtype:
-            assert (
-                self.use_cuda_kernel and self.kv_cache_dtype == "fp8"
-            ), f"FP8 kv_cache is only supported with use_cuda_kernel open now"
-            self.kv_cache_dtype = torch.uint8
-
-        # skip using casting when the data type is float32
-        if self.dtype == torch.float32:
-            self.high_precision = False
-
-        # check prompt template
-        if self.prompt_template is None:
-            return
+        self.normalized_shape = (normalized_shape,)
+        self.variance_epsilon = eps
 
-        if self.prompt_template in _DEFAULT_PROMPT_TEMPLATES:
-            self.prompt_template = _DEFAULT_PROMPT_TEMPLATES[self.prompt_template]
+        factory_kwargs = {"device": get_accelerator().get_current_device(), "dtype": dtype}
+
+        self.weight = nn.Parameter(torch.ones(normalized_shape, **factory_kwargs))
+        if bias:
+            self.bias = nn.Parameter(torch.zeros(normalized_shape, **factory_kwargs))
         else:
-            # make sure the template can be formatted with input_text
-            assert (
-                "{input_text}" in self.prompt_template
-            ), "The prompt template should contain '{input_text}' for formatting the input text. For example: 'USER: {input_text}\n\nASSISTANT: '"
-
-    def to_generation_config(self, model_config) -> GenerationConfig:
-        meta_config = {
-            "max_length": self.max_input_len + self.max_output_len,
-            "max_new_tokens": self.max_output_len,
-            "early_stopping": self.early_stopping,
-            "do_sample": self.do_sample,
-            "num_beams": self.beam_width,
-        }
-        for type in ["repetition_penalty", "no_repeat_ngram_size", "temperature", "top_k", "top_p"]:
-            if hasattr(self, type):
-                meta_config[type] = getattr(self, type)
-        for type in ["pad_token_id", "bos_token_id", "eos_token_id"]:
-            if hasattr(model_config, type):
-                meta_config[type] = getattr(model_config, type)
-
-        return GenerationConfig.from_dict(meta_config)
-
-    def to_rpc_param(self) -> dict:
-        kwargs = {
-            "dtype": str(self.dtype).split(".")[-1],
-            "max_n_spec_tokens": self.max_n_spec_tokens,
-            "max_batch_size": self.max_batch_size,
-            "max_input_len": self.max_input_len,
-            "max_output_len": self.max_output_len,
-            "tp_size": self.tp_size,
-            "pp_size": self.pp_size,
-            "pad_input": self.pad_input,
-            "early_stopping": self.early_stopping,
-            "do_sample": self.do_sample,
-            "beam_width": self.beam_width,
-            "kv_cache_dtype": str(self.kv_cache_dtype).split(".")[-1],
-        }
-        return kwargs
-
-    @staticmethod
-    def from_rpc_param(rpc_dict: dict) -> "InferenceConfig":
-        """
-        We intentionally don't use `dict.get` method to ensure we pass the right rpc param, or program will show error message
-        """
-        return InferenceConfig(
-            dtype=getattr(torch, rpc_dict["dtype"]),
-            max_n_spec_tokens=rpc_dict["max_n_spec_tokens"],
-            max_batch_size=rpc_dict["max_batch_size"],
-            max_input_len=rpc_dict["max_input_len"],
-            max_output_len=rpc_dict["max_output_len"],
-            tp_size=rpc_dict["tp_size"],
-            pp_size=rpc_dict["pp_size"],
-            pad_input=rpc_dict["pad_input"],
-            early_stopping=rpc_dict["early_stopping"],
-            do_sample=rpc_dict["do_sample"],
-            beam_width=rpc_dict["beam_width"],
-            kv_cache_dtype=getattr(torch, rpc_dict["kv_cache_dtype"], None),
-        )
+            self.bias = None
+
+    def forward(self, x: Tensor) -> Tensor:
+        return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.variance_epsilon)
+
+
+@LAYERS.register_module
+class VanillaLinear(nn.Module):
+    """Linear layer.
 
-    @classmethod
-    def from_dict(cls, config_dict: Dict[str, Any]) -> "InferenceConfig":
-        # Get the list of attributes of this dataclass.
-        attrs = [attr.name for attr in fields(cls)]
-        inference_config_args = {}
-        for attr in attrs:
-            if attr in config_dict:
-                inference_config_args[attr] = config_dict[attr]
-            else:
-                inference_config_args[attr] = getattr(cls, attr)
-
-        # Set the attributes from the parsed arguments.
-        inference_config = cls(**inference_config_args)
-        return inference_config
+    Args:
+        in_features (int): size of each input sample.
+        out_features (int): size of each output sample.
+        bias (bool, optional): If set to ``False``, the layer will not learn an additive bias, defaults to ``True``.
+        dtype (:class:`torch.dtype`, optional): The dtype of parameters, defaults to None.
+        skip_bias_add: bool (optional, default to be false).
+        weight_initializer (:class:`typing.Callable`, optional):
+            The initializer of weight, defaults to kaiming uniform initializer.
+        bias_initializer (:class:`typing.Callable`, optional):
+            The initializer of bias, defaults to xavier uniform initializer.
+
+    More details about ``initializer`` please refer to
+    `init <https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/init.py>`_.
+    """
+
+    def __init__(
+        self,
+        in_features: int,
+        out_features: int,
+        bias: bool = True,
+        dtype: torch.dtype = None,
+        skip_bias_add: bool = False,
+        weight_initializer: Callable = init.kaiming_uniform_(a=math.sqrt(5)),
+        bias_initializer: Callable = init.xavier_uniform_(a=1, scale=1),
+        **kwargs,
+    ) -> None:
+        super().__init__()
+        self.in_features = in_features
+        self.out_features = out_features
+        self.skip_bias_add = skip_bias_add
+        factory_kwargs = {"device": get_accelerator().get_current_device(), "dtype": dtype}
+        self.weight = Parameter(torch.empty(self.out_features, self.in_features, **factory_kwargs))
+        if bias:
+            self.bias = Parameter(torch.empty(self.out_features, **factory_kwargs))
+        else:
+            self.bias = None
+        weight_initializer(self.weight, fan_in=in_features, fan_out=out_features)
+        if self.bias is not None:
+            bias_initializer(self.bias, fan_in=in_features)
+
+    def forward(self, input: Tensor) -> Tensor:
+        if not self.skip_bias_add:
+            return F.linear(input, self.weight, self.bias)
+        else:
+            return F.linear(input, self.weight), self.bias
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/inference/modeling/models/glide_llama.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/mistral.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,475 +1,611 @@
-# This is modified from huggingface transformers
-# https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/llama/modeling_llama.py
 import warnings
-from types import MethodType
 from typing import List, Optional, Tuple, Union
 
 import torch
-import torch.nn as nn
+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
 from transformers.cache_utils import Cache, DynamicCache
-from transformers.modeling_attn_mask_utils import (
-    _prepare_4d_causal_attention_mask,
-    _prepare_4d_causal_attention_mask_for_sdpa,
-)
-from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
-from transformers.models.llama.modeling_llama import (
-    LlamaAttention,
-    LlamaConfig,
-    LlamaDecoderLayer,
-    LlamaDynamicNTKScalingRotaryEmbedding,
-    LlamaForCausalLM,
-    LlamaLinearScalingRotaryEmbedding,
-    LlamaMLP,
-    LlamaModel,
-    LlamaRMSNorm,
-    LlamaRotaryEmbedding,
+from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
+from transformers.modeling_outputs import (
+    BaseModelOutputWithPast,
+    CausalLMOutputWithPast,
+    SequenceClassifierOutputWithPast,
 )
+from transformers.models.mistral.modeling_mistral import MistralForCausalLM, MistralModel
+from transformers.utils import logging
 
-from colossalai.inference.spec import GlideInput
-from colossalai.kernel.triton import flash_decoding_attention
-from colossalai.logging import get_dist_logger
-
-logger = get_dist_logger(__name__)
-
-
-def rotate_half(x):
-    """Rotates half the hidden dims of the input."""
-    x1 = x[..., : x.shape[-1] // 2]
-    x2 = x[..., x.shape[-1] // 2 :]
-    return torch.cat((-x2, x1), dim=-1)
-
-
-def apply_single_rotary_pos_emb(q, cos, sin, position_ids):
-    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
-    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
-    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
-    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
-    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
-    q_embed = (q * cos) + (rotate_half(q) * sin)
-    return q_embed
-
-
-def glide_llama_causal_lm_forward(
-    self: LlamaForCausalLM,
-    input_ids: torch.LongTensor = None,
-    glide_input: Optional[GlideInput] = None,
-    attention_mask: Optional[torch.Tensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[List[torch.FloatTensor]] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    labels: Optional[torch.LongTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, CausalLMOutputWithPast]:
-    r"""
-    Args:
-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
-
-    Returns:
-
-    Example:
-
-    ```python
-    >>> from transformers import AutoTokenizer, LlamaForCausalLM
-
-    >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
-    >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)
-
-    >>> prompt = "Hey, are you conscious? Can you talk to me?"
-    >>> inputs = tokenizer(prompt, return_tensors="pt")
-
-    >>> # Generate
-    >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
-    >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
-    "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
-    ```"""
-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-    output_hidden_states = (
-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-    )
-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-    outputs = self.model(
-        input_ids=input_ids,
-        glide_input=glide_input,
-        attention_mask=attention_mask,
-        position_ids=position_ids,
-        past_key_values=past_key_values,
-        inputs_embeds=inputs_embeds,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-        output_hidden_states=output_hidden_states,
-        return_dict=return_dict,
-    )
-
-    hidden_states = outputs[0]
-    logits = self.lm_head(hidden_states)
-    logits = logits.float()
-
-    if not return_dict:
-        output = (logits,) + outputs[1:]
-        return output
-
-    return CausalLMOutputWithPast(
-        loss=None,
-        logits=logits,
-        past_key_values=outputs.past_key_values,
-        hidden_states=outputs.hidden_states,
-        attentions=outputs.attentions,
-    )
-
-
-def glide_llama_model_forward(
-    self: LlamaModel,
-    input_ids: torch.LongTensor = None,
-    glide_input: GlideInput = None,
-    attention_mask: Optional[torch.Tensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[List[torch.FloatTensor]] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, BaseModelOutputWithPast]:
-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-    output_hidden_states = (
-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-    )
-    use_cache = use_cache if use_cache is not None else self.config.use_cache
-
-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-    # retrieve input_ids and inputs_embeds
-    if input_ids is not None and inputs_embeds is not None:
-        raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
-    elif input_ids is not None:
-        batch_size, seq_length = input_ids.shape[:2]
-    elif inputs_embeds is not None:
-        batch_size, seq_length = inputs_embeds.shape[:2]
-    else:
-        raise ValueError("You have to specify either input_ids or inputs_embeds")
-
-    past_key_values_length = 0
-    if use_cache:
-        use_legacy_cache = not isinstance(past_key_values, Cache)
-        if use_legacy_cache:
-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)
-        past_key_values_length = past_key_values.get_usable_length(seq_length)
-
-    if position_ids is None:
-        device = input_ids.device if input_ids is not None else inputs_embeds.device
-        position_ids = torch.arange(
-            past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
-        )
-        position_ids = position_ids.unsqueeze(0)
+from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer.shard import ShardConfig
 
-    if inputs_embeds is None:
-        inputs_embeds = self.embed_tokens(input_ids)
+from ..layer import ColoAttention
 
-    if self._use_flash_attention_2:
-        # 2d mask is passed through the layers
-        attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
-    elif self._use_sdpa and not output_attentions:
-        # output_attentions=True can not be supported when using SDPA, and we fall back on
-        # the manual implementation that requires a 4D causal mask in all cases.
-        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
-            attention_mask,
-            (batch_size, seq_length),
-            inputs_embeds,
-            past_key_values_length,
-        )
-    else:
-        # 4d mask is passed through the layers
-        attention_mask = _prepare_4d_causal_attention_mask(
-            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
+logger = logging.get_logger(__name__)
+
+
+class MistralForwards:
+    @staticmethod
+    def mistral_model_forward(
+        self: MistralModel,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        stage_manager: Optional[PipelineStageManager] = None,
+        hidden_states: Optional[torch.FloatTensor] = None,
+        stage_index: Optional[List[int]] = None,
+        shard_config: ShardConfig = None,
+    ) -> Union[Tuple, BaseModelOutputWithPast]:
+        if use_cache:
+            logger.warning_once("use_cache=True is not supported for Mistral models at the moment.")
+            use_cache = False
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
 
-    # embed positions
-    hidden_states = inputs_embeds
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-    # decoder layers
-    all_hidden_states = () if output_hidden_states else None
-    all_self_attns = () if output_attentions else None
-    next_decoder_cache = () if use_cache else None
+        # retrieve input_ids and inputs_embeds
+        if stage_manager.is_first_stage():
+            if input_ids is not None and inputs_embeds is not None:
+                raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+            elif input_ids is not None:
+                batch_size, seq_length = input_ids.shape
+            elif inputs_embeds is not None:
+                batch_size, seq_length, _ = inputs_embeds.shape
+            else:
+                raise ValueError("You have to specify either input_ids or inputs_embeds")
+            inputs_embeds = self.embed_tokens(input_ids)
+            hidden_states = inputs_embeds
+        else:
+            input_shape = hidden_states.shape[:-1]
+            batch_size, seq_length = input_shape
+            device = hidden_states.device
+
+        past_key_values_length = 0
+
+        if position_ids is None:
+            device = input_ids.device if input_ids is not None else inputs_embeds.device
+            position_ids = torch.arange(
+                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
+            )
+            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
+        else:
+            position_ids = position_ids.view(-1, seq_length).long()
 
-    for decoder_layer in self.layers:
+        if attention_mask is not None and self._use_flash_attention_2 and use_cache:
+            is_padding_right = attention_mask[:, -1].sum().item() != batch_size
+            if is_padding_right:
+                raise ValueError(
+                    "You are attempting to perform batched generation with padding_side='right'"
+                    " this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to "
+                    " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
+                )
+
+        if shard_config.enable_flash_attention:
+            # in this case, attention_mask is a dict rather than a tensor
+            mask_shape = (batch_size, 1, seq_length, seq_length)
+            attention_mask = ColoAttention.prepare_attn_kwargs(
+                mask_shape,
+                hidden_states.dtype,
+                hidden_states.device,
+                q_padding_mask=attention_mask,
+                is_causal=True,
+            )
+        else:
+            if self._use_flash_attention_2:
+                # 2d mask is passed through the layers
+                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+            else:
+                # 4d mask is passed through the layers
+                attention_mask = _prepare_4d_causal_attention_mask(
+                    attention_mask,
+                    (batch_size, seq_length),
+                    hidden_states,
+                    past_key_values_length,
+                    sliding_window=self.config.sliding_window,
+                )
+
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
+
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+
+        start_idx, end_idx = stage_index[0], stage_index[1]
+        num_ckpt_layers = 0
+        if self.gradient_checkpointing and self.training:
+            num_ckpt_layers = end_idx - start_idx
+            # TODO: We can replace `gradient_checkpointing_enable` fn and initialize a gradient_checkpointing (List[bool]) for each layer
+            if shard_config.gradient_checkpoint_config is not None:
+                num_ckpt_layers = shard_config.gradient_checkpoint_config.get_num_ckpt_layers(
+                    stage=stage_manager.stage,
+                    num_stages=stage_manager.num_stages,
+                    num_layers=end_idx - start_idx,
+                    model_chunk_id=(stage_manager.model_chunk_id if stage_manager.is_interleave else 0),
+                    num_model_chunks=stage_manager.num_model_chunks,
+                )
+            assert num_ckpt_layers <= end_idx - start_idx
+
+        for idx, decoder_layer in enumerate(self.layers[start_idx:end_idx], start=start_idx):
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
+
+            if idx - start_idx < num_ckpt_layers:
+                layer_outputs = self._gradient_checkpointing_func(
+                    decoder_layer.__call__,
+                    hidden_states,
+                    attention_mask,
+                    position_ids,
+                    past_key_values,
+                    output_attentions,
+                    use_cache,
+                )
+            else:
+                layer_outputs = decoder_layer(
+                    hidden_states,
+                    attention_mask=attention_mask,
+                    position_ids=position_ids,
+                    past_key_value=past_key_values,
+                    output_attentions=output_attentions,
+                    use_cache=use_cache,
+                )
+
+            hidden_states = layer_outputs[0]
+
+            if use_cache:
+                layer_outputs[2 if output_attentions else 1]
+
+            if output_attentions:
+                all_self_attns += (layer_outputs[1],)
+
+        if stage_manager.is_last_stage():
+            hidden_states = self.norm(hidden_states)
+
+        # add hidden states from the last decoder layer
         if output_hidden_states:
             all_hidden_states += (hidden_states,)
 
-        # GlideLlamaDecoderLayer
-        layer_outputs = decoder_layer(
-            hidden_states,
-            glide_input=glide_input,
+        next_cache = None
+        if stage_manager.is_last_stage():
+            if not return_dict:
+                return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+            return BaseModelOutputWithPast(
+                last_hidden_state=hidden_states,
+                past_key_values=next_cache,
+                hidden_states=all_hidden_states,
+                attentions=all_self_attns,
+            )
+        else:
+            return {"hidden_states": hidden_states}
+
+    @staticmethod
+    def mistral_for_causal_lm_forward(
+        self: MistralForCausalLM,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        stage_manager: Optional[PipelineStageManager] = None,
+        hidden_states: Optional[torch.FloatTensor] = None,
+        stage_index: Optional[List[int]] = None,
+        shard_config: ShardConfig = None,
+    ) -> Union[Tuple, CausalLMOutputWithPast]:
+        r"""
+        Args:
+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
+
+        Returns:
+
+        Example:
+
+        ```python
+        >>> from transformers import AutoTokenizer, MistralForCausalLM
+
+        >>> model = MistralForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
+        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)
+
+        >>> prompt = "Hey, are you conscious? Can you talk to me?"
+        >>> inputs = tokenizer(prompt, return_tensors="pt")
+
+        >>> # Generate
+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
+        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
+        ```"""
+
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+        outputs = MistralForwards.mistral_model_forward(
+            self.model,
+            input_ids=input_ids,
             attention_mask=attention_mask,
             position_ids=position_ids,
-            past_key_value=past_key_values,
+            past_key_values=past_key_values,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
             output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+            stage_manager=stage_manager,
+            hidden_states=hidden_states,
+            stage_index=stage_index,
+            shard_config=shard_config,
+        )
+
+        past_key_values = None
+
+        if stage_manager.is_last_stage():
+            hidden_states = outputs[0]
+            logits = self.lm_head(hidden_states)
+            logits = logits.float()
+
+            loss = None
+            if labels is not None:
+                # Shift so that tokens < n predict n
+                shift_logits = logits[..., :-1, :].contiguous()
+                shift_labels = labels[..., 1:].contiguous()
+                # Flatten the tokens
+                loss_fct = CrossEntropyLoss()
+                shift_logits = shift_logits.view(-1, self.config.vocab_size)
+                shift_labels = shift_labels.view(-1)
+                # Enable model parallelism
+                shift_labels = shift_labels.to(shift_logits.device)
+                loss = loss_fct(shift_logits, shift_labels)
+
+            if not return_dict:
+                output = (logits,) + outputs[1:]
+                return (loss,) + output if loss is not None else output
+
+            return CausalLMOutputWithPast(
+                loss=loss,
+                logits=logits,
+                past_key_values=outputs.past_key_values,
+                hidden_states=outputs.hidden_states,
+                attentions=outputs.attentions,
+            )
+        else:
+            hidden_states = outputs.get("hidden_states")
+            return {"hidden_states": hidden_states}
+
+    @staticmethod
+    def mistral_for_sequence_classification_forward(
+        self,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        stage_manager: Optional[PipelineStageManager] = None,
+        hidden_states: Optional[torch.FloatTensor] = None,
+        stage_index: Optional[List[int]] = None,
+        shard_config: ShardConfig = None,
+    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
+        """
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        transformer_outputs = MistralForwards.mistral_model_forward(
+            self.model,
+            input_ids,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_values=past_key_values,
+            inputs_embeds=inputs_embeds,
             use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+            stage_manager=stage_manager,
+            hidden_states=hidden_states,
+            stage_index=stage_index,
+            shard_config=shard_config,
         )
 
-        hidden_states = layer_outputs[0]
+        if input_ids is not None:
+            batch_size = input_ids.shape[0]
+        elif inputs_embeds is not None:
+            batch_size = inputs_embeds.shape[0]
+        else:
+            batch_size = hidden_states.shape[0]
 
-        if use_cache:
-            next_decoder_cache = layer_outputs[2 if output_attentions else 1]
+        if stage_manager.is_last_stage():
+            hidden_states = transformer_outputs[0]
+            logits = self.score(hidden_states)
+            if self.config.pad_token_id is None and batch_size != 1:
+                raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
+            if self.config.pad_token_id is None:
+                sequence_lengths = -1
+            else:
+                if input_ids is not None:
+                    sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1).to(
+                        logits.device
+                    )
+                else:
+                    sequence_lengths = -1
+
+            pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]
+
+            loss = None
+            if labels is not None:
+                labels = labels.to(logits.device)
+                if self.config.problem_type is None:
+                    if self.num_labels == 1:
+                        self.config.problem_type = "regression"
+                    elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
+                        self.config.problem_type = "single_label_classification"
+                    else:
+                        self.config.problem_type = "multi_label_classification"
+
+                if self.config.problem_type == "regression":
+                    loss_fct = MSELoss()
+                    if self.num_labels == 1:
+                        loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())
+                    else:
+                        loss = loss_fct(pooled_logits, labels)
+                elif self.config.problem_type == "single_label_classification":
+                    loss_fct = CrossEntropyLoss()
+                    loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
+                elif self.config.problem_type == "multi_label_classification":
+                    loss_fct = BCEWithLogitsLoss()
+                    loss = loss_fct(pooled_logits, labels)
+            if not return_dict:
+                output = (pooled_logits,) + transformer_outputs[1:]
+                return ((loss,) + output) if loss is not None else output
+        else:
+            hidden_states = transformer_outputs.get("hidden_states")
+            return {"hidden_states": hidden_states}
+
+        return SequenceClassifierOutputWithPast(
+            loss=loss,
+            logits=pooled_logits,
+            past_key_values=transformer_outputs.past_key_values,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
 
-        if output_attentions:
-            all_self_attns += (layer_outputs[1],)
 
-    hidden_states = self.norm(hidden_states)
+def get_mistral_model_forward_for_flash_attn(shard_config: ShardConfig):
+    logger = logging.get_logger(__name__)
+    assert shard_config.enable_flash_attention, "Flash Attention is not enabled."
 
-    # add hidden states from the last decoder layer
-    if output_hidden_states:
-        all_hidden_states += (hidden_states,)
-
-    next_cache = None
-    if use_cache:
-        next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache
-    if not return_dict:
-        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
-    return BaseModelOutputWithPast(
-        last_hidden_state=hidden_states,
-        past_key_values=next_cache,
-        hidden_states=all_hidden_states,
-        attentions=all_self_attns,
-    )
+    def forward(
+        self: MistralModel,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, BaseModelOutputWithPast]:
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
 
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-class GlideLlamaConfig(LlamaConfig):
-    """Configuration class with specific arguments used by GLIDE llama model as a drafter"""
+        # retrieve input_ids and inputs_embeds
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+        elif input_ids is not None:
+            batch_size, seq_length = input_ids.shape
+        elif inputs_embeds is not None:
+            batch_size, seq_length, _ = inputs_embeds.shape
+        else:
+            raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
 
-    def __init__(
-        self,
-        large_hidden_size=4096,
-        large_num_attention_heads=32,
-        **kwargs,
-    ):
-        super().__init__(**kwargs)
-        self.large_hidden_size = large_hidden_size
-        self.large_num_attention_heads = large_num_attention_heads
-
-
-class LlamaCrossAttention(nn.Module):
-    """Multi-headed attention from 'Attention Is All You Need' paper"""
-
-    def __init__(self, config: GlideLlamaConfig):
-        super().__init__()
-        self.config = config
-        self.hidden_size = config.hidden_size
-        self.num_heads = config.num_attention_heads
-        self.head_dim = self.hidden_size // self.num_heads
-        self.num_key_value_heads = config.num_key_value_heads
-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
-        self.max_position_embeddings = config.max_position_embeddings
-
-        if (self.head_dim * self.num_heads) != self.hidden_size:
-            raise ValueError(
-                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
-                f" and `num_heads`: {self.num_heads})."
-            )
+        past_key_values_length = 0
 
-        # large model (verifier) configs
-        self.large_hidden_size = config.large_hidden_size
-        self.large_num_heads = config.large_num_attention_heads
-        self.large_head_dim = self.large_hidden_size // self.large_num_heads
-
-        self.q_proj = nn.Linear(self.hidden_size, self.large_num_heads * self.large_head_dim, bias=False)
-        self.o_proj = nn.Linear(self.large_num_heads * self.large_head_dim, self.hidden_size, bias=False)
-        self._init_rope()
-
-    def _init_rope(self):
-        if self.config.rope_scaling is None:
-            self.rotary_emb = LlamaRotaryEmbedding(
-                self.large_head_dim,
-                max_position_embeddings=self.max_position_embeddings,
+        if use_cache:
+            use_legacy_cache = not isinstance(past_key_values, Cache)
+            if use_legacy_cache:
+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)
+            past_key_values_length = past_key_values.get_usable_length(seq_length)
+
+        if position_ids is None:
+            device = input_ids.device if input_ids is not None else inputs_embeds.device
+            position_ids = torch.arange(
+                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
             )
+            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
         else:
-            scaling_type = self.config.rope_scaling["type"]
-            scaling_factor = self.config.rope_scaling["factor"]
-            if scaling_type == "linear":
-                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(
-                    self.large_head_dim,
-                    max_position_embeddings=self.max_position_embeddings,
-                    scaling_factor=scaling_factor,
-                )
-            elif scaling_type == "dynamic":
-                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(
-                    self.large_head_dim,
-                    max_position_embeddings=self.max_position_embeddings,
-                    scaling_factor=scaling_factor,
+            position_ids = position_ids.view(-1, seq_length).long()
+
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+
+        if attention_mask is not None and self._use_flash_attention_2 and use_cache:
+            is_padding_right = attention_mask[:, -1].sum().item() != batch_size
+            if is_padding_right:
+                raise ValueError(
+                    "You are attempting to perform batched generation with padding_side='right'"
+                    " this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to "
+                    " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
                 )
+        if shard_config.enable_flash_attention:
+            # in this case, attention_mask is a dict rather than a tensor
+            mask_shape = (batch_size, 1, seq_length, seq_length)
+            attention_mask = ColoAttention.prepare_attn_kwargs(
+                mask_shape,
+                inputs_embeds.dtype,
+                inputs_embeds.device,
+                q_padding_mask=attention_mask,
+                is_causal=True,
+            )
+        else:
+            if self._use_flash_attention_2:
+                # 2d mask is passed through the layers
+                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
             else:
-                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")
+                # 4d mask is passed through the layers
+                attention_mask = _prepare_4d_causal_attention_mask(
+                    attention_mask,
+                    (batch_size, seq_length),
+                    inputs_embeds,
+                    past_key_values_length,
+                    sliding_window=self.config.sliding_window,
+                )
 
-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
+        hidden_states = inputs_embeds
 
-    def forward(
-        self,
-        hidden_states: torch.Tensor,
-        glide_input: GlideInput = None,  # Used for glimpsing main model's KV caches
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        output_attentions: bool = False,
-        use_cache: bool = False,
-    ) -> Optional[torch.Tensor]:
-        bsz, q_len, _ = hidden_states.size()
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
 
-        block_tables = glide_input.block_tables
-        large_k_cache = glide_input.large_k_cache
-        large_v_cache = glide_input.large_v_cache
-        sequence_lengths = glide_input.sequence_lengths
-        cache_block_size = large_k_cache.size(-2)
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+        next_decoder_cache = None
+
+        for decoder_layer in self.layers:
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
+
+            if self.gradient_checkpointing and self.training:
+                layer_outputs = self._gradient_checkpointing_func(
+                    decoder_layer.__call__,
+                    hidden_states,
+                    attention_mask,
+                    position_ids,
+                    past_key_values,
+                    output_attentions,
+                    use_cache,
+                )
+            else:
+                layer_outputs = decoder_layer(
+                    hidden_states,
+                    attention_mask=attention_mask,
+                    position_ids=position_ids,
+                    past_key_value=past_key_values,
+                    output_attentions=output_attentions,
+                    use_cache=use_cache,
+                )
 
-        query_states = self.q_proj(hidden_states)
-        kv_seq_len = sequence_lengths.max().item()
+            hidden_states = layer_outputs[0]
 
-        query_states = query_states.view(bsz, -1, self.large_num_heads, self.large_head_dim).transpose(1, 2)
+            if use_cache:
+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]
 
-        # for RoPE
-        cos, sin = self.rotary_emb(query_states, seq_len=kv_seq_len + 32)
-        query_states = apply_single_rotary_pos_emb(query_states, cos, sin, position_ids)
-        query_states = query_states.transpose(1, 2)
-        query_states = query_states.reshape(-1, self.large_num_heads, self.large_head_dim)
-
-        attn_output = flash_decoding_attention(
-            q=query_states,
-            k_cache=large_k_cache,
-            v_cache=large_v_cache,
-            kv_seq_len=sequence_lengths,
-            block_tables=block_tables,
-            block_size=cache_block_size,
-            max_seq_len_in_batch=kv_seq_len,
-        )  # attn_output: [bsz * q_len, num_heads * head_dim]
+            if output_attentions:
+                all_self_attns += (layer_outputs[1],)
 
-        attn_output = attn_output.reshape(bsz, q_len, self.large_hidden_size)
+        hidden_states = self.norm(hidden_states)
 
-        attn_output = self.o_proj(attn_output)
+        # add hidden states from the last decoder layer
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
 
-        return attn_output
+        next_cache = None
+        if use_cache:
+            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache
 
+        if not return_dict:
+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+        return BaseModelOutputWithPast(
+            last_hidden_state=hidden_states,
+            past_key_values=next_cache,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attns,
+        )
 
-# A class to be used to replace LlamaDecoderLayer in a Llama Model as Drafter in speculative decoding.
-# Refer to GLIDE with a CAPE https://arxiv.org/pdf/2402.02082.pdf
-class GlideLlamaDecoderLayer(nn.Module):
-    def __init__(self, config: GlideLlamaConfig, layer_idx: Optional[int] = None):
-        super().__init__()
-        self.hidden_size = config.hidden_size
-        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)
-        self.cross_attn = LlamaCrossAttention(config=config)
-        self.mlp = LlamaMLP(config)
-        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
-        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+    return forward
 
-    @staticmethod
-    def from_native_module(module: LlamaDecoderLayer, *args, **kwargs) -> "GlideLlamaDecoderLayer":
-        """Build a GlideLlamaDecoderLayer from a native LlamaDecoderLayer"""
-        config: LlamaConfig = module.mlp.config  # XXX
-        layer_idx = module.self_attn.layer_idx
-        glide_config = GlideLlamaConfig(**config.to_dict())
-        glide_decoder_layer = GlideLlamaDecoderLayer(glide_config, layer_idx=layer_idx)
 
-        return glide_decoder_layer
+def get_mistral_flash_attention_forward(shard_config: ShardConfig):
+    from transformers.models.mistral.modeling_mistral import MistralAttention, apply_rotary_pos_emb, repeat_kv
 
     def forward(
-        self,
+        self: MistralAttention,
         hidden_states: torch.Tensor,
-        glide_input: GlideInput = None,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        output_attentions: Optional[bool] = False,
-        use_cache: Optional[bool] = False,
+        past_key_value: Optional[Cache] = None,
+        output_attentions: bool = False,
+        use_cache: bool = False,
         **kwargs,
-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
-        """
-        Args:
-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
-            attention_mask (`torch.FloatTensor`, *optional*):
-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,
-                query_sequence_length, key_sequence_length)` if default attention is used.
-            output_attentions (`bool`, *optional*):
-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
-                returned tensors for more detail.
-            use_cache (`bool`, *optional*):
-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
-                (see `past_key_values`).
-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
-        """
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
         if "padding_mask" in kwargs:
             warnings.warn(
                 "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
             )
+        bsz, q_len, _ = hidden_states.size()
 
-        residual = hidden_states
-
-        hidden_states = self.input_layernorm(hidden_states)
-
-        # Self Attention
-        hidden_states, self_attn_weights, present_key_value = self.self_attn(
-            hidden_states=hidden_states,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_value=past_key_value,
-            output_attentions=output_attentions,
-            use_cache=use_cache,
-            **kwargs,
-        )
-        hidden_states = residual + hidden_states
-
-        curr_q_len = hidden_states.size(1)
-        # Cross attention
-        if glide_input is None or not glide_input.glimpse_ready:
-            warnings.warn(
-                "Data used for glimpsing the past KV caches of the main model (verifier) is not complete. "
-                "Fall back to normal decoder layer modeling (drafter). "
-                "This might lead to incorrect results when using the Glide Models for speculative decoding."
-            )
-        elif curr_q_len == 1:
-            # Notice that we skip prefill stage
-            # always use the output of the main model as the inputs for the next round of speculation
-            residual = hidden_states
-
-            hidden_states = self.cross_attn(
-                hidden_states=hidden_states,
-                glide_input=glide_input,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                output_attentions=output_attentions,
-                use_cache=True,
-            )
-            hidden_states = residual + hidden_states
+        query_states = self.q_proj(hidden_states)
+        key_states = self.k_proj(hidden_states)
+        value_states = self.v_proj(hidden_states)
 
-        # Fully Connected
-        residual = hidden_states
-        hidden_states = self.post_attention_layernorm(hidden_states)
-        hidden_states = self.mlp(hidden_states)
-        hidden_states = residual + hidden_states
+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
+
+        kv_seq_len = key_states.shape[-2]
+        if past_key_value is not None:
+            if self.layer_idx is None:
+                raise ValueError(
+                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
+                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
+                    "with a layer index."
+                )
+            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
+        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
+
+        if past_key_value is not None:
+            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
+
+        # repeat k/v heads if n_kv_heads < n_heads
+        key_states = repeat_kv(key_states, self.num_key_value_groups)
+        value_states = repeat_kv(value_states, self.num_key_value_groups)
 
-        outputs = (hidden_states,)
+        assert isinstance(attention_mask, dict), "Flash Attention Error: attention_mask should be a dict."
+        attn_output = ColoAttention.attention(query_states, key_states, value_states, **attention_mask)
 
-        if use_cache:
-            outputs += (present_key_value,)
+        attn_output = attn_output.transpose(1, 2).contiguous()
+        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
 
-        return outputs
+        attn_output = self.o_proj(attn_output)
 
+        return attn_output, None, past_key_value
 
-class GlideLlamaForCausalLM(LlamaForCausalLM):
-    def __init__(self, config: GlideLlamaConfig):
-        super().__init__(config)
-        self.config = config
-        bound_method = MethodType(glide_llama_causal_lm_forward, self)
-        setattr(self, "forward", bound_method)
-        bound_method = MethodType(glide_llama_model_forward, self.model)
-        model = getattr(self, "model")
-        setattr(model, "forward", bound_method)
-        replaced_layers = nn.ModuleList(
-            [GlideLlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
-        )
-        setattr(model, "layers", replaced_layers)
+    return forward
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/inference/modeling/policy/nopadding_baichuan.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/bloom.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,110 +1,102 @@
-from colossalai.inference.config import RPC_PARAM
-from colossalai.inference.modeling.layers.baichuan_tp_linear import (
-    BaichuanLMHeadLinear1D_Col,
-    BaichuanWpackLinear1D_Col,
-)
-from colossalai.inference.modeling.models.nopadding_baichuan import (
-    NopadBaichuanAttention,
-    NopadBaichuanMLP,
-    baichuan_rmsnorm_forward,
-)
-from colossalai.inference.modeling.models.nopadding_llama import (
-    llama_causal_lm_forward,
-    llama_decoder_layer_forward,
-    llama_model_forward,
-)
-from colossalai.inference.utils import init_to_get_rotary
-from colossalai.shardformer.layer import Linear1D_Col, Linear1D_Row
+from functools import partial
+
+import torch
+from torch.nn import LayerNorm
+
+import colossalai.shardformer.layer as col_nn
 from colossalai.shardformer.policies.base_policy import ModulePolicyDescription, SubModuleReplacementDescription
-from colossalai.shardformer.policies.llama import LlamaForCausalLMPolicy
+from colossalai.shardformer.policies.bloom import BloomForCausalLMPolicy
+
+from ..modeling.bloom import BloomInferenceForwards
+
+try:
+    from colossalai.kernel.triton import layer_norm
+
+    HAS_TRITON_NORM = True
+except:
+    print("Some of our kernels require triton. You might want to install triton from https://github.com/openai/triton")
+    HAS_TRITON_NORM = False
+
+
+def get_triton_layernorm_forward():
+    if HAS_TRITON_NORM:
+
+        def _triton_layernorm_forward(self: LayerNorm, hidden_states: torch.Tensor):
+            return layer_norm(hidden_states, self.weight.data, self.bias, self.eps)
 
+        return _triton_layernorm_forward
+    else:
+        return None
 
-class NoPaddingBaichuanModelInferPolicy(LlamaForCausalLMPolicy, RPC_PARAM):
+
+class BloomModelInferPolicy(BloomForCausalLMPolicy):
     def __init__(self) -> None:
         super().__init__()
 
     def module_policy(self):
+        from transformers.models.bloom.modeling_bloom import BloomAttention, BloomBlock, BloomForCausalLM, BloomModel
+
         policy = super().module_policy()
 
-        if self.shard_config.enable_tensor_parallelism:
-            decoder_attribute_replacement = {
-                "self_attn.hidden_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
-                "self_attn.num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
-            }
-            if getattr(self.model.config, "num_key_value_heads", False):
-                decoder_attribute_replacement["self_attn.num_key_value_heads"] = (
-                    self.model.config.num_key_value_heads // self.shard_config.tensor_parallel_size
-                )
-        else:
-            decoder_attribute_replacement = None
-
-        # used for Baichuan 7B and 13B for baichuan DecoderLayer
-        for DecoderLayer in ["DecoderLayer", "BaichuanLayer"]:
-            policy[DecoderLayer] = ModulePolicyDescription(
-                attribute_replacement=decoder_attribute_replacement,
+        if self.shard_config.extra_kwargs.get("inference_gptq", False):
+            from colossalai.inference.quant.gptq.cai_gptq import ColCaiQuantLinear, RowCaiQuantLinear
+
+            policy[BloomBlock] = ModulePolicyDescription(
+                attribute_replacement={
+                    "self_attention.hidden_size": self.model.config.hidden_size
+                    // self.shard_config.tensor_parallel_size,
+                    "self_attention.split_size": self.model.config.hidden_size
+                    // self.shard_config.tensor_parallel_size,
+                    "self_attention.num_heads": self.model.config.n_head // self.shard_config.tensor_parallel_size,
+                },
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
-                        suffix="mlp.gate_proj",
-                        target_module=Linear1D_Col,
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="mlp.up_proj",
-                        target_module=Linear1D_Col,
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="mlp.down_proj",
-                        target_module=Linear1D_Row,
+                        suffix="self_attention.query_key_value",
+                        target_module=ColCaiQuantLinear,
+                        kwargs={"split_num": 3},
                     ),
                     SubModuleReplacementDescription(
-                        suffix="mlp",
-                        target_module=NopadBaichuanMLP,
+                        suffix="self_attention.dense", target_module=RowCaiQuantLinear, kwargs={"split_num": 1}
                     ),
                     SubModuleReplacementDescription(
-                        suffix="self_attn.W_pack",
-                        target_module=BaichuanWpackLinear1D_Col,
+                        suffix="self_attention.attention_dropout",
+                        target_module=col_nn.DropoutForParallelInput,
                     ),
                     SubModuleReplacementDescription(
-                        suffix="self_attn.o_proj",
-                        target_module=Linear1D_Row,
+                        suffix="mlp.dense_h_to_4h", target_module=ColCaiQuantLinear, kwargs={"split_num": 1}
                     ),
                     SubModuleReplacementDescription(
-                        suffix="self_attn",
-                        target_module=NopadBaichuanAttention,
+                        suffix="mlp.dense_4h_to_h", target_module=RowCaiQuantLinear, kwargs={"split_num": 1}
                     ),
                 ],
             )
+        # NOTE set inference mode to shard config
+        self.shard_config._infer()
 
-            self.append_or_create_method_replacement(
-                description={"forward": llama_decoder_layer_forward}, policy=policy, target_key=DecoderLayer
-            )
-
-        policy["BaichuanForCausalLM"] = ModulePolicyDescription(
-            sub_module_replacement=[
-                SubModuleReplacementDescription(
-                    suffix="lm_head", target_module=BaichuanLMHeadLinear1D_Col, kwargs={"gather_output": True}
-                )
-            ],
-        )
-
-        self.append_or_create_method_replacement(
-            description={"forward": llama_causal_lm_forward}, policy=policy, target_key="BaichuanForCausalLM"
-        )
+        method_replacement = {
+            "forward": BloomInferenceForwards.bloom_for_causal_lm_forward,
+            "prepare_inputs_for_generation": BloomInferenceForwards.bloom_for_causal_lm_prepare_inputs_for_generation,
+        }
         self.append_or_create_method_replacement(
-            description={"forward": llama_model_forward}, policy=policy, target_key="BaichuanModel"
+            description=method_replacement, policy=policy, target_key=BloomForCausalLM
         )
+
+        method_replacement = {"forward": BloomInferenceForwards.bloom_model_forward}
+        self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=BloomModel)
+
+        method_replacement = {"forward": BloomInferenceForwards.bloom_block_forward}
+        self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=BloomBlock)
+
+        method_replacement = {"forward": BloomInferenceForwards.bloom_attention_forward}
         self.append_or_create_method_replacement(
-            description={"forward": baichuan_rmsnorm_forward}, policy=policy, target_key="RMSNorm"
+            description=method_replacement, policy=policy, target_key=BloomAttention
         )
 
-        return policy
+        if HAS_TRITON_NORM:
+            infer_method = get_triton_layernorm_forward()
+            method_replacement = {"forward": partial(infer_method)}
+            self.append_or_create_method_replacement(
+                description=method_replacement, policy=policy, target_key=LayerNorm
+            )
 
-    def postprocess(self):
-        init_to_get_rotary(self.model.model)
-        return self.model
-
-    def to_rpc_param(self) -> str:
-        return __class__.__name__
-
-    @staticmethod
-    def from_rpc_param() -> "NoPaddingBaichuanModelInferPolicy":
-        return NoPaddingBaichuanModelInferPolicy()
+        return policy
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/inference/modeling/policy/nopadding_llama.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/llama.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,112 +1,121 @@
-from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaForCausalLM, LlamaModel, LlamaRMSNorm
+from functools import partial
+
+import torch
+from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaModel, LlamaRMSNorm
 
-from colossalai.inference.config import RPC_PARAM
-from colossalai.inference.modeling.models.nopadding_llama import (
-    NopadLlamaAttention,
-    NopadLlamaMLP,
-    llama_causal_lm_forward,
-    llama_decoder_layer_forward,
-    llama_model_forward,
-    llama_rmsnorm_forward,
-)
-from colossalai.inference.utils import init_to_get_rotary
-from colossalai.shardformer.layer import Linear1D_Col, Linear1D_Row
 from colossalai.shardformer.policies.base_policy import ModulePolicyDescription, SubModuleReplacementDescription
+
+# import colossalai
 from colossalai.shardformer.policies.llama import LlamaForCausalLMPolicy
 
+from ..modeling._utils import init_to_get_rotary
+from ..modeling.llama import LlamaInferenceForwards
+
+try:
+    from lightllm.models.llama.triton_kernel.rmsnorm import rmsnorm_forward as lightllm_rmsnorm_forward
+
+    HAS_TRITON_RMSNORM = True
+except:
+    print("you should install triton from https://github.com/openai/triton")
+    HAS_TRITON_RMSNORM = False
+
+
+def get_triton_rmsnorm_forward():
+    if HAS_TRITON_RMSNORM:
 
-class NoPaddingLlamaModelInferPolicy(LlamaForCausalLMPolicy, RPC_PARAM):
+        def _triton_rmsnorm_forward(self: LlamaRMSNorm, hidden_states: torch.Tensor):
+            return lightllm_rmsnorm_forward(hidden_states, self.weight.data, self.variance_epsilon)
+
+        return _triton_rmsnorm_forward
+    else:
+        return None
+
+
+class LlamaModelInferPolicy(LlamaForCausalLMPolicy):
     def __init__(self) -> None:
         super().__init__()
 
     def module_policy(self):
         policy = super().module_policy()
 
-        if self.shard_config.enable_tensor_parallelism:
+        if self.shard_config.extra_kwargs.get("inference_gptq", False):
+            from colossalai.inference.quant.gptq.cai_gptq import ColCaiQuantLinear, RowCaiQuantLinear
+
             decoder_attribute_replacement = {
                 "self_attn.hidden_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
                 "self_attn.num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
             }
-            if getattr(self.model.config, "num_key_value_heads", False):
-                decoder_attribute_replacement["self_attn.num_key_value_heads"] = (
-                    self.model.config.num_key_value_heads // self.shard_config.tensor_parallel_size
-                )
-        else:
-            decoder_attribute_replacement = None
-
-        policy[LlamaDecoderLayer] = ModulePolicyDescription(
-            attribute_replacement=decoder_attribute_replacement,
-            sub_module_replacement=[
-                SubModuleReplacementDescription(
-                    suffix="mlp.gate_proj",
-                    target_module=Linear1D_Col,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="mlp.up_proj",
-                    target_module=Linear1D_Col,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="mlp.down_proj",
-                    target_module=Linear1D_Row,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="mlp",
-                    target_module=NopadLlamaMLP,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="self_attn.q_proj",
-                    target_module=Linear1D_Col,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="self_attn.k_proj",
-                    target_module=Linear1D_Col,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="self_attn.v_proj",
-                    target_module=Linear1D_Col,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="self_attn.o_proj",
-                    target_module=Linear1D_Row,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="self_attn",
-                    target_module=NopadLlamaAttention,
-                ),
-            ],
-        )
+            policy[LlamaDecoderLayer] = ModulePolicyDescription(
+                attribute_replacement=decoder_attribute_replacement,
+                sub_module_replacement=[
+                    SubModuleReplacementDescription(
+                        suffix="self_attn.q_proj",
+                        target_module=ColCaiQuantLinear,
+                        kwargs={"split_num": 1},
+                    ),
+                    SubModuleReplacementDescription(
+                        suffix="self_attn.k_proj",
+                        target_module=ColCaiQuantLinear,
+                        kwargs={"split_num": 1},
+                    ),
+                    SubModuleReplacementDescription(
+                        suffix="self_attn.v_proj",
+                        target_module=ColCaiQuantLinear,
+                        kwargs={"split_num": 1},
+                    ),
+                    SubModuleReplacementDescription(
+                        suffix="self_attn.o_proj",
+                        target_module=RowCaiQuantLinear,
+                        kwargs={"split_num": 1},
+                    ),
+                    SubModuleReplacementDescription(
+                        suffix="mlp.gate_proj",
+                        target_module=ColCaiQuantLinear,
+                        kwargs={"split_num": 1},
+                    ),
+                    SubModuleReplacementDescription(
+                        suffix="mlp.up_proj",
+                        target_module=ColCaiQuantLinear,
+                        kwargs={"split_num": 1},
+                    ),
+                    SubModuleReplacementDescription(
+                        suffix="mlp.down_proj",
+                        target_module=RowCaiQuantLinear,
+                        kwargs={"split_num": 1},
+                    ),
+                ],
+            )
+
+        self.shard_config._infer()
+
+        infer_forward = LlamaInferenceForwards.llama_model_forward
+        method_replacement = {"forward": partial(infer_forward)}
+        self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=LlamaModel)
 
-        policy[LlamaForCausalLM] = ModulePolicyDescription(
-            sub_module_replacement=[
-                SubModuleReplacementDescription(
-                    suffix="lm_head", target_module=Linear1D_Col, kwargs={"gather_output": True}
-                )
-            ],
-        )
-
-        # self.shard_config._infer()
-        self.append_or_create_method_replacement(
-            description={"forward": llama_causal_lm_forward}, policy=policy, target_key=LlamaForCausalLM
-        )
-        self.append_or_create_method_replacement(
-            description={"forward": llama_model_forward}, policy=policy, target_key=LlamaModel
-        )
+        infer_forward = LlamaInferenceForwards.llama_decoder_layer_forward
+        method_replacement = {"forward": partial(infer_forward)}
         self.append_or_create_method_replacement(
-            description={"forward": llama_decoder_layer_forward}, policy=policy, target_key=LlamaDecoderLayer
+            description=method_replacement, policy=policy, target_key=LlamaDecoderLayer
         )
+
+        infer_forward = LlamaInferenceForwards.llama_flash_attn_kvcache_forward
+        method_replacement = {"forward": partial(infer_forward)}
         self.append_or_create_method_replacement(
-            description={"forward": llama_rmsnorm_forward}, policy=policy, target_key=LlamaRMSNorm
+            description=method_replacement, policy=policy, target_key=LlamaAttention
         )
 
+        infer_forward = None
+        if HAS_TRITON_RMSNORM:
+            infer_forward = get_triton_rmsnorm_forward()
+
+        if infer_forward is not None:
+            method_replacement = {"forward": partial(infer_forward)}
+            self.append_or_create_method_replacement(
+                description=method_replacement, policy=policy, target_key=LlamaRMSNorm
+            )
+
         return policy
 
     def postprocess(self):
-        init_to_get_rotary(self.model.model, self.model.config.rope_theta)
+        init_to_get_rotary(self.model.model)
         return self.model
-
-    def to_rpc_param(self) -> str:
-        return __class__.__name__
-
-    @staticmethod
-    def from_rpc_param() -> "NoPaddingLlamaModelInferPolicy":
-        return NoPaddingLlamaModelInferPolicy()
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/inference/struct.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/io_struct.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,180 +1,166 @@
-import enum
-from dataclasses import dataclass
-from typing import Any, List
+# Adapted from https://github.com/ModelTC/lightllm
 
-from colossalai.logging import get_dist_logger
+from typing import Dict, List, Tuple
 
-logger = get_dist_logger(__name__)
+from .sampling_params import SamplingParams
 
-"""
-The abstraction of request and sequence are defined here.
-"""
-
-
-class RequestStatus(enum.Enum):
-    """
-    The status of Sentences
-    """
-
-    # running status
-    WAITING = enum.auto()
-    RUNNING = enum.auto()
-    ABORTED = enum.auto()
-
-    # completion status
-    OVERLENGTH = enum.auto()
-    COMPLETED = enum.auto()
-    LENGTH_CAPPED = enum.auto()
-
-    # recycle status
-    RECYCLED = enum.auto()
-
-    @staticmethod
-    def is_finished(status: "RequestStatus") -> bool:
-        return status in [
-            RequestStatus.OVERLENGTH,
-            RequestStatus.COMPLETED,
-            RequestStatus.LENGTH_CAPPED,
-        ]
-
-    @staticmethod
-    def is_running(status: "RequestStatus") -> bool:
-        return status == RequestStatus.RUNNING
-
-    @staticmethod
-    def is_waiting(status: "RequestStatus") -> bool:
-        return status == RequestStatus.WAITING
-
-
-@dataclass
-class Sequence:
-    """Store information of input sequence.
-
-    Args:
-        request_id (int): The ID of input sequence.
-        prompt (str): The prompt of input sequence.
-        input_token_id (List[int]): The tokens ID of input sequence.
-        block_size (int): The block size of input sequence.
-        sample_params (SampleParams): The sample_params of input sequence.
-        block_table (torch.Tensor): The index of input sequence in block_table.
-        eos_token_id (int): The eos token id for this inference process.
-        pad_token_id (int): The pad token id for this inference process.
-        max_output_len (int): Maximum output length.
-        ignore_eos(bool): Whether to ignore the EOS token and continue generating tokens when encountering the EOS token.
-        output(str): The output of sequence
-    """
-
-    request_id: int
-    prompt: str
-    input_token_id: List[int]
-    block_size: int
-    sample_params: Any  # SampleParams needs to be imported later.
-    eos_token_id: int
-    pad_token_id: int
-    max_output_len: int = 256
-    # NOTE(caidi) This is a temporary solution. It's better to move the logic to turn on or off the flag in sampling module in future.
-    ignore_eos: bool = False
-    output: str = None
-
-    def __post_init__(self):
-        self.output_token_id = []
-        self.status = RequestStatus.WAITING
-
-    @property
-    def sentence_len(self) -> int:
-        """
-        Get length of current sentence.
-        """
-        return len(self.input_token_id) + len(self.output_token_id)
-
-    @property
-    def input_len(self) -> int:
-        """
-        Get length of input sentence.
-        """
-        return len(self.input_token_id)
-
-    @property
-    def output_len(self) -> int:
-        """
-        Get length of output sentence.
-        """
-        return len(self.output_token_id)
-
-    def check_finish(self) -> bool:
-        """
-        Check whether the inference is finished.
-
-        Returns:
-            bool: Whether the inference is finished.
-        """
-        if RequestStatus.is_finished(self.status):
-            return True
-
-        if self.output_token_id:
-            if (
-                self.output_token_id[-1] == self.eos_token_id and not self.ignore_eos
-            ) or self.output_len >= self.max_output_len:
-                self.status = RequestStatus.COMPLETED
-                return True
 
+class Req:
+    def __init__(self, request_id, prompt_ids, sample_params: SamplingParams, prompts: str = ""):
+        self.request_id = request_id
+        self.prompt_ids = prompt_ids
+        self.input_len = len(prompt_ids)
+        self.max_output_len = sample_params.max_new_tokens
+        self.sample_params = sample_params
+        self.output_ids = []
+        self.output_metadata_list = []
+        self.has_generate_finished = False
+        self.aborted = False
+        self.prompts = prompts
+
+    def to_rpc_obj(self):
+        return {
+            "request_id": self.request_id,
+            "input_id": self.prompt_ids,
+            "output_len": self.max_output_len,
+            "sampling_param": self.sample_params.to_dict(),
+        }
+
+    def stop_sequences_matched(self):
+        # should we add stpp sequences to the sample params?
+        if self.sample_params.stop_sequences is not None:
+            for stop_token_ids in self.sample_params.stop_sequences:
+                stop_len = len(stop_token_ids)
+                if (
+                    stop_len > 0
+                    and len(self.output_ids) >= stop_len
+                    and all(self.output_ids[-(stop_len - i)] == stop_token_ids[i] for i in range(stop_len))
+                ):
+                    return True
         return False
 
-    def revoke_finished_status(self) -> None:
-        """
-        Revoke the finished status of the sequence.
-        This is only used by speculative decoding for now.
-        """
-        if RequestStatus.is_finished(self.status):
-            self.status = RequestStatus.RUNNING
+    def __repr__(self):
+        return f"request_id(n={self.request_id}, " f"prompt_ids={self.prompt_ids}, "
 
-    def __hash__(self):
-        return hash(self.request_id)
 
-    def mark_running(self) -> None:
-        """
-        Set status for prefill reqs.
-        """
-        assert (
-            self.status == RequestStatus.WAITING or RequestStatus.RECYCLED
-        ), "Sequence is not in WAITTING/RECYCLED STATUS"
-        self.status = RequestStatus.RUNNING
+class Batch:
+    def __init__(self, batch_id, reqs: List[Req]):
+        self.batch_id = batch_id
+        self.reqs = reqs
+        self.id_to_reqs = {req.request_id: req for req in reqs}
+
+    def input_tokens(self):
+        batch_input_tokens = 0
+        for req in self.reqs:
+            batch_input_tokens += req.input_len
+        return batch_input_tokens
+
+    def calcu_max_tokens(self):
+        tokens = 0
+        for req in self.reqs:
+            tokens += req.input_len + req.max_output_len
+        return tokens
+
+    def calcu_used_tokens(self):
+        tokens = 0
+        for req in self.reqs:
+            tokens += req.input_len + len(req.output_ids)
+        return tokens
+
+    def mark_finished_req(self, eos_id, engine_max_output_len):
+        has_new_finish = False
+        for req in self.reqs:
+            if req.stop_sequences_matched():
+                req.has_generate_finished = True
+                has_new_finish = True
+            if len(req.output_ids) >= engine_max_output_len:
+                req.has_generate_finished = True
+                has_new_finish = True
+            if req.output_ids[-1] == eos_id and req.sample_params.ignore_eos == False:
+                req.has_generate_finished = True
+                has_new_finish = True
+            if len(req.output_ids) >= req.max_output_len or req.aborted:
+                req.has_generate_finished = True
+                has_new_finish = True
+        return has_new_finish
+
+    def filter_finished(self) -> List[Req]:
+        """
+        Filter finished requests from the batch, the finished ones will be removed from 'reqs'.
+        """
+        # TODO: the logic of return should be defined here.
+        unfinished_req = []
+        finished_req = []
+        for req in self.reqs:
+            if not req.has_generate_finished:
+                unfinished_req.append(req)
+            else:
+                finished_req.append(req)
+        self.reqs = unfinished_req
+        self.id_to_reqs = {req.request_id: req for req in self.reqs}
+        return finished_req
+
+    def is_clear(self):
+        return len(self.reqs) == 0
+
+    def merge(self, mini_batch):
+        for _req in mini_batch.reqs:
+            self.reqs.append(_req)
+        self.id_to_reqs = {req.request_id: req for req in self.reqs}
+        return
+
+    def __repr__(self):
+        return f"batch_id={self.batch_id}, " f"reqs={self.reqs}, "
+
+    def __len__(self):
+        return len(self.reqs)
+
+
+class BatchTokenIdOut:
+    def __init__(self):
+        self.reqs_infs: List[
+            Tuple[str, int, Dict, bool, bool]
+        ] = []  # [req_id, new_token_id, gen_metadata, finished_state, abort_state]
+
+
+class BatchStrOut:
+    def __init__(self):
+        self.reqs_infs: List[
+            Tuple[str, str, Dict, bool, bool]
+        ] = []  # [req_id, token_str, gen_metadata, finished_state, abort_state]
+
+
+class AbortReq:
+    def __init__(self, req_id):
+        self.req_id = req_id
 
-    def mark_finished(self) -> None:
-        """
-        Set status for finished reqs.
-        """
-        self.status = RequestStatus.COMPLETED
 
-    def mark_aborted(self) -> None:
-        """
-        Set status for aborted reqs.
-        """
-        self.status = RequestStatus.ABORTED
+class RequestOutput:
+    """The output data of a request to the LLM.
 
-    def recycle(self) -> None:
-        """
-        Recycle a running sequnce to waiitting list
-        """
-        assert (
-            not self.check_finish() and not self.status == RequestStatus.ABORTED
-        ), "The running sequence \
-        is already done but it still in running list"
-        self.status = RequestStatus.RECYCLED
+    Args:
+        request_id: The unique ID of the request.
+        prompt: The prompt string of the request.
+        prompt_token_ids: The token IDs of the prompt.
+        outputs: The output sequences of the request.
+    """
+
+    def __init__(
+        self,
+        request_id: str,
+        prompt: str,
+        prompt_token_ids: List[int],
+        outputs,
+    ) -> None:
+        self.request_id = request_id
+        self.prompt = prompt
+        self.prompt_token_ids = prompt_token_ids
+        self.outputs = outputs
 
     def __repr__(self) -> str:
         return (
-            f"(request_id={self.request_id}, "
-            f"prompt={self.prompt},\n"
-            f"output_token_id={self.output_token_id},\n"
-            f"output={self.output},\n"
-            f"status={self.status.name},\n"
-            f"sample_params={self.sample_params},\n"
-            f"input_len={self.input_len},\n"
-            f"output_len={self.output_len})\n"
+            f"RequestOutput(request_id={self.request_id}, "
+            f"prompt={self.prompt!r}, "
+            f"prompt_token_ids={self.prompt_token_ids}, "
+            f"outputs={self.outputs}, "
         )
-
-
-def _pad_to_max(x: List[int], max_len: int, pad: int) -> List[int]:
-    assert len(x) <= max_len
-    return [pad] * (max_len - len(x)) + x
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/inference/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,115 +1,134 @@
-"""
-Utils for model inference
-"""
 import os
-import re
-from pathlib import Path
-from typing import Optional, Tuple
+import threading
+import time
+from enum import Enum
+from typing import List
 
 import torch
-from torch import nn
 
-from colossalai.testing import free_port
-
-
-def init_to_get_rotary(self, base=10000, use_elem=False):
-    """
-    This function initializes the rotary positional embedding, it is compatible for all models and is called in ShardFormer
-    Args:
-        self : Model that holds the rotary positional embedding
-        base : calculation arg
-        use_elem : activated when using chatglm-based models
-    """
-    self.config.head_dim_ = self.config.hidden_size // self.config.num_attention_heads
-    if not hasattr(self.config, "rope_scaling"):
-        rope_scaling_factor = 1.0
-    else:
-        rope_scaling_factor = self.config.rope_scaling.factor if self.config.rope_scaling is not None else 1.0
-
-    if hasattr(self.config, "max_sequence_length"):
-        max_seq_len = self.config.max_sequence_length
-    elif hasattr(self.config, "max_position_embeddings"):
-        max_seq_len = self.config.max_position_embeddings * rope_scaling_factor
-    else:
-        max_seq_len = 2048 * rope_scaling_factor
-    base = float(base)
-
-    # NTK  ref: https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
-    ntk_alpha = os.environ.get("INFER_NTK_ALPHA", None)
-
-    if ntk_alpha is not None:
-        ntk_alpha = float(ntk_alpha)
-        assert ntk_alpha >= 1, "NTK alpha must be greater than or equal to 1"
-        if ntk_alpha > 1:
-            print(f"Note: NTK enabled, alpha set to {ntk_alpha}")
-        max_seq_len *= ntk_alpha
-        base = base * (ntk_alpha ** (self.head_dim_ / (self.head_dim_ - 2)))  # Base change formula
-
-    n_elem = self.config.head_dim_
-    if use_elem:
-        n_elem //= 2
-
-    inv_freq = 1.0 / (base ** (torch.arange(0, n_elem, 2, device="cpu", dtype=torch.float32) / n_elem))
-    t = torch.arange(max_seq_len + 1024 * 64, device="cpu", dtype=torch.float32) / rope_scaling_factor
-    freqs = torch.outer(t, inv_freq)
-
-    self._cos_cached = torch.cos(freqs).to(self.dtype).cuda()
-    self._sin_cached = torch.sin(freqs).to(self.dtype).cuda()
-
-
-def has_index_file(checkpoint_path: str) -> Tuple[bool, Optional[Path]]:
-    """
-    Check whether the checkpoint has an index file.
-
-    Args:
-        checkpoint_path (str): path to the checkpoint.
-
-    Returns:
-        Tuple[bool, Optional[Path]]: a tuple of (has_index_file, index_file_path)
-    """
-    checkpoint_path = Path(checkpoint_path)
-    if checkpoint_path.is_file():
-        # check if it is .index.json
-        reg = re.compile("(.*?).index((\..*)?).json")
-        if reg.fullmatch(checkpoint_path.name) is not None:
-            return True, checkpoint_path
-        else:
-            return False, None
-    elif checkpoint_path.is_dir():
-        index_files = list(checkpoint_path.glob("*.index.*json"))
-
-        for index_file in index_files:
-            if "safetensors" in index_file.__str__():
-                return True, index_file.__str__()  # return the safetensors file first
-
-        if len(index_files) == 1:
-            return True, index_files[0]
-        else:
-            assert (
-                len(index_files) == 1
-            ), f"Expected to find one .index.json file in {checkpoint_path}, but found {len(index_files)}"
-            return False, None
-    else:
-        raise RuntimeError(f"Invalid checkpoint path {checkpoint_path}. Expected a file or a directory.")
-
-
-def get_model_size(model: nn.Module):
-    """Calculates the total size of the model weights (including biases) in bytes.
-    Args:
-        model: The PyTorch model to analyze.
-    Returns:
-        The total size of the model weights in bytes.
-    """
-    total_size = 0
-    for key, param in model.named_parameters():
-        total_size += param.element_size() * param.numel()
-    return total_size / (1024**3)
-
-
-def find_available_ports(num: int):
-    try:
-        free_ports = [free_port() for i in range(num)]
-    except OSError as e:
-        print(f"An OS error occurred: {e}")
-        raise RuntimeError("Error finding available ports")
-    return free_ports
+from colossalai.gemini.ophooks import BaseOpHook
+from colossalai.gemini.stateful_tensor import StatefulTensor
+from colossalai.legacy.engine import Engine
+from colossalai.legacy.utils.profiler.extention import ProfilerExtension
+
+
+class DeviceType(Enum):
+    CPU = 0
+    CUDA = 1
+
+
+def get_timestamp_us():
+    return int(time.time() * 1e6)
+
+
+def generic_instant_event(name, pid, tid, timestamp, args):
+    return {"ph": "i", "s": "t", "name": name, "pid": pid, "tid": tid, "ts": timestamp, "args": args}
+
+
+class StatefulTensorMemoryEvent:
+    EVENT_NAME = "[statefulTensorMemory]"
+
+    def __init__(self, timestamp: int, device_type: DeviceType, bytes_: int) -> None:
+        self.pid = os.getpid()
+        self.tid = threading.get_ident()
+        self.timestamp = timestamp
+        self.device_type = device_type
+        self.device_id = torch.cuda.current_device() if device_type == DeviceType.CUDA else -1
+        self.bytes = bytes_
+
+    def state_dict(self):
+        return generic_instant_event(
+            StatefulTensorMemoryEvent.EVENT_NAME,
+            self.pid,
+            self.tid,
+            self.timestamp,
+            {"Device Type": self.device_type.value, "Device Id": self.device_id, "Bytes": self.bytes},
+        )
+
+
+class StatefulTensorMemoryTracer:
+    def __init__(self) -> None:
+        self.events: List[StatefulTensorMemoryEvent] = []
+        self._tracing = False
+
+    def sample(self):
+        cuda_mem = StatefulTensor.GST_MGR.total_mem["cuda"]
+        cpu_mem = StatefulTensor.GST_MGR.total_mem["cpu"]
+        timestamp = get_timestamp_us()
+        if self._tracing:
+            self.events.append(StatefulTensorMemoryEvent(timestamp, DeviceType.CUDA, cuda_mem))
+            self.events.append(StatefulTensorMemoryEvent(timestamp, DeviceType.CPU, cpu_mem))
+
+    def start_trace(self):
+        self.events.clear()
+        self._tracing = True
+
+    def stop_trace(self):
+        self._tracing = False
+
+    def state_dict(self):
+        return [event.state_dict() for event in self.events]
+
+
+class StatefulTensorMemoryTracerHook(BaseOpHook):
+    def __init__(self, tracer: StatefulTensorMemoryTracer):
+        super().__init__()
+        self.tracer = tracer
+        self._enable = False
+
+    def pre_fwd_exec(self, module: torch.nn.Module, *args):
+        if self._enable:
+            self.tracer.sample()
+
+    def post_fwd_exec(self, module: torch.nn.Module, *args):
+        if self._enable:
+            self.tracer.sample()
+
+    def pre_bwd_exec(self, module: torch.nn.Module, input_, output):
+        if self._enable:
+            self.tracer.sample()
+
+    def post_bwd_exec(self, module: torch.nn.Module, input_):
+        if self._enable:
+            self.tracer.sample()
+
+    def post_iter(self):
+        if self._enable:
+            self.tracer.sample()
+
+    def enable(self):
+        self._enable = True
+
+    def disable(self):
+        self._enable = False
+
+
+class StatefulTensorMemoryProfilerExtention(ProfilerExtension):
+    def __init__(self, engine: Engine) -> None:
+        self.engine = engine
+        self.tracer = StatefulTensorMemoryTracer()
+        self.hook = StatefulTensorMemoryTracerHook(self.tracer)
+        self.hook_registered = False
+
+    def prepare_trace(self):
+        self.hook.enable()
+        if not self.hook_registered:
+            self.engine.add_hook(self.hook)
+            self.hook_registered = True
+
+    def start_trace(self):
+        self.prepare_trace()
+        self.tracer.start_trace()
+
+    def stop_trace(self):
+        self.tracer.stop_trace()
+        self.hook.disable()
+        if self.hook_registered:
+            self.engine.remove_hook(self.hook)
+            # remove_hook is not implemented now
+            # FIXME(ver217): uncomment below line when remove_hook is implemented
+            # self.hook_registered = False
+
+    def extend_chrome_trace(self, trace: dict) -> dict:
+        trace["traceEvents"].extend(self.tracer.state_dict())
+        return trace
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/initialize.py` & `colossalai-nightly-2024.5.4/colossalai/initialize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/interface/model.py` & `colossalai-nightly-2024.5.4/colossalai/interface/model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,39 +1,32 @@
-from .pybind.cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
-from .pybind.flash_attention import (
-    FlashAttentionDaoCudaExtension,
-    FlashAttentionNpuExtension,
-    FlashAttentionSdpaCudaExtension,
-)
-from .pybind.inference import InferenceOpsCudaExtension
-from .pybind.layernorm import LayerNormCudaExtension
-from .pybind.moe import MoeCudaExtension
-from .pybind.optimizer import FusedOptimizerCudaExtension
-from .pybind.softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
+from .cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
+from .flash_attention import FlashAttentionDaoCudaExtension, FlashAttentionNpuExtension, FlashAttentionSdpaCudaExtension
+from .layernorm import LayerNormCudaExtension
+from .moe import MoeCudaExtension
+from .optimizer import FusedOptimizerCudaExtension
+from .softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
 
 ALL_EXTENSIONS = [
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     FusedOptimizerCudaExtension,
-    InferenceOpsCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
     FlashAttentionDaoCudaExtension,
     FlashAttentionSdpaCudaExtension,
     FlashAttentionNpuExtension,
 ]
 
 __all__ = [
     "CpuAdamArmExtension",
     "CpuAdamX86Extension",
     "LayerNormCudaExtension",
     "MoeCudaExtension",
     "FusedOptimizerCudaExtension",
-    "InferenceOpsCudaExtension",
     "ScaledMaskedSoftmaxCudaExtension",
     "ScaledUpperTriangleMaskedSoftmaxCudaExtension",
     "FlashAttentionDaoCudaExtension",
     "FlashAttentionSdpaCudaExtension",
     "FlashAttentionNpuExtension",
 ]
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/base_extension.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/base_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/cpp_extension.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpp_extension.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,17 +21,14 @@
         self.prebuilt_module_path = "colossalai._C"
         self.prebuilt_import_path = f"{self.prebuilt_module_path}.{self.name}"
         self.version_dependent_macros = ["-DVERSION_GE_1_1", "-DVERSION_GE_1_3", "-DVERSION_GE_1_5"]
 
     def csrc_abs_path(self, path):
         return os.path.join(self.relative_to_abs_path("csrc"), path)
 
-    def pybind_abs_path(self, path):
-        return os.path.join(self.relative_to_abs_path("pybind"), path)
-
     def relative_to_abs_path(self, code_path: str) -> str:
         """
         This function takes in a path relative to the colossalai root directory and return the absolute path.
         """
 
         # get the current file path
         # iteratively check the parent directory
@@ -115,15 +112,14 @@
         """
 
     @abstractmethod
     def include_dirs(self) -> List[str]:
         """
         This function should return a list of include files for extensions.
         """
-        return [self.csrc_abs_path("")]
 
     @abstractmethod
     def cxx_flags(self) -> List[str]:
         """
         This function should return a list of cxx compilation flags for extensions.
         """
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.h` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/layer_norm_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu`

 * *Files 6% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 
 #include "ATen/ATen.h"
 #include "ATen/AccumulateType.h"
 #include "ATen/cuda/CUDAContext.h"
 #include "ATen/cuda/DeviceUtils.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 template <typename U>
 __device__ void cuWelfordOnlineSum(const U curr, U& mu, U& sigma2, U& count) {
   count = count + U(1);
   U delta = curr - mu;
   U lmean = mu + delta / count;
   mu = lmean;
@@ -602,19 +602,19 @@
 #else
                      at::IntList normalized_shape,
 #endif
                      at::Tensor* gamma, at::Tensor* beta, double epsilon) {
   using namespace at;
   DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
       input->scalar_type(), output->scalar_type(), "cuda_layer_norm_kernel",
-      HostApplyLayerNorm(output->data_ptr<scalar_t_out>(),
-                         mean->data_ptr<float>(), invvar->data_ptr<float>(),
-                         input->data_ptr<scalar_t_in>(), n1, n2, epsilon,
-                         gamma != NULL ? gamma->data_ptr<scalar_t_out>() : NULL,
-                         beta != NULL ? beta->data_ptr<scalar_t_out>() : NULL);)
+      HostApplyLayerNorm(output->DATA_PTR<scalar_t_out>(),
+                         mean->DATA_PTR<float>(), invvar->DATA_PTR<float>(),
+                         input->DATA_PTR<scalar_t_in>(), n1, n2, epsilon,
+                         gamma != NULL ? gamma->DATA_PTR<scalar_t_out>() : NULL,
+                         beta != NULL ? beta->DATA_PTR<scalar_t_out>() : NULL);)
 }
 
 template <typename T, typename U, typename V>
 void HostLayerNormGradient(const V* dout, const U* mean, const U* invvar,
                            at::Tensor* input, int n1, int n2, const V* gamma,
                            const V* beta, double epsilon, T* grad_input,
                            V* grad_gamma, V* grad_beta) {
@@ -629,33 +629,33 @@
         2 * sizeof(U) * threads2.y * threads2.y * (threads2.x + 1);
     const int nshared2_b = threads2.x * threads2.y * sizeof(U);
     const int nshared2 = nshared2_a > nshared2_b ? nshared2_a : nshared2_b;
     at::Tensor part_grad_gamma = at::empty(
         {part_size, n2}, input->options().dtype(at::ScalarType::Float));
     at::Tensor part_grad_beta = at::empty_like(part_grad_gamma);
     cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
-        dout, input->data_ptr<T>(), n1, n2, mean, invvar, U(epsilon),
-        part_grad_gamma.data_ptr<U>(), part_grad_beta.data_ptr<U>());
+        dout, input->DATA_PTR<T>(), n1, n2, mean, invvar, U(epsilon),
+        part_grad_gamma.DATA_PTR<U>(), part_grad_beta.DATA_PTR<U>());
 
     const dim3 threads3(32, 8, 1);
     const dim3 blocks3((n2 + threads2.x - 1) / threads2.x, 1, 1);
     const int nshared3 = threads3.x * threads3.y * sizeof(U);
     cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
-        part_grad_gamma.data_ptr<U>(), part_grad_beta.data_ptr<U>(), part_size,
+        part_grad_gamma.DATA_PTR<U>(), part_grad_beta.DATA_PTR<U>(), part_size,
         n1, n2, grad_gamma, grad_beta);
   }
 
   // compute grad_input
   const uint64_t maxGridY =
       at::cuda::getCurrentDeviceProperties()->maxGridSize[1];
   const dim3 blocks1(1, std::min((uint64_t)n1, maxGridY), 1);
   const dim3 threads1(32, 4, 1);
   int nshared = threads1.y > 1 ? threads1.y * threads1.x * sizeof(U) : 0;
   cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
-      dout, input->data_ptr<T>(), n1, n2, mean, invvar, U(epsilon), gamma,
+      dout, input->DATA_PTR<T>(), n1, n2, mean, invvar, U(epsilon), gamma,
       grad_input);
 }
 
 void cuda_layer_norm_gradient(at::Tensor* dout, at::Tensor* mean,
                               at::Tensor* invvar, at::Tensor* input, int n1,
                               int n2,
 #ifdef VERSION_GE_1_1
@@ -667,17 +667,17 @@
                               double epsilon, at::Tensor* grad_input,
                               at::Tensor* grad_gamma, at::Tensor* grad_beta) {
   using namespace at;
   DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
       input->scalar_type(), gamma->scalar_type(),
       "cuda_layer_norm_gradient_kernel",
       HostLayerNormGradient(
-          dout->data_ptr<scalar_t_out>(), mean->data_ptr<float>(),
-          invvar->data_ptr<float>(), input, n1, n2,
+          dout->DATA_PTR<scalar_t_out>(), mean->DATA_PTR<float>(),
+          invvar->DATA_PTR<float>(), input, n1, n2,
           // TMJ pass NULL argument for gamma, beta, grad_gamma and grad_beta
           // if gamma Tensor is NULL on input.
-          gamma != NULL ? gamma->data_ptr<scalar_t_out>() : NULL,
-          gamma != NULL ? beta->data_ptr<scalar_t_out>() : NULL, epsilon,
-          grad_input->data_ptr<scalar_t_in>(),
-          gamma != NULL ? grad_gamma->data_ptr<scalar_t_out>() : NULL,
-          gamma != NULL ? grad_beta->data_ptr<scalar_t_out>() : NULL);)
+          gamma != NULL ? gamma->DATA_PTR<scalar_t_out>() : NULL,
+          gamma != NULL ? beta->DATA_PTR<scalar_t_out>() : NULL, epsilon,
+          grad_input->DATA_PTR<scalar_t_in>(),
+          gamma != NULL ? grad_gamma->DATA_PTR<scalar_t_out>() : NULL,
+          gamma != NULL ? grad_beta->DATA_PTR<scalar_t_out>() : NULL);)
 }
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/moe_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,14 @@
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <torch/extension.h>
 
 #include <cub/cub.cuh>
 
-#include "funcs/reduce_function.h"
-
-using colossalAI::funcs::block_reduce;
-using colossalAI::funcs::ReduceType;
+#include "block_reduce.h"
 
 template <typename T, int block_size, int pack_size>
 __device__ void moe_dpch_one_fwd(T *src_row, T *dst_row, const int cols) {
   assert(cols % pack_size == 0);
   const int bpack_size = block_size * pack_size;
 
   typedef cub::BlockLoad<T, block_size, pack_size, cub::BLOCK_LOAD_VECTORIZE>
@@ -156,15 +153,16 @@
     for (int i = 0; i < pack_size; ++i) {
       thread_sum += grad[i] * tokens[i];
       grad[i] *= weight;
     }
 
     BlockStore(ts_store).Store(src_row + idx, grad);
   }
-  block_reduce<float, ReduceType::kSum, 1>(&thread_sum);
+
+  blockReduce<ReduceType::kSum, 1>(&thread_sum);
 
   if (threadIdx.x == 0) *weight_grad = static_cast<T>(thread_sum);
 }
 
 template <typename T, int block_size, int pack_size>
 __device__ void moe_cb_two_fwd(T *src_row1, T *src_row2, T *dst_row,
                                const T weight1, const T weight2,
@@ -228,15 +226,15 @@
       sgrad2[i] = weight2 * grad[i];
     }
 
     BlockStore(ts_store).Store(src_row1 + idx, sgrad1);
     BlockStore(ts_store).Store(src_row2 + idx, sgrad2);
   }
 
-  block_reduce<float, ReduceType::kSum, 2>(thread_sum);
+  blockReduce<ReduceType::kSum, 2>(thread_sum);
 
   if (threadIdx.x == 0)
     *weight_grad1 = static_cast<T>(thread_sum[0]);
   else if (threadIdx.x == 1)
     *weight_grad2 = static_cast<T>(thread_sum[1]);
 }
 
@@ -535,15 +533,15 @@
     cumsum_kernel<1024, 2><<<e, 1024>>>(inputs, outputs, s, e);
   else
     cumsum_kernel<1024, 4><<<e, 1024>>>(inputs, outputs, s, e);
 }
 
 // API FUNCTIONS --------------------------------
 
-#define DISPATCH_FLOAT_AND_HALF_MOE(TYPE, NAME, ...)                   \
+#define DISPATCH_FLOAT_AND_HALF(TYPE, NAME, ...)                       \
   switch (TYPE) {                                                      \
     case at::ScalarType::Float: {                                      \
       using scalar_t = float;                                          \
       __VA_ARGS__;                                                     \
       break;                                                           \
     }                                                                  \
     case at::ScalarType::Half: {                                       \
@@ -561,41 +559,41 @@
                                         torch::Tensor dest_idx) {
   assert(h % 16 == 0);
   auto res = torch::zeros(
       {ec, h},
       torch::dtype(batch_tokens.dtype()).device(batch_tokens.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF_MOE(
+  DISPATCH_FLOAT_AND_HALF(
       batch_tokens.scalar_type(), "moe dispatch forward",
       moe_dpch_fwd_launch<scalar_t>(
-          batch_tokens.data_ptr<scalar_t>(), res.data_ptr<scalar_t>(),
-          mask[0].data_ptr<int>(), k == 1 ? nullptr : mask[1].data_ptr<int>(),
-          dest_idx[0].data_ptr<int>(),
-          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, h));
+          batch_tokens.data<scalar_t>(), res.data<scalar_t>(),
+          mask[0].data<int>(), k == 1 ? nullptr : mask[1].data<int>(),
+          dest_idx[0].data<int>(),
+          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, h));
 
   return res;
 }
 
 torch::Tensor moe_dispatch_cuda_backward(int s, int ec, int h,
                                          torch::Tensor expert_grad,
                                          torch::Tensor mask,
                                          torch::Tensor dest_idx) {
   assert(h % 16 == 0);
   auto res = torch::zeros(
       {s, h}, torch::dtype(expert_grad.dtype()).device(expert_grad.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF_MOE(
+  DISPATCH_FLOAT_AND_HALF(
       expert_grad.scalar_type(), "moe dispatch backward",
       moe_dpch_bwd_launch<scalar_t>(
-          res.data_ptr<scalar_t>(), expert_grad.data_ptr<scalar_t>(),
-          mask[0].data_ptr<int>(), k == 1 ? nullptr : mask[1].data_ptr<int>(),
-          dest_idx[0].data_ptr<int>(),
-          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, h));
+          res.data<scalar_t>(), expert_grad.data<scalar_t>(),
+          mask[0].data<int>(), k == 1 ? nullptr : mask[1].data<int>(),
+          dest_idx[0].data<int>(),
+          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, h));
 
   return res;
 }
 
 torch::Tensor moe_combine_cuda_forward(int s, int e, int c, int h,
                                        torch::Tensor expert_tokens,
                                        torch::Tensor logits, torch::Tensor mask,
@@ -604,21 +602,21 @@
   assert(expert_tokens.dtype() == logits.dtype());
 
   auto res = torch::zeros(
       {s, h},
       torch::dtype(expert_tokens.dtype()).device(expert_tokens.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF_MOE(
+  DISPATCH_FLOAT_AND_HALF(
       expert_tokens.scalar_type(), "moe combine forward",
       moe_cb_fwd_launch<scalar_t>(
-          expert_tokens.data_ptr<scalar_t>(), res.data_ptr<scalar_t>(),
-          logits.data_ptr<scalar_t>(), mask[0].data_ptr<int>(),
-          k == 1 ? nullptr : mask[1].data_ptr<int>(), dest_idx[0].data_ptr<int>(),
-          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, e, c,
+          expert_tokens.data<scalar_t>(), res.data<scalar_t>(),
+          logits.data<scalar_t>(), mask[0].data<int>(),
+          k == 1 ? nullptr : mask[1].data<int>(), dest_idx[0].data<int>(),
+          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, e, c,
           h));
 
   return res;
 }
 
 std::vector<torch::Tensor> moe_combine_cuda_backward(
     int s, int e, int c, int h, torch::Tensor tokens_grad,
@@ -631,31 +629,31 @@
   auto egrad = torch::zeros(
            {e * c, h},
            torch::dtype(tokens_grad.dtype()).device(tokens_grad.device())),
        wgrad = torch::zeros(
            {s, e}, torch::dtype(logits.dtype()).device(logits.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF_MOE(
+  DISPATCH_FLOAT_AND_HALF(
       tokens_grad.scalar_type(), "moe combine backward",
       moe_cb_bwd_launch<scalar_t>(
-          tokens_grad.data_ptr<scalar_t>(), egrad.data_ptr<scalar_t>(),
-          expert_tokens.data_ptr<scalar_t>(), logits.data_ptr<scalar_t>(),
-          wgrad.data_ptr<scalar_t>(), mask[0].data_ptr<int>(),
-          k == 1 ? nullptr : mask[1].data_ptr<int>(), dest_idx[0].data_ptr<int>(),
-          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, e, c,
+          tokens_grad.data<scalar_t>(), egrad.data<scalar_t>(),
+          expert_tokens.data<scalar_t>(), logits.data<scalar_t>(),
+          wgrad.data<scalar_t>(), mask[0].data<int>(),
+          k == 1 ? nullptr : mask[1].data<int>(), dest_idx[0].data<int>(),
+          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, e, c,
           h));
 
   return {egrad, wgrad};
 }
 
 torch::Tensor cumsum_sub_one_in_dim0(torch::Tensor mask) {
   assert(mask.dim() == 2);
   assert(mask.dtype() == torch::kInt32);
 
   const int s = mask.size(0), e = mask.size(1);
   auto res =
       torch::empty({s, e}, torch::dtype(torch::kInt32).device(mask.device()));
-  cumsum_launch(mask.data_ptr<int>(), res.data_ptr<int>(), s, e);
+  cumsum_launch(mask.data<int>(), res.data<int>(), s, e);
 
   return res;
 }
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 typedef enum {
   ADAM_MODE_0 = 0,  // L2 regularization mode
   ADAM_MODE_1 = 1   // Decoupled weight decay mode(AdamW)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <assert.h>
 #include <c10/cuda/CUDAGuard.h>
 
-#include "common/micros.h"
+#include "compat.h"
 
 // #include <iostream>
 
 // This header is the one-stop shop for all your multi-tensor apply needs.
 
 // TODO:  Kernel arg size limit may be <4KB for some other cards (ie Jetson)
 constexpr int depth_to_max_tensors[5] = {110, 64, 48, 36, 30};
@@ -100,15 +100,15 @@
       bool tensors_full = (loc_tensor_info == depth_to_max_tensors[depth - 1] &&
                            chunk == chunks_this_tensor - 1);
       bool blocks_full = (loc_block_info == depth_to_max_blocks[depth - 1]);
       bool last_chunk = (t == ntensors - 1 && chunk == chunks_this_tensor - 1);
       if (tensors_full || blocks_full || last_chunk) {
         // using accscalar_t = acc_type<scalar_t, true>;
         multi_tensor_apply_kernel<<<loc_block_info, block_size, 0, stream>>>(
-            chunk_size, noop_flag.data_ptr<int>(), tl, callable, args...);
+            chunk_size, noop_flag.DATA_PTR<int>(), tl, callable, args...);
 
         AT_CUDA_CHECK(cudaGetLastError());
 
         // Reset.  The control flow possibilities here make my brain hurt.
         loc_block_info = 0;
         if (chunk == chunks_this_tensor - 1) {
           // std::cout << "Hit case 1 " << cond1 << " " << cond2 << " " << cond3
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu`

 * *Files 15% similar despite different names*

```diff
@@ -7,107 +7,19 @@
 #include <c10/cuda/CUDAGuard.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
-
-template <typename T>
-__device__ __forceinline__ T reduce_block_into_lanes(
-    T* x, T val, int lanes = 1,
-    bool share_result = false)  // lanes is intended to be <= 32.
-{
-  int tid = threadIdx.x + threadIdx.y * blockDim.x;
-  int blockSize =
-      blockDim.x * blockDim.y;  // blockSize is intended to be a multiple of 32.
-
-  if (blockSize >= 64) {
-    x[tid] = val;
-    __syncthreads();
-  }
-
-#pragma unroll
-  for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
-    if (tid < i) x[tid] = x[tid] + x[tid + i];
-    __syncthreads();
-  }
-
-  T final;
-
-  if (tid < 32) {
-    if (blockSize >= 64)
-      final = x[tid] + x[tid + 32];
-    else
-      final = val;
-      // __SYNCWARP();
-
-#pragma unroll
-    for (int i = 16; i >= lanes; i >>= 1)
-      final = final + __shfl_down_sync(0xffffffff, final, i);
-  }
-
-  if (share_result) {
-    if (tid < lanes) x[tid] = final;  // EpilogueOp
-    // Make sure the smem result is visible to all warps.
-    __syncthreads();
-  }
-
-  return final;
-}
-
-template <typename T>
-__device__ __forceinline__ T reduce_block_into_lanes_max_op(
-    T* x, T val, int lanes = 1,
-    bool share_result = false)  // lanes is intended to be <= 32.
-{
-  int tid = threadIdx.x + threadIdx.y * blockDim.x;
-  int blockSize =
-      blockDim.x * blockDim.y;  // blockSize is intended to be a multiple of 32.
-
-  if (blockSize >= 64) {
-    x[tid] = val;
-    __syncthreads();
-  }
-
-#pragma unroll
-  for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
-    if (tid < i) x[tid] = fmaxf(fabsf(x[tid]), fabsf(x[tid + i]));
-    __syncthreads();
-  }
-
-  T final;
-
-  if (tid < 32) {
-    if (blockSize >= 64)
-      final = fmaxf(fabsf(x[tid]), fabsf(x[tid + 32]));
-    else
-      final = val;
-      // __SYNCWARP();
-
-#pragma unroll
-    for (int i = 16; i >= lanes; i >>= 1)
-      final =
-          fmaxf(fabsf(final), fabsf(__shfl_down_sync(0xffffffff, final, i)));
-  }
-
-  if (share_result) {
-    if (tid < lanes) x[tid] = final;  // EpilogueOp
-    // Make sure the smem result is visible to all warps.
-    __syncthreads();
-  }
-
-  return final;
-}
-
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
 }
 
 template <typename T>
 __device__ __forceinline__ void load_store(T *dst, T *src, int dst_offset,
@@ -373,32 +285,32 @@
     ret_per_tensor = at::empty({0}, float_options);
   }
 
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "multi_tensor_l2norm_cuda",
       multi_tensor_apply<1>(
           BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-          L2NormFunctor<scalar_t_0>(), output.data_ptr<float>(),
-          per_tensor ? output_per_tensor.data_ptr<float>() : nullptr,
+          L2NormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
+          per_tensor ? output_per_tensor.DATA_PTR<float>() : nullptr,
           per_tensor, max_chunks_per_tensor);)
 
   AT_CUDA_CHECK(cudaGetLastError());
   // AT_CUDA_CHECK(cudaDeviceSynchronize());
 
   // This involves one more small kernel launches, but will be negligible end to
   // end. I could get rid of these by hacking the functor + multi tensor harness
   // with persistence logic, but keeping it simple for now
   auto ret = at::empty({1}, output.options());
   const at::cuda::OptionalCUDAGuard device_guard(device_of(output));
   auto stream = at::cuda::getCurrentCUDAStream();
   cleanup<<<per_tensor ? ntensors : 1, 512, 0, stream>>>(
-      output.data_ptr<float>(),
-      per_tensor ? output_per_tensor.data_ptr<float>() : nullptr,
-      ret.data_ptr<float>(),
-      per_tensor ? ret_per_tensor.data_ptr<float>() : nullptr, per_tensor,
+      output.DATA_PTR<float>(),
+      per_tensor ? output_per_tensor.DATA_PTR<float>() : nullptr,
+      ret.DATA_PTR<float>(),
+      per_tensor ? ret_per_tensor.DATA_PTR<float>() : nullptr, per_tensor,
       max_chunks_per_tensor);
 
   return std::tuple<at::Tensor, at::Tensor>(ret, ret_per_tensor);
 }
 
 // Compute and update grad norm
 // Here use a per tensor norm, and blend new norm(n) and old norm(gn) by
@@ -433,23 +345,23 @@
       at::zeros({ntensors * max_chunks_per_tensor}, float_options);
 
   if (norm_type == 0) {
     DISPATCH_FLOAT_AND_HALF(
         tensor_lists[0][0].scalar_type(), 0, "multi_tensor_maxnorm_cuda",
         multi_tensor_apply<1>(
             BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-            MaxNormFunctor<scalar_t_0>(), output.data_ptr<float>(),
-            output_per_tensor.data_ptr<float>(), true, max_chunks_per_tensor);)
+            MaxNormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
+            output_per_tensor.DATA_PTR<float>(), true, max_chunks_per_tensor);)
   } else {
     DISPATCH_FLOAT_AND_HALF(
         tensor_lists[0][0].scalar_type(), 0, "multi_tensor_l2norm_cuda",
         multi_tensor_apply<1>(
             BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-            L2NormFunctor<scalar_t_0>(), output.data_ptr<float>(),
-            output_per_tensor.data_ptr<float>(), true, max_chunks_per_tensor);)
+            L2NormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
+            output_per_tensor.DATA_PTR<float>(), true, max_chunks_per_tensor);)
   }
   AT_CUDA_CHECK(cudaGetLastError());
 
   // AT_CUDA_CHECK(cudaDeviceSynchronize());
 
   // This involves one more small kernel launches, but will be negligible end to
   // end. I could get rid of these by hacking the functor + multi tensor harness
@@ -458,13 +370,13 @@
 
   // Adding the following device guard since it happens sometimes that the
   // tensors are on one device and the cuda stream is on another device which
   // results in ILLEGAL MEM ACCESS error.
   const at::cuda::OptionalCUDAGuard device_guard(device_of(output));
   auto stream = at::cuda::getCurrentCUDAStream();
   cleanup_v2<<<ntensors, 512, 0, stream>>>(
-      output.data_ptr<float>(), output_per_tensor.data_ptr<float>(),
-      ret.data_ptr<float>(), out.data_ptr<float>(), true, max_chunks_per_tensor,
+      output.DATA_PTR<float>(), output_per_tensor.DATA_PTR<float>(),
+      ret.DATA_PTR<float>(), out.DATA_PTR<float>(), true, max_chunks_per_tensor,
       norm_type, alpha, beta);
 
   return;
 }
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
@@ -329,26 +329,26 @@
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "lamb_stage_1",
       multi_tensor_apply<4>(BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
                             LAMBStage1Functor<scalar_t_0>(), beta1, beta2,
                             beta3,  // 1-beta1 or 1 depends on averaging mode
                             bias_correction1, bias_correction2, epsilon,
                             (adamMode_t)mode, weight_decay,
-                            global_grad_norm.data_ptr<float>(), max_grad_norm);)
+                            global_grad_norm.DATA_PTR<float>(), max_grad_norm);)
 
   // Compute update norms
   auto update_norm_tuple =
       multi_tensor_l2norm_cuda(chunk_size, noop_flag, grad_list, true);
 
   std::vector<std::vector<at::Tensor>> grad_param_list(
       tensor_lists.begin(), tensor_lists.begin() + 2);
 
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "lamb_stage_2",
       multi_tensor_apply<2>(BLOCK_SIZE, chunk_size, noop_flag, grad_param_list,
                             LAMBStage2Functor<scalar_t_0>(),
-                            std::get<1>(param_norm_tuple).data_ptr<float>(),
-                            std::get<1>(update_norm_tuple).data_ptr<float>(),
+                            std::get<1>(param_norm_tuple).DATA_PTR<float>(),
+                            std::get<1>(update_norm_tuple).DATA_PTR<float>(),
                             lr, weight_decay, use_nvlamb);)
 
   AT_CUDA_CHECK(cudaGetLastError());
 }
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 // #include <torch/all.h>
 
 #include <assert.h>
 // Stringstream is a big hammer, but I want to rely on operator<< for dtype.
 #include <sstream>
 
 #include "multi_tensor_apply.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <assert.h>
 #include <cuda_runtime.h>
 
-#include "common/micros.h"
+#include "compat.h"
 #include "multi_tensor_apply.cuh"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 /**
  * Perform fused SGD on multiple buffers
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,34 +1,103 @@
 /*This code from NVIDIA Megatron:
  *     with minor changes. */
 
-#include <ATen/ATen.h>
-#include <ATen/cuda/CUDAContext.h>
-#include <cuda.h>
-#include <cuda_fp16.h>
-#include <cuda_profiler_api.h>
-#include <cuda_runtime.h>
-#include <torch/extension.h>
+#pragma once
 
 #include <assert.h>
 #include <c10/macros/Macros.h>
+#include <cuda_fp16.h>
+#include <stdint.h>
+
 #include <cfloat>
 #include <limits>
 
-#include "common/micros.h"
-#include "utils/vec_copy.h"
-#include "funcs/reduce_function.h"
-#include "funcs/unary_functor.h"
-
-using colossalAI::funcs::UnaryOpFunctor;
-using colossalAI::funcs::UnaryOpType;
-using colossalAI::funcs::warp_reduce;
-using colossalAI::funcs::ReduceType;
-using colossalAI::cuda::utils::copy;
+namespace {
+
+template <typename Datatype, int ELEMENTS_PER_LDG>
+__device__ __inline__ void copy_vector(Datatype *dst, const Datatype *src);
+
+template <>
+__device__ __inline__ void copy_vector<c10::BFloat16, 1>(
+    c10::BFloat16 *dst, const c10::BFloat16 *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::BFloat16, 4>(
+    c10::BFloat16 *dst, const c10::BFloat16 *src) {
+  *((float2 *)dst) = *((float2 *)src);
+}
 
+template <>
+__device__ __inline__ void copy_vector<c10::Half, 1>(c10::Half *dst,
+                                                     const c10::Half *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::Half, 4>(c10::Half *dst,
+                                                     const c10::Half *src) {
+  *((float2 *)dst) = *((float2 *)src);
+}
+
+template <>
+__device__ __inline__ void copy_vector<uint8_t, 1>(uint8_t *dst,
+                                                   const uint8_t *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,
+                                                   const uint8_t *src) {
+  *((half2 *)dst) = *((half2 *)src);
+}
+
+int log2_ceil(int value) {
+  int log2_value = 0;
+  while ((1 << log2_value) < value) ++log2_value;
+  return log2_value;
+}
+
+template <typename T>
+struct Add {
+  __device__ __forceinline__ T operator()(T a, T b) const { return a + b; }
+};
+
+template <typename T>
+struct Max {
+  __device__ __forceinline__ T operator()(T a, T b) const {
+    return a < b ? b : a;
+  }
+};
+
+template <typename T>
+__device__ __forceinline__ T
+WARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,
+                     unsigned int mask = 0xffffffff) {
+#if CUDA_VERSION >= 9000
+  return __shfl_xor_sync(mask, value, laneMask, width);
+#else
+  return __shfl_xor(value, laneMask, width);
+#endif
+}
+
+template <typename acc_t, int WARP_BATCH, int WARP_SIZE,
+          template <typename> class ReduceOp>
+__device__ __forceinline__ void warp_reduce(acc_t *sum) {
+  ReduceOp<acc_t> r;
+#pragma unroll
+  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
+#pragma unroll
+    for (int i = 0; i < WARP_BATCH; ++i) {
+      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);
+      sum[i] = r(sum[i], b);
+    }
+  }
+}
 
 /*
  * Extended softmax (from native aten pytorch) with following additional
  * features 1) input scaling 2) Explicit masking
  */
 template <typename input_t, typename output_t, typename acc_t,
           int log2_elements>
@@ -83,16 +152,16 @@
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
 
       if (element_index < batch_element_count) {
         int itr_idx = i * element_count + it * WARP_SIZE;
-        copy<input_t, ELEMENTS_PER_LDG_STG>(src + itr_idx, temp_data);
-        copy<uint8_t, ELEMENTS_PER_LDG_STG>(mask + itr_idx, temp_mask);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(temp_data, src + itr_idx);
+        copy_vector<uint8_t, ELEMENTS_PER_LDG_STG>(temp_mask, mask + itr_idx);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (temp_mask[element] != 1) {
             elements[i][it + element] = (acc_t)temp_data[element] * scale;
           } else {
             elements[i][it + element] = -10000.0;
@@ -114,42 +183,42 @@
     max_value[i] = elements[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       max_value[i] =
           (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kMax,WARP_BATCH,WARP_SIZE>(max_value);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);
 
   acc_t sum[WARP_BATCH]{0.0f};
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; ++it) {
       elements[i][it] = std::exp((elements[i][it] - max_value[i]));
       sum[i] += elements[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
 
   // store result
   output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < element_count) {
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] = elements[i][it + element] / sum[i];
         }
-        copy<output_t, ELEMENTS_PER_LDG_STG>(
-          out,  dst + i * element_count + it * WARP_SIZE);
+        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
+            dst + i * element_count + it * WARP_SIZE, out);
       } else {
         break;
       }
     }
   }
 }
 
@@ -196,18 +265,18 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     int batch_element_count = (i >= local_batches) ? 0 : element_count;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < batch_element_count) {
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            grad + i * element_count + it * WARP_SIZE, temp_grad);
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            output + i * element_count + it * WARP_SIZE, temp_output);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_grad, grad + i * element_count + it * WARP_SIZE);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_output, output + i * element_count + it * WARP_SIZE);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           output_reg[i][it + element] = (acc_t)temp_output[element];
         }
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
@@ -223,15 +292,15 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     sum[i] = grad_reg[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       sum[i] += grad_reg[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
 
 // store result
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
@@ -241,25 +310,25 @@
         output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] =
               (output_t)(scale * (grad_reg[i][it + element] -
                                   output_reg[i][it + element] * sum[i]));
         }
-        copy<output_t, ELEMENTS_PER_LDG_STG>(
-          out, gradInput + i * element_count + it * WARP_SIZE);
+        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
+            gradInput + i * element_count + it * WARP_SIZE, out);
       }
     }
   }
 }
-
+}  // end of anonymous namespace
 
 int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
                         int attn_heads) {
-  int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
+  int log2_elements = log2_ceil(key_seq_len);
   const int next_power_of_two = 1 << log2_elements;
 
   int warp_size =
       (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
   int batches_per_warp = (next_power_of_two <= 128) ? 2 : 1;
 
   constexpr int threads_per_block = 128;
@@ -276,15 +345,15 @@
                                             int query_seq_len, int key_seq_len,
                                             int batches, int attn_heads,
                                             int pad_batches) {
   TORCH_INTERNAL_ASSERT(key_seq_len >= 0 && key_seq_len <= 2048);
   if (key_seq_len == 0) {
     return;
   } else {
-    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
+    int log2_elements = log2_ceil(key_seq_len);
     const int next_power_of_two = 1 << log2_elements;
     int batch_count = batches * attn_heads * query_seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_forward.
     int warp_size =
         (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
@@ -376,15 +445,15 @@
                                              const acc_t scale,
                                              int query_seq_len, int key_seq_len,
                                              int batches, int attn_heads) {
   TORCH_INTERNAL_ASSERT(key_seq_len >= 0 && key_seq_len <= 2048);
   if (key_seq_len == 0) {
     return;
   } else {
-    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
+    int log2_elements = log2_ceil(key_seq_len);
     const int next_power_of_two = 1 << log2_elements;
     int batch_count = batches * attn_heads * query_seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_backward.
     int warp_size =
         (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
@@ -463,71 +532,7 @@
                 grad_input, grad, output, scale, batch_count, key_seq_len);
         break;
       default:
         break;
     }
   }
 }
-
-torch::Tensor fwd_cuda(torch::Tensor const& input, torch::Tensor const& mask,
-                       float scale_factor) {
-  // input is a 4d tensor with dimensions [batches, attn_heads, seq_len,
-  // seq_len]
-  const int batches = input.size(0);
-  const int pad_batches = mask.size(0);
-  const int attn_heads = input.size(1);
-  const int query_seq_len = input.size(2);
-  const int key_seq_len = input.size(3);
-  TORCH_INTERNAL_ASSERT(key_seq_len <= 2048);
-  TORCH_INTERNAL_ASSERT(query_seq_len > 1);
-  TORCH_INTERNAL_ASSERT(pad_batches == 1 || pad_batches == batches);
-  TORCH_INTERNAL_ASSERT(mask.size(1) == 1);
-  TORCH_INTERNAL_ASSERT(mask.size(2) == query_seq_len);
-  TORCH_INTERNAL_ASSERT(mask.size(3) == key_seq_len);
-
-  // Output
-  auto act_options = input.options().requires_grad(false);
-  torch::Tensor softmax_results = torch::empty(
-      {batches, attn_heads, query_seq_len, key_seq_len}, act_options);
-
-  // Softmax Intermediate Result Ptr
-  void* input_ptr = static_cast<void*>(input.data_ptr());
-  void* mask_ptr = static_cast<void*>(mask.data_ptr());
-  void* softmax_results_ptr = static_cast<void*>(softmax_results.data_ptr());
-
-  DISPATCH_HALF_AND_BFLOAT(
-      input.scalar_type(), "dispatch_scaled_masked_softmax_forward",
-      dispatch_scaled_masked_softmax_forward<scalar_t, scalar_t, float>(
-          reinterpret_cast<scalar_t*>(softmax_results_ptr),
-          reinterpret_cast<const scalar_t*>(input_ptr),
-          reinterpret_cast<const uint8_t*>(mask_ptr), scale_factor,
-          query_seq_len, key_seq_len, batches, attn_heads, pad_batches););
-  return softmax_results;
-}
-
-torch::Tensor bwd_cuda(torch::Tensor const& output_grads_,
-                       torch::Tensor const& softmax_results_,
-                       float scale_factor) {
-  auto output_grads = output_grads_.contiguous();
-  auto softmax_results = softmax_results_.contiguous();
-
-  // output grads is a 4d tensor with dimensions [batches, attn_heads, seq_len,
-  // seq_len]
-  const int batches = output_grads.size(0);
-  const int attn_heads = output_grads.size(1);
-  const int query_seq_len = output_grads.size(2);
-  const int key_seq_len = output_grads.size(3);
-
-  void* output_grads_ptr = static_cast<void*>(output_grads.data_ptr());
-
-  // Softmax Grad
-  DISPATCH_HALF_AND_BFLOAT(
-      output_grads_.scalar_type(), "dispatch_scaled_masked_softmax_backward",
-      dispatch_scaled_masked_softmax_backward<scalar_t, scalar_t, float>(
-          reinterpret_cast<scalar_t*>(output_grads_ptr),
-          reinterpret_cast<scalar_t*>(output_grads_ptr),
-          reinterpret_cast<scalar_t const*>(softmax_results.data_ptr()),
-          scale_factor, query_seq_len, key_seq_len, batches, attn_heads););
-
-  // backward pass is completely in-place
-  return output_grads;
-}
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,128 @@
 /*This code from NVIDIA Megatron:
  *     with minor changes. */
 
-#include <ATen/ATen.h>
-#include <ATen/cuda/CUDAContext.h>
-#include <cuda.h>
-#include <cuda_fp16.h>
-#include <cuda_profiler_api.h>
-#include <cuda_runtime.h>
-#include <torch/extension.h>
+#pragma once
+
 #include <assert.h>
 #include <c10/macros/Macros.h>
+#include <cuda_fp16.h>
 #include <stdint.h>
+
 #include <cfloat>
 #include <limits>
 
-#include "common/micros.h"
-#include "utils/vec_copy.h"
-#include "funcs/reduce_function.h"
-#include "funcs/unary_functor.h"
-
-using colossalAI::funcs::UnaryOpFunctor;
-using colossalAI::funcs::UnaryOpType;
-using colossalAI::funcs::warp_reduce;
-using colossalAI::funcs::ReduceType;
-using colossalAI::cuda::utils::copy;
-using colossalAI::cuda::utils::copy_zero;
+namespace {
+
+template <typename Datatype, int ELEMENTS_PER_LDG>
+__device__ __inline__ void copy_vector(Datatype *dst, const Datatype *src);
+
+template <>
+__device__ __inline__ void copy_vector<c10::BFloat16, 1>(
+    c10::BFloat16 *dst, const c10::BFloat16 *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::BFloat16, 4>(
+    c10::BFloat16 *dst, const c10::BFloat16 *src) {
+  *((float2 *)dst) = *((float2 *)src);
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::Half, 1>(c10::Half *dst,
+                                                     const c10::Half *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::Half, 4>(c10::Half *dst,
+                                                     const c10::Half *src) {
+  *((float2 *)dst) = *((float2 *)src);
+}
+
+template <>
+__device__ __inline__ void copy_vector<uint8_t, 1>(uint8_t *dst,
+                                                   const uint8_t *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,
+                                                   const uint8_t *src) {
+  *((half2 *)dst) = *((half2 *)src);
+}
+
+template <typename Datatype, int ELEMENTS_PER_LDG>
+__device__ __inline__ void copy_zero_vector(Datatype *dst);
+
+template <>
+__device__ __inline__ void copy_zero_vector<c10::BFloat16, 1>(
+    c10::BFloat16 *dst) {
+  *dst = 0.0;
+}
+
+template <>
+__device__ __inline__ void copy_zero_vector<c10::BFloat16, 4>(
+    c10::BFloat16 *dst) {
+  *((float2 *)dst) = make_float2(0.0f, 0.0f);
+}
+
+template <>
+__device__ __inline__ void copy_zero_vector<c10::Half, 1>(c10::Half *dst) {
+  *dst = 0.0;
+}
+
+template <>
+__device__ __inline__ void copy_zero_vector<c10::Half, 4>(c10::Half *dst) {
+  *((float2 *)dst) = make_float2(0.0f, 0.0f);
+}
+
+int log2_ceil(int value) {
+  int log2_value = 0;
+  while ((1 << log2_value) < value) ++log2_value;
+  return log2_value;
+}
+
+template <typename T>
+struct Add {
+  __device__ __forceinline__ T operator()(T a, T b) const { return a + b; }
+};
+
+template <typename T>
+struct Max {
+  __device__ __forceinline__ T operator()(T a, T b) const {
+    return a < b ? b : a;
+  }
+};
+
+template <typename T>
+__device__ __forceinline__ T
+WARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,
+                     unsigned int mask = 0xffffffff) {
+#if CUDA_VERSION >= 9000
+  return __shfl_xor_sync(mask, value, laneMask, width);
+#else
+  return __shfl_xor(value, laneMask, width);
+#endif
+}
+
+template <typename acc_t, int WARP_BATCH, int WARP_SIZE,
+          template <typename> class ReduceOp>
+__device__ __forceinline__ void warp_reduce(acc_t *sum) {
+  ReduceOp<acc_t> r;
+#pragma unroll
+  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
+#pragma unroll
+    for (int i = 0; i < WARP_BATCH; ++i) {
+      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);
+      sum[i] = r(sum[i], b);
+    }
+  }
+}
 
 /*
  * Extended softmax (from native aten pytorch) with following additional
  * features 1) input scaling 2) Implicit time (diagonal masking)
  */
 template <typename input_t, typename output_t, typename acc_t,
           int log2_elements>
@@ -71,16 +165,16 @@
     int batch_element_count = (i >= local_batches) ? 0 : local_seq;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
 
       if (element_index < batch_element_count) {
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            src + i * element_count * stride + it * WARP_SIZE, temp_data);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_data, src + i * element_count * stride + it * WARP_SIZE);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if ((element_index + element) < batch_element_count) {
             elements[i][it + element] = (acc_t)temp_data[element] * scale;
           } else {
             elements[i][it + element] = -std::numeric_limits<acc_t>::infinity();
@@ -102,29 +196,28 @@
     max_value[i] = elements[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       max_value[i] =
           (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kMax,WARP_BATCH,WARP_SIZE>(max_value);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);
 
   acc_t sum[WARP_BATCH]{0.0f};
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; ++it) {
       if (it < warp_iteration_limit) {
         elements[i][it] = std::exp((elements[i][it] - max_value[i]));
         sum[i] += elements[i][it];
       }
     }
   }
-  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
-
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
 
   // store result
   output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
@@ -136,18 +229,18 @@
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (element_index + element < local_seq) {
             out[element] = elements[i][it + element] / sum[i];
           } else {
             out[element] = 0;
           }
         }
-        copy<output_t, ELEMENTS_PER_LDG_STG>(
-            out, dst + i * element_count * stride + it * WARP_SIZE);
+        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
+            dst + i * element_count * stride + it * WARP_SIZE, out);
       } else if (element_index < element_count) {
-        copy_zero<output_t, ELEMENTS_PER_LDG_STG>(
+        copy_zero_vector<output_t, ELEMENTS_PER_LDG_STG>(
             dst + i * element_count * stride + it * WARP_SIZE);
       } else {
         break;
       }
     }
   }
 }
@@ -195,18 +288,18 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     int batch_element_count = (i >= local_batches) ? 0 : local_seq;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < batch_element_count) {
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            grad + i * element_count * stride + it * WARP_SIZE, temp_grad);
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            output + i * element_count * stride + it * WARP_SIZE, temp_output);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_grad, grad + i * element_count * stride + it * WARP_SIZE);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_output, output + i * element_count * stride + it * WARP_SIZE);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (element_index + element < batch_element_count) {
             output_reg[i][it + element] = (acc_t)temp_output[element];
           }
         }
@@ -226,15 +319,15 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     sum[i] = grad_reg[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       sum[i] += grad_reg[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
 
 // store result
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
@@ -244,30 +337,32 @@
         output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] =
               (output_t)(scale * (grad_reg[i][it + element] -
                                   output_reg[i][it + element] * sum[i]));
         }
-        copy<output_t, ELEMENTS_PER_LDG_STG>(
-            out, gradInput + i * element_count * stride + it * WARP_SIZE);
+        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
+            gradInput + i * element_count * stride + it * WARP_SIZE, out);
       }
     }
   }
 }
 
+}  // end of anonymous namespace
+
 template <typename input_t, typename output_t, typename acc_t>
 void dispatch_scaled_upper_triang_masked_softmax_forward(
     output_t *dst, const input_t *src, const input_t scale,
     int softmax_elements, int softmax_elements_stride, int attn_batches) {
   TORCH_INTERNAL_ASSERT(softmax_elements >= 0 && softmax_elements <= 2048);
   if (softmax_elements == 0) {
     return;
   } else {
-    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(softmax_elements);
+    int log2_elements = log2_ceil(softmax_elements);
     const int next_power_of_two = 1 << log2_elements;
     int seq_len = softmax_elements;
     int batch_count = attn_batches * seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_forward.
     int warp_size =
@@ -384,15 +479,15 @@
     output_t *grad_input, input_t *grad, const input_t *output,
     const acc_t scale, int softmax_elements, int softmax_elements_stride,
     int attn_batches) {
   TORCH_INTERNAL_ASSERT(softmax_elements >= 0 && softmax_elements <= 2048);
   if (softmax_elements == 0) {
     return;
   } else {
-    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(softmax_elements);
+    int log2_elements = log2_ceil(softmax_elements);
     const int next_power_of_two = 1 << log2_elements;
     int seq_len = softmax_elements;
     int batch_count = attn_batches * seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_backward.
     int warp_size =
@@ -499,65 +594,7 @@
                 softmax_elements_stride, softmax_elements);
         break;
       default:
         break;
     }
   }
 }
-
-
-
-
-torch::Tensor fwd_cuda(torch::Tensor const& input, float scale_factor) {
-  // input is a 3d tensor with dimensions [attn_batches, seq_len, seq_len]
-  const int attn_batches = input.size(0);
-  const int seq_len = input.size(1);
-  TORCH_INTERNAL_ASSERT(seq_len <= 2048);
-
-  // Output
-  auto act_options = input.options().requires_grad(false);
-  torch::Tensor softmax_results =
-      torch::empty({attn_batches, seq_len, seq_len}, act_options);
-
-  // Softmax Intermediate Result Ptr
-  void* input_ptr = static_cast<void*>(input.data_ptr());
-  void* softmax_results_ptr = static_cast<void*>(softmax_results.data_ptr());
-
-  DISPATCH_HALF_AND_BFLOAT(
-      input.scalar_type(),
-      "dispatch_scaled_upper_triang_masked_softmax_forward",
-      dispatch_scaled_upper_triang_masked_softmax_forward<scalar_t, scalar_t,
-                                                          float>(
-          reinterpret_cast<scalar_t*>(softmax_results_ptr),
-          reinterpret_cast<const scalar_t*>(input_ptr), scale_factor, seq_len,
-          seq_len, attn_batches););
-  return softmax_results;
-}
-
-torch::Tensor bwd_cuda(torch::Tensor const& output_grads_,
-                       torch::Tensor const& softmax_results_,
-                       float scale_factor) {
-  auto output_grads = output_grads_.contiguous();
-  auto softmax_results = softmax_results_.contiguous();
-
-  // output grads is a 3d tensor with dimensions [attn_batches, seq_len,
-  // seq_len]
-  const int attn_batches = output_grads.size(0);
-  const int seq_len = output_grads.size(1);
-  TORCH_INTERNAL_ASSERT(output_grads.size(1) == output_grads.size(2));
-
-  void* output_grads_ptr = static_cast<void*>(output_grads.data_ptr());
-
-  // Softmax Grad
-  DISPATCH_HALF_AND_BFLOAT(
-      output_grads_.scalar_type(),
-      "dispatch_scaled_upper_triang_masked_softmax_backward",
-      dispatch_scaled_upper_triang_masked_softmax_backward<scalar_t, scalar_t,
-                                                           float>(
-          reinterpret_cast<scalar_t*>(output_grads_ptr),
-          reinterpret_cast<scalar_t*>(output_grads_ptr),
-          reinterpret_cast<scalar_t const*>(softmax_results.data_ptr()),
-          scale_factor, seq_len, seq_len, attn_batches););
-
-  // backward pass is completely in-place
-  return output_grads;
-}
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.h` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/cuda_extension.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cuda_extension.py`

 * *Files 13% similar despite different names*

```diff
@@ -17,15 +17,14 @@
 
 class _CudaExtension(_CppExtension):
     @abstractmethod
     def nvcc_flags(self) -> List[str]:
         """
         This function should return a list of nvcc compilation flags for extensions.
         """
-        return ["-DCOLOSSAL_WITH_CUDA"]
 
     def is_available(self) -> bool:
         # cuda extension can only be built if cuda is available
         try:
             import torch
 
             cuda_available = torch.cuda.is_available()
@@ -50,20 +49,14 @@
         from torch.utils.cpp_extension import CUDA_HOME
 
         if CUDA_HOME is None:
             raise RuntimeError("CUDA_HOME is None, please set CUDA_HOME to compile C++/CUDA kernels in ColossalAI.")
         cuda_include = os.path.join(CUDA_HOME, "include")
         return cuda_include
 
-    def include_dirs(self) -> List[str]:
-        """
-        This function should return a list of include files for extensions.
-        """
-        return super().include_dirs() + [self.get_cuda_home_include()]
-
     def build_jit(self) -> None:
         from torch.utils.cpp_extension import CUDA_HOME, load
 
         set_cuda_arch_list(CUDA_HOME)
 
         # get build dir
         build_directory = _Extension.get_jit_extension_folder_path()
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_x86.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import platform
 
-from ...cuda_extension import _CudaExtension
-from ...utils import append_nvcc_threads
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads
 
 
 class CpuAdamX86Extension(_CudaExtension):
     def __init__(self):
         super().__init__(name="cpu_adam_x86")
 
     def is_available(self) -> bool:
@@ -17,18 +17,21 @@
             arch == "x86_64"
         ), f"[extension] The {self.name} kernel requires the CPU architecture to be x86_64 but got {arch}"
         super().assert_compatible()
 
     # necessary 4 functions
     def sources_files(self):
         ret = [
-            self.csrc_abs_path("kernel/x86/cpu_adam.cpp"),
+            self.csrc_abs_path("cuda/cpu_adam.cpp"),
         ]
         return ret
 
+    def include_dirs(self):
+        return [self.csrc_abs_path("includes"), self.get_cuda_home_include()]
+
     def cxx_flags(self):
         extra_cxx_flags = [
             "-std=c++14",
             "-std=c++17",
             "-lcudart",
             "-lcublas",
             "-g",
@@ -43,9 +46,9 @@
             "-std=c++14",
             "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
             "-U__CUDA_NO_HALF2_OPERATORS__",
             "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
         ]
-        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags + super().nvcc_flags()
+        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_dao_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ...base_extension import _Extension
+from ..base_extension import _Extension
 
 
 class FlashAttentionDaoCudaExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_dao_cuda", support_aot=False, support_jit=False, priority=10)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_npu.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_npu.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ...base_extension import _Extension
+from ..base_extension import _Extension
 
 
 class FlashAttentionNpuExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_npu", support_aot=False, support_jit=False)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_sdpa_cuda.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ...base_extension import _Extension
+from ..base_extension import _Extension
 
 
 class FlashAttentionSdpaCudaExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_sdpa_cuda", support_aot=False, support_jit=False)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/inference/inference_ops_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/moe_cuda.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,32 +1,29 @@
-from ...cuda_extension import _CudaExtension
-from ...utils import get_cuda_cc_flag
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads, get_cuda_cc_flag
 
 
-class InferenceOpsCudaExtension(_CudaExtension):
+class MoeCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="inference_ops_cuda")
+        super().__init__(name="moe_cuda")
+
+    def include_dirs(self):
+        ret = [self.csrc_abs_path("cuda/include"), self.get_cuda_home_include()]
+        return ret
 
     def sources_files(self):
-        ret = [
-            self.csrc_abs_path(fname)
-            for fname in [
-                "kernel/cuda/decode_kv_cache_memcpy_kernel.cu",
-                "kernel/cuda/context_kv_cache_memcpy_kernel.cu",
-                "kernel/cuda/fused_rotary_emb_and_cache_kernel.cu",
-                "kernel/cuda/activation_kernel.cu",
-                "kernel/cuda/rms_layernorm_kernel.cu",
-                "kernel/cuda/get_cos_and_sin_kernel.cu",
-                "kernel/cuda/flash_decoding_attention_kernel.cu",
-                "kernel/cuda/convert_fp8_kernel.cu",
-            ]
-        ] + [self.pybind_abs_path("inference/inference.cpp")]
+        ret = [self.csrc_abs_path(fname) for fname in ["cuda/moe_cuda.cpp", "cuda/moe_cuda_kernel.cu"]]
         return ret
 
     def cxx_flags(self):
-        version_dependent_macros = ["-DVERSION_GE_1_1", "-DVERSION_GE_1_3", "-DVERSION_GE_1_5"]
-        return ["-O3"] + version_dependent_macros
+        return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
-        extra_cuda_flags = ["-lineinfo"]
+        extra_cuda_flags = [
+            "-U__CUDA_NO_HALF_OPERATORS__",
+            "-U__CUDA_NO_HALF_CONVERSIONS__",
+            "--expt-relaxed-constexpr",
+            "--expt-extended-lambda",
+        ]
         extra_cuda_flags.extend(get_cuda_cc_flag())
-        return ["-O3", "--use_fast_math"] + extra_cuda_flags + super().nvcc_flags()
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags
+        return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/layernorm/layer_norm.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
  *     with minor changes. */
 
 #include <torch/extension.h>
 
 #include <cassert>
 #include <vector>
 
-#include "common/micros.h"
+#include "compat.h"
 
 namespace {
 
 void compute_n1_n2(at::Tensor input, at::IntArrayRef normalized_shape, int &n1,
                    int &n2) {
   int idiff = input.ndimension() - normalized_shape.size();
   n2 = 1;
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/layernorm/layernorm_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,26 +1,34 @@
-from ...cuda_extension import _CudaExtension
-from ...utils import append_nvcc_threads, get_cuda_cc_flag
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads, get_cuda_cc_flag
 
 
-class LayerNormCudaExtension(_CudaExtension):
+class ScaledUpperTriangleMaskedSoftmaxCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="layernorm_cuda")
+        super().__init__(name="scaled_upper_triangle_masked_softmax_cuda")
+
+    def include_dirs(self):
+        return [self.get_cuda_home_include()]
 
     def sources_files(self):
-        ret = [self.csrc_abs_path(fname) for fname in ["kernel/cuda/layer_norm_kernel.cu"]] + [
-            self.pybind_abs_path("layernorm/layer_norm.cpp")
+        ret = [
+            self.csrc_abs_path(fname)
+            for fname in [
+                "cuda/scaled_upper_triang_masked_softmax.cpp",
+                "cuda/scaled_upper_triang_masked_softmax_cuda.cu",
+            ]
         ]
         return ret
 
-    def include_dirs(self):
-        ret = [self.get_cuda_home_include()] + [self.csrc_abs_path("")]
-        return ret
-
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
-        extra_cuda_flags = ["-maxrregcount=50"]
+        extra_cuda_flags = [
+            "-U__CUDA_NO_HALF_OPERATORS__",
+            "-U__CUDA_NO_HALF_CONVERSIONS__",
+            "--expt-relaxed-constexpr",
+            "--expt-extended-lambda",
+        ]
         extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + self.version_dependent_macros + super().nvcc_flags()
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/moe/moe.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/optimizer/optimizer.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp`

 * *Files 18% similar despite different names*

```diff
@@ -2,23 +2,27 @@
  *     with minor changes. */
 
 #include <cuda_fp16.h>
 #include <torch/extension.h>
 
 #include <vector>
 
+namespace multihead_attn {
+namespace fused_softmax {
+namespace scaled_masked_softmax {
+
 torch::Tensor fwd_cuda(torch::Tensor const& input, torch::Tensor const& mask,
                        float scale_factor);
 
 torch::Tensor bwd_cuda(torch::Tensor const& output_grads,
                        torch::Tensor const& softmax_results,
                        float scale_factor);
 
-int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
-                        int attn_heads);
+int get_batch_per_block_cuda(int query_seq_len, int key_seq_len, int batches,
+                             int attn_heads);
 
 torch::Tensor fwd(torch::Tensor const& input, torch::Tensor const& mask,
                   float scale_factor) {
   AT_ASSERTM(input.dim() == 4, "expected 4D tensor");
   AT_ASSERTM((input.scalar_type() == at::ScalarType::Half) ||
                  (input.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
@@ -38,17 +42,29 @@
   AT_ASSERTM((softmax_results.scalar_type() == at::ScalarType::Half) ||
                  (softmax_results.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
 
   return bwd_cuda(output_grads, softmax_results, scale_factor);
 }
 
+int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
+                        int attn_heads) {
+  return get_batch_per_block_cuda(query_seq_len, key_seq_len, batches,
+                                  attn_heads);
+}
+
+}  // end namespace scaled_masked_softmax
+}  // end namespace fused_softmax
+}  // end namespace multihead_attn
+
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("forward", &fwd,
+  m.def("forward", &multihead_attn::fused_softmax::scaled_masked_softmax::fwd,
         "Self Multihead Attention scaled, time masked softmax -- Forward.");
 
-  m.def("backward", &bwd,
+  m.def("backward", &multihead_attn::fused_softmax::scaled_masked_softmax::bwd,
         "Self Multihead Attention scaled, time masked softmax -- Backward.");
 
-  m.def("get_batch_per_block", &get_batch_per_block,
+  m.def("get_batch_per_block",
+        &multihead_attn::fused_softmax::scaled_masked_softmax::
+            get_batch_per_block,
         "Return Batch per block size.");
 }
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax_cuda.py` & `colossalai-nightly-2024.5.4/extensions/moe/moe_cuda.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,28 +1,29 @@
-from ...cuda_extension import _CudaExtension
-from ...utils import append_nvcc_threads
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads, get_cuda_cc_flag
 
 
-class ScaledMaskedSoftmaxCudaExtension(_CudaExtension):
+class MoeCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="scaled_masked_softmax_cuda")
+        super().__init__(name="moe_cuda")
+
+    def include_dirs(self):
+        ret = [self.csrc_abs_path("cuda/include"), self.get_cuda_home_include()]
+        return ret
 
     def sources_files(self):
-        ret = [self.csrc_abs_path(fname) for fname in ["kernel/cuda/scaled_masked_softmax_kernel.cu"]] + [
-            self.pybind_abs_path("softmax/scaled_masked_softmax.cpp")
-        ]
+        ret = [self.csrc_abs_path(fname) for fname in ["cuda/moe_cuda.cpp", "cuda/moe_cuda_kernel.cu"]]
         return ret
 
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
         extra_cuda_flags = [
-            "-std=c++14",
-            "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
-            "-U__CUDA_NO_HALF2_OPERATORS__",
-            "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
+            "--expt-relaxed-constexpr",
+            "--expt-extended-lambda",
         ]
-        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags + super().nvcc_flags()
+        extra_cuda_flags.extend(get_cuda_cc_flag())
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,30 +1,32 @@
-from ...cuda_extension import _CudaExtension
-from ...utils import append_nvcc_threads, get_cuda_cc_flag
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads
 
 
-class ScaledUpperTriangleMaskedSoftmaxCudaExtension(_CudaExtension):
+class ScaledMaskedSoftmaxCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="scaled_upper_triangle_masked_softmax_cuda")
+        super().__init__(name="scaled_masked_softmax_cuda")
 
     def sources_files(self):
         ret = [
             self.csrc_abs_path(fname)
-            for fname in [
-                "kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu",
-            ]
-        ] + [self.pybind_abs_path("softmax/scaled_upper_triang_masked_softmax.cpp")]
+            for fname in ["cuda/scaled_masked_softmax.cpp", "cuda/scaled_masked_softmax_cuda.cu"]
+        ]
         return ret
 
+    def include_dirs(self):
+        return [self.get_cuda_home_include()]
+
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
         extra_cuda_flags = [
+            "-std=c++14",
+            "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
-            "--expt-relaxed-constexpr",
-            "--expt-extended-lambda",
+            "-U__CUDA_NO_HALF2_OPERATORS__",
+            "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
         ]
-        extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + super().nvcc_flags()
+        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/triton_extension.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/triton_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/extensions/utils.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/jit/bias_dropout_add.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_dropout_add.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/jit/bias_gelu.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_gelu.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/jit/option.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/jit/option.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/kernel_loader.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/kernel_loader.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,29 +4,27 @@
 from .extensions import (
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     FlashAttentionDaoCudaExtension,
     FlashAttentionNpuExtension,
     FlashAttentionSdpaCudaExtension,
     FusedOptimizerCudaExtension,
-    InferenceOpsCudaExtension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
 )
 from .extensions.base_extension import _Extension
 
 __all__ = [
     "KernelLoader",
     "CPUAdamLoader",
     "LayerNormLoader",
     "MoeLoader",
     "FusedOptimizerLoader",
-    "InferenceOpsLoader",
     "ScaledMaskedSoftmaxLoader",
     "ScaledUpperTriangleMaskedSoftmaxLoader",
 ]
 
 
 class KernelLoader:
     """
@@ -95,18 +93,14 @@
     REGISTRY = [MoeCudaExtension]
 
 
 class FusedOptimizerLoader(KernelLoader):
     REGISTRY = [FusedOptimizerCudaExtension]
 
 
-class InferenceOpsLoader(KernelLoader):
-    REGISTRY = [InferenceOpsCudaExtension]
-
-
 class ScaledMaskedSoftmaxLoader(KernelLoader):
     REGISTRY = [ScaledMaskedSoftmaxCudaExtension]
 
 
 class ScaledUpperTriangleMaskedSoftmaxLoader(KernelLoader):
     REGISTRY = [ScaledUpperTriangleMaskedSoftmaxCudaExtension]
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/triton/flash_decoding.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/gptq_triton.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,533 +1,543 @@
-# Applying Flash-Decoding as descibed in
-# https://pytorch.org/blog/flash-decoding/
-# by Tri Dao, 2023
+# Adapted from AutoGPTQ auto_gptq: https://github.com/PanQiWei/AutoGPTQ
+
 import torch
 import triton
 import triton.language as tl
 
+from .custom_autotune import autotune, matmul248_kernel_config_pruner
+
 
-# Triton 2.1.0
 @triton.jit
-def _flash_decoding_fwd_kernel(
-    Q,  # [batch_size * q_len, head_num, head_dim]
-    KCache,  # [num_blocks, num_kv_heads, block_size, head_dim]
-    VCache,  # [num_blocks, num_kv_heads, block_size, head_dim],
-    # or [num_blocks, num_kv_heads, head_dim//x, block_size, x], depends on strides provided
-    block_tables,  # [batch_size, max_blocks_per_sequence]
-    mid_o,  # [batch_size * q_len, head_num, kv_split_num, head_dim]
-    mid_o_lse,  # [batch_size * q_len, head_num, kv_split_num]
-    kv_seq_len,  # [batch_size]
-    q_len,
-    batch_size,
-    kv_group_num,
-    x,
-    sm_scale,
-    stride_qt,
-    stride_qh,
-    stride_qd,
-    stride_kcb,
-    stride_kch,
-    stride_kcsplit_x,
-    stride_kcs,
-    stride_kcd,
-    stride_vcb,
-    stride_vch,
-    stride_vcs,
-    stride_vcd,
-    stride_bts,
-    stride_btb,
-    stride_mid_ot,
-    stride_mid_oh,
-    stride_mid_ob,
-    stride_mid_od,
-    stride_mid_o_lset,
-    stride_mid_o_lseh,
-    stride_mid_o_lseb,
-    BLOCK_KV: tl.constexpr,
-    BLOCK_SIZE: tl.constexpr,
-    HEAD_DIM: tl.constexpr,
-):
-    cur_token_idx = tl.program_id(0)
-    cur_seq_idx = cur_token_idx // q_len
-    if cur_seq_idx >= batch_size:
-        return
-    cur_token_off = (cur_token_idx % q_len) - q_len + 1
-    cur_head_idx = tl.program_id(1)
-    block_start_kv = tl.program_id(2)  # for splitting k/v
-
-    # NOTE It requires BLOCK_KV and BLOCK_SIZE to be the same
-    # TODO might want to replace with BLOCK_KV % BLOCK_SIZE == 0 (optimize BLOCK_KV as multiple of BLOCK_SIZE)
-    #      and then support calculating multiple kv cache blocks on an instance
-    tl.static_assert(BLOCK_KV == BLOCK_SIZE)
-    # get the current (kv) sequence length
-    # cur_token_off is used as a "mask" here for spec-dec during verification process
-    cur_kv_seq_len = tl.load(kv_seq_len + cur_seq_idx) + cur_token_off
-    if block_start_kv * BLOCK_KV >= cur_kv_seq_len:
-        return
-    offsets_dmodel = tl.arange(0, HEAD_DIM)
-    offsets_block = tl.arange(0, BLOCK_SIZE)
-
-    # block table for the current sequence
-    block_table_ptr = block_tables + cur_seq_idx * stride_bts
-    # cur_bt_start_idx = block_start_kv * (BLOCK_KV // BLOCK_SIZE)
-    # cur_block_id = tl.load(block_table_ptr + cur_bt_start_idx * stride_btb)
-    cur_block_id = tl.load(block_table_ptr + block_start_kv * stride_btb)
-    cur_occupied_size = tl.where(
-        (block_start_kv + 1) * BLOCK_SIZE <= cur_kv_seq_len, BLOCK_SIZE, cur_kv_seq_len - block_start_kv * BLOCK_SIZE
-    )
-    tl.device_assert(cur_occupied_size >= 0)
+def tanh(x):
+    # Tanh is just a scaled sigmoid
+    return 2 * tl.sigmoid(2 * x) - 1
 
-    offsets_q = cur_token_idx * stride_qt + cur_head_idx * stride_qh + offsets_dmodel * stride_qd
-    q = tl.load(Q + offsets_q)
-    cur_kv_head_idx = cur_head_idx // kv_group_num
-    offset_kvcache = cur_block_id * stride_kcb + cur_kv_head_idx * stride_kch
-    offsets_k = (
-        offset_kvcache
-        + (offsets_dmodel[None, :] // x) * stride_kcsplit_x
-        + (offsets_dmodel[None, :] % x) * stride_kcd
-        + offsets_block[:, None] * stride_kcs
-    )
-    k_cur_block = tl.load(KCache + offsets_k)
-    V_block_ptr = tl.make_block_ptr(
-        base=VCache + offset_kvcache,
-        shape=(cur_occupied_size, HEAD_DIM),
-        strides=(stride_vcs, stride_vcd),
-        offsets=(0, 0),
-        block_shape=(BLOCK_SIZE, HEAD_DIM),
-        order=(0, 1),
-    )
-    v_cur_block = tl.load(V_block_ptr)
-    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)
-    # use block size of the paged/blocked kv cache
-    S_ij = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
-
-    # NOTE a trick to come across triton's requirement that values in both first and second input shapes must be >= 16,
-    # Multiplying two tensors with shapes [1, d] * [d, block_size] will fail.
-    # Refer to https://github.com/openai/triton/discussions/895
-    S_ij += tl.sum(q[None, :] * k_cur_block, 1)
-    S_ij *= sm_scale
-    S_ij += tl.where(block_start_kv * BLOCK_KV + offsets_block < cur_kv_seq_len, 0, float("-inf"))
-
-    m = tl.max(S_ij, 0)
-    S_ij -= m
-    p_ij_hat = tl.exp(S_ij)
-    l_i = tl.sum(p_ij_hat, 0)
-    p_ij_hat = p_ij_hat.to(v_cur_block.type.element_ty)
-    acc += tl.sum(v_cur_block * p_ij_hat[:, None], 0)
-    acc = acc / l_i
-
-    offsets_mid_o = (
-        cur_token_idx * stride_mid_ot
-        + cur_head_idx * stride_mid_oh
-        + block_start_kv * stride_mid_ob
-        + offsets_dmodel * stride_mid_od
-    )
-    tl.store(mid_o + offsets_mid_o, acc)
-    offsets_mid_o_lse = (
-        cur_token_idx * stride_mid_o_lset + cur_head_idx * stride_mid_o_lseh + block_start_kv * stride_mid_o_lseb
-    )
-    # logsumexp l_i^(j) = m^(j) + log(l_i^(j))
-    tl.store(mid_o_lse + offsets_mid_o_lse, m + tl.log(l_i))
 
+@triton.jit
+def cosh(x):
+    exp_x = tl.exp(x)
+    return (exp_x + 1.0 / exp_x) * 0.5
 
-# Triton 2.1.0
+
+# a Triton implementation of the most used activations
+# See for instance http://arxiv.org/abs/1606.08415 for an overview
+
+
+# ReLU
 @triton.jit
-def _alibi_flash_decoding_fwd_kernel(
-    Q,  # [batch_size * q_len, head_num, head_dim]
-    KCache,  # [num_blocks, num_kv_heads, block_size, head_dim]
-    VCache,  # [num_blocks, num_kv_heads, block_size, head_dim]
-    block_tables,  # [batch_size, max_blocks_per_sequence]
-    mid_o,  # [batch_size * q_len, head_num, kv_split_num, head_dim]
-    mid_o_lse,  # [batch_size * q_len, head_num, kv_split_num]
-    kv_seq_len,  # [batch_size]
-    q_len,
-    batch_size,
-    alibi_slopes,
-    stride_qt,
-    stride_qh,
-    stride_qd,
-    stride_cacheb,
-    stride_cacheh,
-    stride_cachebs,
-    stride_cached,
-    stride_bts,
-    stride_btb,
-    stride_mid_ot,
-    stride_mid_oh,
-    stride_mid_ob,
-    stride_mid_od,
-    stride_mid_o_lset,
-    stride_mid_o_lseh,
-    stride_mid_o_lseb,
-    sm_scale,
-    KV_GROUPS: tl.constexpr,
-    BLOCK_KV: tl.constexpr,
-    BLOCK_SIZE: tl.constexpr,
-    HEAD_DIM: tl.constexpr,
-):
-    cur_token_idx = tl.program_id(0)
-    cur_seq_idx = cur_token_idx // q_len
-    if cur_seq_idx >= batch_size:
-        return
-    cur_token_off = (cur_token_idx % q_len) - q_len + 1
-    cur_head_idx = tl.program_id(1)
-    block_start_kv = tl.program_id(2)  # for splitting k/v
-
-    # NOTE It requires BLOCK_KV and BLOCK_SIZE to be the same
-    # TODO might want to replace with BLOCK_KV % BLOCK_SIZE == 0 (optimize BLOCK_KV as multiple of BLOCK_SIZE)
-    #      and then support calculating multiple kv cache blocks on an instance
-    tl.static_assert(BLOCK_KV == BLOCK_SIZE)
-    # get the current (kv) sequence length
-    # cur_token_off is used as a "mask" here for spec-dec during verification process
-    cur_kv_seq_len = tl.load(kv_seq_len + cur_seq_idx) + cur_token_off
-    if block_start_kv * BLOCK_KV >= cur_kv_seq_len:
-        return
-
-    offsets_dmodel = tl.arange(0, HEAD_DIM)
-    offsets_q = cur_token_idx * stride_qt + cur_head_idx * stride_qh + offsets_dmodel * stride_qd
-    q = tl.load(Q + offsets_q)
-    # block table for the current sequence
-    block_table_ptr = block_tables + cur_seq_idx * stride_bts
-    # cur_bt_start_idx = block_start_kv * (BLOCK_KV // BLOCK_SIZE)
-    # cur_block_id = tl.load(block_table_ptr + cur_bt_start_idx * stride_btb)
-    cur_block_id = tl.load(block_table_ptr + block_start_kv * stride_btb)
-    cur_occupied_size = tl.where(
-        (block_start_kv + 1) * BLOCK_SIZE <= cur_kv_seq_len, BLOCK_SIZE, cur_kv_seq_len - block_start_kv * BLOCK_SIZE
-    )
-    tl.device_assert(cur_occupied_size >= 0)
+def relu(x):
+    """
+    ReLU_ activation function
 
-    cur_kv_head_idx = cur_head_idx // KV_GROUPS
-    offset_kvcache = cur_block_id * stride_cacheb + cur_kv_head_idx * stride_cacheh
-    K_block_ptr = tl.make_block_ptr(
-        base=KCache + offset_kvcache,
-        shape=(cur_occupied_size, HEAD_DIM),
-        strides=(stride_cachebs, stride_cached),
-        offsets=(0, 0),
-        block_shape=(BLOCK_SIZE, HEAD_DIM),
-        order=(0, 1),
-    )
-    V_block_ptr = tl.make_block_ptr(
-        base=VCache + offset_kvcache,
-        shape=(cur_occupied_size, HEAD_DIM),
-        strides=(stride_cachebs, stride_cached),
-        offsets=(0, 0),
-        block_shape=(BLOCK_SIZE, HEAD_DIM),
-        order=(0, 1),
-    )
-    k_cur_block = tl.load(K_block_ptr)
-    v_cur_block = tl.load(V_block_ptr)
-    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)
-    # use block size of the paged/blocked kv cache
-    S_ij = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
-
-    alibi_slope = tl.load(alibi_slopes + cur_head_idx)
-    position_k_offset = block_start_kv * BLOCK_KV + tl.arange(0, BLOCK_SIZE)
-
-    # NOTE a trick to come across triton's requirement that values in both first and second input shapes must be >= 16,
-    # Multiplying two tensors with shapes [1, d] * [d, block_size] will fail.
-    # Refer to https://github.com/openai/triton/discussions/895
-    S_ij += tl.sum(q[None, :] * k_cur_block, 1)
-    S_ij *= sm_scale
-    S_ij -= alibi_slope * (cur_kv_seq_len - 1 - position_k_offset)
-    S_ij = tl.where(cur_kv_seq_len > position_k_offset, S_ij, float("-inf"))
-
-    m = tl.max(S_ij, 0)
-    S_ij -= m
-    p_ij_hat = tl.exp(S_ij)
-    l_i = tl.sum(p_ij_hat, 0)
-    p_ij_hat = p_ij_hat.to(v_cur_block.type.element_ty)
-    acc += tl.sum(v_cur_block * p_ij_hat[:, None], 0)
-    acc = acc / l_i
-
-    offsets_mid_o = (
-        cur_token_idx * stride_mid_ot
-        + cur_head_idx * stride_mid_oh
-        + block_start_kv * stride_mid_ob
-        + offsets_dmodel * stride_mid_od
-    )
-    tl.store(mid_o + offsets_mid_o, acc)
-    offsets_mid_o_lse = (
-        cur_token_idx * stride_mid_o_lset + cur_head_idx * stride_mid_o_lseh + block_start_kv * stride_mid_o_lseb
-    )
-    # logsumexp l_i^(j) = m^(j) + log(l_i^(j))
-    tl.store(mid_o_lse + offsets_mid_o_lse, m + tl.log(l_i))
+    .. _ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html
+    """
+    return tl.where(x >= 0, x, 0.0)
 
 
-# Triton 2.1.0
 @triton.jit
-def _flash_decoding_fwd_reduce_kernel(
-    mid_o,  # [batch_size, head_num, kv_split_num, head_dim]
-    mid_o_lse,  # [batch_size, head_num, kv_split_num]
-    O,  # [batch_size, num_heads, head_dim] or [batch_size, 1, num_heads, head_dim]
-    kv_seq_len,
-    q_len,
-    batch_size,
-    stride_mid_ot,
-    stride_mid_oh,
-    stride_mid_ob,
-    stride_mid_od,
-    stride_o_lset,
-    stride_o_lseh,
-    stride_o_lseb,
-    stride_ot,
-    stride_oh,
-    stride_od,
-    BLOCK_KV: tl.constexpr,
-    HEAD_DIM: tl.constexpr,
-):
-    cur_token_idx = tl.program_id(0)
-    cur_seq_idx = cur_token_idx // q_len
-    if cur_seq_idx >= batch_size:
-        return
-    cur_head_idx = tl.program_id(1)
-
-    # cur_token_off is used as a "mask" here for spec-dec during verification process
-    cur_token_off = (cur_token_idx % q_len) - q_len + 1
-    cur_kv_seq_len = tl.load(kv_seq_len + cur_seq_idx) + cur_token_off
-    offsets_dmodel = tl.arange(0, HEAD_DIM)
-
-    # NOTE currently the block size BLOCK_KV splitting kv is relatively small as we have
-    # BLOCK_KV == BLOCK_SIZE for now. We might want to decrease the number of blocks of kv splitted.
-    kv_split_num = (cur_kv_seq_len + BLOCK_KV - 1) // BLOCK_KV
-    m_i = float("-inf")  # max logic
-    l_i = 0.0  # sum exp
-    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)
-
-    offsets_mid_o = cur_token_idx * stride_mid_ot + cur_head_idx * stride_mid_oh + offsets_dmodel
-    offset_mid_lse = cur_token_idx * stride_o_lset + cur_head_idx * stride_o_lseh
-    for block_i in range(0, kv_split_num, 1):
-        mid_o_block = tl.load(mid_o + offsets_mid_o + block_i * stride_mid_ob)
-        lse = tl.load(mid_o_lse + offset_mid_lse + block_i * stride_o_lseb)
-        m_ij = tl.maximum(m_i, lse)
-        scale = tl.exp(m_i - m_ij)
-        acc = acc * scale
-        lse -= m_ij
-        exp_logic = tl.exp(lse)
-        acc += exp_logic * mid_o_block
-        l_i = scale * l_i + exp_logic
-        m_i = m_ij
-
-    acc = acc / l_i
-    offsets_O = cur_token_idx * stride_ot + cur_head_idx * stride_oh + offsets_dmodel
-    tl.store(O + offsets_O, acc.to(O.type.element_ty))
-    return
-
-
-# Decoding Stage
-# Used with blocked KV Cache (PagedAttention)
-def flash_decoding_attention(
-    q: torch.Tensor,
-    k_cache: torch.Tensor,
-    v_cache: torch.Tensor,
-    kv_seq_len: torch.Tensor,
-    block_tables: torch.Tensor,
-    block_size: int,
-    max_seq_len_in_batch: int = None,
-    output: torch.Tensor = None,
-    mid_output: torch.Tensor = None,
-    mid_output_lse: torch.Tensor = None,
-    alibi_slopes: torch.Tensor = None,
-    sm_scale: int = None,
-    kv_group_num: int = 1,
-    q_len: int = 1,  # NOTE alibi flash decoding does not support q_len > 1 at this moment.
-    use_new_kcache_layout: bool = False,
+def squared_relu(x):
+    """
+    Squared ReLU activation, as proposed in the Primer_ paper.
+
+    .. _Primer: https://arxiv.org/abs/2109.08668
+    """
+    x_sq = x * x
+    return tl.where(x > 0.0, x_sq, 0.0)
+
+
+@triton.jit
+def star_relu(x):
+    """
+    Star ReLU activation, as proposed in the "MetaFormer Baselines for Vision"_ paper.
+
+    .. _ "MetaFormer Baselines for Vision": https://arxiv.org/pdf/2210.13452.pdf
+    """
+    x_sq = x * x
+    return 0.8944 * tl.where(x > 0.0, x_sq, 0.0) - 0.4472
+
+
+# Leaky ReLU
+@triton.jit
+def leaky_relu(x):
+    """
+    LeakyReLU_ activation
+
+    .. _LeakyReLU: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html
+    """
+    return tl.where(x >= 0.0, x, 0.01 * x)
+
+
+@triton.jit
+def gelu(x):
+    """
+    GeLU_ activation - Gaussian error linear unit
+
+    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf
+    """
+    return 0.5 * x * (1 + tanh(_kAlpha * (x + 0.044715 * x * x * x)))
+
+
+@triton.jit
+def smelu(x):
+    """
+    SmeLU_ activation -  Smooth ReLU with beta=2.0
+
+    .. _SmeLU: https://arxiv.org/pdf/2202.06499.pdf
+    """
+    beta = 2.0
+
+    relu = tl.where(x >= beta, x, 0.0)
+    return tl.where(tl.abs(x) <= beta, (x + beta) * (x + beta) / (4.0 * beta), relu)
+
+
+@triton.jit
+def silu(x):
+    return x * tl.sigmoid(x)
+
+
+@autotune(
+    configs=[
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 256, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=2, num_warps=8
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 64, "GROUP_SIZE_M": 8}, num_stages=3, num_warps=8
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 32, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 128, "GROUP_SIZE_M": 8}, num_stages=2, num_warps=4
+        ),
+    ],
+    key=["M", "N", "K"],
+    nearest_power_of_two=True,
+    prune_configs_by={
+        "early_config_prune": matmul248_kernel_config_pruner,
+        "perf_model": None,
+        "top_k": None,
+    },
+)
+@triton.jit
+def cai_gptq_matmul_248_kernel(
+    a_ptr,
+    b_ptr,
+    c_ptr,
+    scales_ptr,
+    zeros_ptr,
+    bias_ptr,
+    residual_ptr,
+    M,
+    N,
+    K,
+    bits,
+    maxq,
+    gptq_group_size,
+    stride_am,
+    stride_ak,
+    stride_bk,
+    stride_bn,
+    stride_cm,
+    stride_cn,
+    stride_scales,
+    stride_zeros,
+    QKV_FUSED: tl.constexpr,
+    ADD_BIAS: tl.constexpr,
+    ADD_RESIDUAL: tl.constexpr,
+    ACT_TYPE: tl.constexpr,
+    BLOCK_SIZE_M: tl.constexpr,
+    BLOCK_SIZE_N: tl.constexpr,
+    BLOCK_SIZE_K: tl.constexpr,
+    GROUP_SIZE_M: tl.constexpr,
 ):
     """
-    Flash decoding implemented with a blocked KV Cache (PagedAttention) during decoding stage.
+    Compute the matrix multiplication C = A x B.
+    A is of shape (M, K) float16
+    B is of shape (K//8, N) int32
+    C is of shape (M, N) float16
+    scales is of shape (G, N) float16
+    zeros is of shape (G, N) float16
+    """
+    infearure_per_bits = 32 // bits
 
-    Args:
-        q (torch.Tensor): [bsz * q_len, num_heads, head_dim]
-            q_len > 1 only for verification process in speculative-decoding.
-        k_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim]
-        v_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim]
-        kv_seq_len (torch.Tensor): [batch_size]
-            records the (kv) sequence lengths incorporating past kv sequence lengths.
-        block_tables (torch.Tensor): [batch_size, max_blocks_per_sequence]
-        max_seq_len_in_batch (int): Maximum sequence length in the batch.
-        output (torch.Tensor):  [bsz, num_heads * head_dim]
-        mid_output (torch.Tensor): [max_bsz * q_len, num_heads, kv_max_split_num, head_dim]
-            Intermediate output tensor. `max_bsz` should be greater than or equal to `bsz`.
-            q_len > 1 only for verification process in speculative-decoding.
-        mid_output_lse (torch.Tensor): [max_bsz * q_len, num_heads, kv_max_split_num]
-            Log-sum-exp of intermediate output. `max_bsz` should be greater than or equal to `bsz`.
-            q_len > 1 only for verification process in speculative-decoding.
-        alibi_slopes (torch.Tensor): [num_heads] alibi slopes used for alibi flash decoding.
-        block_size (int): Size of each block in the blocked key/value cache.
-        num_kv_group (int, optional): Number of key/value groups. Defaults to 1.
-        q_length (int): Query length. Use for speculative decoding when `q_length` > 1 (i.e. the last n tokens).
-            Defaults to 1.
-        use_new_kcache_layout (bool): Whether to use the new kcache layout. Defaults to False.
-
-    Returns:
-        Output tensor with shape [bsz * q_len, num_heads * head_dim]
-    """
-    q = q.squeeze() if q.dim() == 4 else q
-    assert q.dim() == 3, f"Incompatible q dim: {q.dim()}"
-    n_tokens, num_heads, head_dim = q.shape
-    assert n_tokens % q_len == 0, "Invalid q_len"
-    bsz = n_tokens // q_len
-
-    assert head_dim in {32, 64, 128, 256}
-    assert kv_seq_len.shape[0] == block_tables.shape[0] == bsz, (
-        f"Got incompatible batch size (number of seqs):\n"
-        f"  KV seq lengths bsz {kv_seq_len.size(0)}, Block tables bsz {block_tables.size(0)}, "
-        f"batch size {bsz}"
-    )
-    assert k_cache.size(-2) == v_cache.size(-2) == block_size, (
-        f"Got incompatible block size on kv caches:\n"
-        f"  assigned block_size {block_size}, k_cache block_size {k_cache.size(-2)}, "
-        f"v_cache block_size {v_cache.size(-2)}"
-    )
+    pid = tl.program_id(axis=0)
+    NK = K
 
-    # NOTE BLOCK_KV could be considered as block splitting the sequence on k/v
-    # For now, BLOCK_KV is supposed to be equivalent with the size of physical cache block (i.e.`block_size`)
-    assert block_size in {16, 32, 64, 128}
-    BLOCK_KV = block_size
-
-    sm_scale = 1.0 / (head_dim**0.5) if sm_scale is None else sm_scale
-    max_seq_len_in_batch = kv_seq_len.max().item() if max_seq_len_in_batch is None else max_seq_len_in_batch
-    # For compatibility (TODO revise modeling in future)
-    kv_max_split_num = (max_seq_len_in_batch + BLOCK_KV - 1) // BLOCK_KV
-
-    if mid_output is None:
-        mid_output = torch.empty(
-            (bsz * q_len, num_heads, kv_max_split_num, head_dim), dtype=torch.float32, device=q.device
-        )
-    if mid_output_lse is None:
-        mid_output_lse = torch.empty((bsz * q_len, num_heads, kv_max_split_num), dtype=torch.float32, device=q.device)
-    if output is None:
-        # A hack to prevent `view` operation in modeling
-        output = torch.empty((bsz * q_len, num_heads * head_dim), dtype=q.dtype, device=q.device)
-
-    assert (
-        mid_output.size(2) == mid_output_lse.size(2) >= kv_max_split_num
-    ), "Incompatible kv split number of intermediate output tensors"
-    assert (
-        mid_output.size(0) == mid_output_lse.size(0) >= output.size(0) == n_tokens
-    ), f"Incompatible first dimension of output tensors"
-
-    # NOTE use `triton.next_power_of_2` here to utilize the cache mechanism of triton
-    # To optimize, revise batching/scheduling to batch 2^n sequences in a batch (preferred)
-    grid = lambda META: (
-        triton.next_power_of_2(bsz * q_len),
-        num_heads,
-        triton.cdiv(triton.next_power_of_2(max_seq_len_in_batch), META["BLOCK_KV"]),
-    )
+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
+    num_pid_k = tl.cdiv(NK, BLOCK_SIZE_K)
+    qkv_offset = pid // (num_pid_m * num_pid_n)
+    pid = pid % (num_pid_m * num_pid_n)
+    num_pid_in_group = GROUP_SIZE_M * num_pid_n
+    group_id = pid // num_pid_in_group
+    first_pid_m = group_id * GROUP_SIZE_M
+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
+    pid_m = first_pid_m + (pid % group_size_m)
+    pid_n = (pid % num_pid_in_group) // group_size_m
+
+    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
+    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
+    offs_k = tl.arange(0, BLOCK_SIZE_K)
+    # offs_bk = offs_k + qkv_offset * NK
+    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
+
+    a_mask = offs_am[:, None] < M
+    # b_ptrs is set up such that it repeats elements along the K axis 8 times
+    b_ptrs = (
+        b_ptr
+        + qkv_offset * N * NK // infearure_per_bits
+        + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)
+    )  # (BLOCK_SIZE_K, BLOCK_SIZE_N)
+    # g_ptrs = g_ptr + offs_k
+    # shifter is used to extract the N bits of each element in the 32-bit word from B
+    scales_ptrs = scales_ptr + qkv_offset * NK * N // gptq_group_size + offs_bn[None, :]
+    zeros_ptrs = (
+        zeros_ptr
+        + qkv_offset * NK * N // gptq_group_size // infearure_per_bits
+        + (offs_bn[None, :] // infearure_per_bits)
+    )
+
+    shifter = (offs_k % infearure_per_bits) * bits
+    zeros_shifter = (offs_bn % infearure_per_bits) * bits
+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+    g_idx_base = tl.arange(0, BLOCK_SIZE_K)
+    g_idx_base = g_idx_base // gptq_group_size
+    g_idx = g_idx_base
+    # tl.device_print("gidx, ", g_idx)
+
+    scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
+    zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
+    zeros = (zeros >> zeros_shifter[None, :]) & maxq
+    zeros = zeros + 1
+
+    for k in range(0, num_pid_k):
+        # g_idx = tl.load(g_ptrs)
+        # if (k + 1) * BLOCK_SIZE_K > currend_group_end:
+        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
+        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
+        zeros = (zeros >> zeros_shifter[None, :]) & maxq
+        zeros = zeros + 1
+        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop
+        a = tl.load(a_ptrs, mask=a_mask, other=0.0)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
+        b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated
+        # Now we need to unpack b (which is N-bit values) into 32-bit values
+        b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values
+        b = (b - zeros).to(tl.float16) * scales  # Scale and shift
+        accumulator += tl.dot(a, b)
+
+        a_ptrs += BLOCK_SIZE_K
+        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk
+        g_idx = g_idx_base + ((k + 1) * BLOCK_SIZE_K) // gptq_group_size
+        # if (k + 2) * BLOCK_SIZE_K > currend_group_end:
+
+    c_ptrs = c_ptr + qkv_offset * M * N + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]
+    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)
+
+    if ADD_BIAS:
+        bias_mask = offs_bn < N
+        offs_bn += qkv_offset * N
+        bias_ptrs = bias_ptr + stride_cn * offs_bn
+        bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
+        accumulator += bias[None, :]
+
+    if ACT_TYPE == 1:
+        accumulator = relu(accumulator)
+    elif ACT_TYPE == 2:
+        accumulator = gelu(accumulator)
+    elif ACT_TYPE == 3:
+        accumulator = silu(accumulator)
+
+    if ADD_RESIDUAL:
+        residual_ptrs = residual_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]
+        res = tl.load(residual_ptrs, mask=c_mask, other=0.0)
+        accumulator += res
+
+    tl.store(c_ptrs, accumulator, mask=c_mask)
+
+
+# Adapted from AutoGPTQ auto_gptq: https://github.com/PanQiWei/AutoGPTQ
+@autotune(
+    configs=[
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 256, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=2, num_warps=8
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 64, "GROUP_SIZE_M": 8}, num_stages=3, num_warps=8
+        ),
+        triton.Config(
+            {"BLOCK_SIZE_M": 32, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 128, "GROUP_SIZE_M": 8}, num_stages=2, num_warps=4
+        ),
+    ],
+    key=["M", "N", "K"],
+    nearest_power_of_two=True,
+    prune_configs_by={
+        "early_config_prune": matmul248_kernel_config_pruner,
+        "perf_model": None,
+        "top_k": None,
+    },
+)
+@triton.jit
+def cai_gptq_idx_matmul_248_kernel(
+    a_ptr,
+    b_ptr,
+    c_ptr,
+    scales_ptr,
+    zeros_ptr,
+    idx_ptr,
+    bias_ptr,
+    residual_ptr,
+    M,
+    N,
+    K,
+    bits,
+    maxq,
+    gptq_group_size,
+    stride_am,
+    stride_ak,
+    stride_bk,
+    stride_bn,
+    stride_cm,
+    stride_cn,
+    stride_scales,
+    stride_zeros,
+    QKV_FUSED: tl.constexpr,
+    ADD_BIAS: tl.constexpr,
+    ADD_RESIDUAL: tl.constexpr,
+    ACT_TYPE: tl.constexpr,
+    BLOCK_SIZE_M: tl.constexpr,
+    BLOCK_SIZE_N: tl.constexpr,
+    BLOCK_SIZE_K: tl.constexpr,
+    GROUP_SIZE_M: tl.constexpr,
+):
+    """
+    Compute the matrix multiplication C = A x B.
+    A is of shape (M, K) float16
+    B is of shape (K//8, N) int32
+    C is of shape (M, N) float16
+    scales is of shape (G, N) float16
+    zeros is of shape (G, N) float16
+    """
+    infearure_per_bits = 32 // bits
 
-    if alibi_slopes is not None:
-        # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
-        # the code (alibi kernel) will be refactored later to avoid code duplication, when
-        # the whole triton flow with new k cache layout has been supported and tested.
-        assert (
-            not use_new_kcache_layout
-        ), "Alibi Slopes will be supported with new kcache layout later when the whole triton flow is ready"
-
-        _alibi_flash_decoding_fwd_kernel[grid](
-            q,
-            k_cache,
-            v_cache,
-            block_tables,
-            mid_output,
-            mid_output_lse,
-            kv_seq_len,
-            q_len,
-            bsz,
-            alibi_slopes,
-            q.stride(0),
-            q.stride(1),
-            q.stride(2),
-            k_cache.stride(0),
-            k_cache.stride(1),
-            k_cache.stride(2),
-            k_cache.stride(3),
-            block_tables.stride(0),
-            block_tables.stride(1),
-            mid_output.stride(0),
-            mid_output.stride(1),
-            mid_output.stride(2),
-            mid_output.stride(3),
-            mid_output_lse.stride(0),
-            mid_output_lse.stride(1),
-            mid_output_lse.stride(2),
-            sm_scale,
-            KV_GROUPS=kv_group_num,
-            BLOCK_KV=block_size,
-            BLOCK_SIZE=block_size,
-            HEAD_DIM=head_dim,
-        )
-    else:
-        # For KCache and VCache with the same layout
-        x = head_dim
-        kcsplit_x_stride, kcs_stride, kcd_stride = 0, k_cache.stride(2), k_cache.stride(3)
-        # For KCache layout [num_blocks, num_kv_heads, head_dim//x, block_size, x]
-        if use_new_kcache_layout:
-            assert (
-                k_cache.dim() == 5
-                and k_cache.shape[1] == v_cache.shape[1]
-                and k_cache.shape[2] * k_cache.shape[4] == v_cache.shape[3]
-            ), f"Invalid KCache shape {k_cache.shape} and VCache shape {v_cache.shape}"
-            x = k_cache.size(-1)
-            kcsplit_x_stride, kcs_stride, kcd_stride = k_cache.stride()[-3:]
-
-        _flash_decoding_fwd_kernel[grid](
-            q,
-            k_cache,
-            v_cache,
-            block_tables,
-            mid_output,
-            mid_output_lse,
-            kv_seq_len,
-            q_len,
-            bsz,
-            kv_group_num,
-            x,
-            sm_scale,
-            q.stride(0),
-            q.stride(1),
-            q.stride(2),
-            k_cache.stride(0),
-            k_cache.stride(1),
-            kcsplit_x_stride,
-            kcs_stride,
-            kcd_stride,
-            v_cache.stride(0),
-            v_cache.stride(1),
-            v_cache.stride(2),
-            v_cache.stride(3),
-            block_tables.stride(0),
-            block_tables.stride(1),
-            mid_output.stride(0),
-            mid_output.stride(1),
-            mid_output.stride(2),
-            mid_output.stride(3),
-            mid_output_lse.stride(0),
-            mid_output_lse.stride(1),
-            mid_output_lse.stride(2),
-            BLOCK_KV=block_size,
-            BLOCK_SIZE=block_size,
-            HEAD_DIM=head_dim,
-        )
-
-    grid = (triton.next_power_of_2(bsz * q_len), num_heads)
-    _flash_decoding_fwd_reduce_kernel[grid](
-        mid_output,
-        mid_output_lse,
-        output,
-        kv_seq_len,
-        q_len,
-        bsz,
-        mid_output.stride(0),
-        mid_output.stride(1),
-        mid_output.stride(2),
-        mid_output.stride(3),
-        mid_output_lse.stride(0),
-        mid_output_lse.stride(1),
-        mid_output_lse.stride(2),
-        output.stride(0),
-        head_dim,
-        1,
-        BLOCK_KV=block_size,
-        HEAD_DIM=head_dim,
-    )
+    pid = tl.program_id(axis=0)
+    NK = K
 
-    return output
+    # if QKV_FUSED:
+    #     NK = K//3
+    # else:
+    #     NK = K
+    # NK = K
+
+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
+    num_pid_k = tl.cdiv(NK, BLOCK_SIZE_K)
+    qkv_offset = pid // (num_pid_m * num_pid_n)
+    pid = pid % (num_pid_m * num_pid_n)
+    num_pid_in_group = GROUP_SIZE_M * num_pid_n
+    group_id = pid // num_pid_in_group
+    first_pid_m = group_id * GROUP_SIZE_M
+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
+    pid_m = first_pid_m + (pid % group_size_m)
+    pid_n = (pid % num_pid_in_group) // group_size_m
+
+    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
+    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
+    offs_k = tl.arange(0, BLOCK_SIZE_K)
+    # offs_bk = offs_k + qkv_offset * NK
+    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
+
+    a_mask = offs_am[:, None] < M
+    # b_ptrs is set up such that it repeats elements along the K axis 8 times
+    b_ptrs = (
+        b_ptr
+        + qkv_offset * N * NK // infearure_per_bits
+        + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)
+    )  # (BLOCK_SIZE_K, BLOCK_SIZE_N)
+    # g_ptrs = g_ptr + offs_k
+    # shifter is used to extract the N bits of each element in the 32-bit word from B
+    scales_ptrs = scales_ptr + qkv_offset * NK * N // gptq_group_size + offs_bn[None, :]
+    zeros_ptrs = (
+        zeros_ptr
+        + qkv_offset * NK * N // gptq_group_size // infearure_per_bits
+        + (offs_bn[None, :] // infearure_per_bits)
+    )
+
+    shifter = (offs_k % infearure_per_bits) * bits
+    zeros_shifter = (offs_bn % infearure_per_bits) * bits
+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+    g_ptrs = idx_ptr + offs_k
+    g_idx = tl.load(g_ptrs)
+    # tl.device_print("gidx, ", g_idx)
+    zeros_ptrs = zeros_ptr + (offs_bn[None, :] // infearure_per_bits)
+
+    scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
+
+    for k in range(0, num_pid_k):
+        g_idx = tl.load(g_ptrs)
+        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
+        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
+
+        zeros = (zeros >> zeros_shifter[None, :]) & maxq
+        zeros = zeros + 1
+
+        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop
+        a = tl.load(a_ptrs, mask=a_mask, other=0.0)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
+        b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated
+        # Now we need to unpack b (which is N-bit values) into 32-bit values
+        b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values
+        b = (b - zeros).to(tl.float16) * scales  # Scale and shift
+        accumulator += tl.dot(a, b)
+
+        a_ptrs += BLOCK_SIZE_K
+        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk
+        g_ptrs += BLOCK_SIZE_K
+
+    c_ptrs = c_ptr + qkv_offset * M * N + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]
+    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)
+
+    if ADD_BIAS:
+        bias_mask = offs_bn < N
+        offs_bn += qkv_offset * N
+        bias_ptrs = bias_ptr + stride_cn * offs_bn
+        bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
+        accumulator += bias[None, :]
+
+    if ACT_TYPE == 1:
+        accumulator = relu(accumulator)
+    elif ACT_TYPE == 2:
+        accumulator = gelu(accumulator)
+    elif ACT_TYPE == 3:
+        accumulator = silu(accumulator)
+
+    if ADD_RESIDUAL:
+        residual_ptrs = residual_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]
+        res = tl.load(residual_ptrs, mask=c_mask, other=0.0)
+        accumulator += res
+
+    tl.store(c_ptrs, accumulator, mask=c_mask)
+
+
+def gptq_fused_linear_triton(
+    input,
+    qweight,
+    scales,
+    qzeros,
+    bias,
+    residual,
+    bits,
+    maxq,
+    gptq_group_size,
+    qkv_fused,
+    add_bias,
+    add_residual,
+    g_idx=None,
+    act_type=0,
+):
+    # print("gptq fused ", qkv_fused, add_bias, add_residual)
+    assert input.is_cuda, "input is not in cuda"
+    assert qweight.is_cuda, "qweight is not in cuda"
+    assert scales.is_cuda, "scales is not in cuda"
+    assert qzeros.is_cuda, "qzeros is not in cuda"
+
+    with torch.cuda.device(input.device):
+        if qkv_fused:
+            grid = lambda META: (
+                triton.cdiv(input.shape[0], META["BLOCK_SIZE_M"])
+                * triton.cdiv(qweight.shape[1], META["BLOCK_SIZE_N"])
+                * 3,
+            )
+            output = torch.empty((input.shape[0] * 3, qweight.shape[1]), device=input.device, dtype=torch.float16)
+        else:
+            grid = lambda META: (
+                triton.cdiv(input.shape[0], META["BLOCK_SIZE_M"]) * triton.cdiv(qweight.shape[1], META["BLOCK_SIZE_N"]),
+            )
+            output = torch.empty((input.shape[0], qweight.shape[1]), device=input.device, dtype=torch.float16)
+        # print("dtype, ", qweight.dtype, output.dtype, scales.dtype, qzeros.dtype, bias.dtype, residual.dtype)
+        if g_idx is None:
+            cai_gptq_matmul_248_kernel[grid](
+                input,
+                qweight,
+                output,
+                scales,
+                qzeros,
+                bias,
+                residual,
+                input.shape[0],
+                qweight.shape[1],
+                input.shape[1],
+                bits,
+                maxq,
+                gptq_group_size,
+                input.stride(0),
+                input.stride(1),
+                qweight.stride(0),
+                qweight.stride(1),
+                output.stride(0),
+                output.stride(1),
+                scales.stride(0),
+                qzeros.stride(0),
+                QKV_FUSED=qkv_fused,
+                ADD_BIAS=add_bias,
+                ADD_RESIDUAL=add_residual,
+                ACT_TYPE=act_type,
+            )
+        else:
+            cai_gptq_idx_matmul_248_kernel[grid](
+                input,
+                qweight,
+                output,
+                scales,
+                qzeros,
+                g_idx,
+                bias,
+                residual,
+                input.shape[0],
+                qweight.shape[1],
+                input.shape[1],
+                bits,
+                maxq,
+                gptq_group_size,
+                input.stride(0),
+                input.stride(1),
+                qweight.stride(0),
+                qweight.stride(1),
+                output.stride(0),
+                output.stride(1),
+                scales.stride(0),
+                qzeros.stride(0),
+                QKV_FUSED=qkv_fused,
+                ADD_BIAS=add_bias,
+                ADD_RESIDUAL=add_residual,
+                ACT_TYPE=act_type,
+            )
+        if qkv_fused:
+            return output.view(3, input.shape[0], qweight.shape[1])
+        else:
+            return output
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/triton/llama_act_combine_kernel.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/llama_act_combine_kernel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/triton/qkv_matmul_kernel.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/qkv_matmul_kernel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/kernel/triton/softmax.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/lazy/construction.py` & `colossalai-nightly-2024.5.4/colossalai/lazy/construction.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/lazy/lazy_init.py` & `colossalai-nightly-2024.5.4/colossalai/lazy/lazy_init.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/lazy/pretrained.py` & `colossalai-nightly-2024.5.4/colossalai/lazy/pretrained.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-import copy
 import os
 from typing import Callable, Optional, Union
 
 import torch
 from torch.nn import Module
 
 from colossalai.interface import pretrained as pretrained_interface
@@ -71,32 +70,14 @@
     from_pipeline = kwargs.pop("_from_pipeline", None)
     from_auto_class = kwargs.pop("_from_auto", False)
     _fast_init = kwargs.pop("_fast_init", True)
     torch_dtype = kwargs.pop("torch_dtype", None)
     subfolder = kwargs.pop("subfolder", "")
     commit_hash = kwargs.pop("_commit_hash", None)
     variant = kwargs.pop("variant", None)
-
-    kwargs.pop("state_dict", None)
-    kwargs.pop("from_tf", False)
-    kwargs.pop("from_flax", False)
-    kwargs.pop("output_loading_info", False)
-    kwargs.pop("trust_remote_code", None)
-    kwargs.pop("low_cpu_mem_usage", None)
-    kwargs.pop("device_map", None)
-    kwargs.pop("max_memory", None)
-    kwargs.pop("offload_folder", None)
-    kwargs.pop("offload_state_dict", False)
-    kwargs.pop("load_in_8bit", False)
-    kwargs.pop("load_in_4bit", False)
-    kwargs.pop("quantization_config", None)
-    kwargs.pop("adapter_kwargs", {})
-    kwargs.pop("adapter_name", "default")
-    kwargs.pop("use_flash_attention_2", False)
-
     use_safetensors = kwargs.pop("use_safetensors", None if is_safetensors_available() else False)
 
     if len(kwargs) > 0:
         logger.warning(f"Below kwargs may be ignored: {list(kwargs.keys())}")
 
     from_pt = True
 
@@ -123,18 +104,14 @@
             revision=revision,
             subfolder=subfolder,
             _from_auto=from_auto_class,
             _from_pipeline=from_pipeline,
             **kwargs,
         )
     else:
-        config = copy.deepcopy(config)
-        kwarg_attn_imp = kwargs.pop("attn_implementation", None)
-        if kwarg_attn_imp is not None and config._attn_implementation != kwarg_attn_imp:
-            config._attn_implementation = kwarg_attn_imp
         model_kwargs = kwargs
 
     if commit_hash is None:
         commit_hash = getattr(config, "_commit_hash", None)
 
     # This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the
     # index of the files.
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/apex_amp/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/apex_amp/apex_amp.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/apex_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/naive_amp/naive_amp.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/naive_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/torch_amp/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/torch_amp/_grad_scaler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/amp/torch_amp/torch_amp.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/torch_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/builder/builder.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/builder/builder.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/communication/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/communication/collective.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/collective.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/communication/p2p.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/communication/p2p_v2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/communication/ring.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/ring.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/communication/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/constants.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/parallel_context.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/parallel_mode.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_mode.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_1d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_1d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_2d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_3d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_data.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_data.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_model.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_sequence.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_sequence.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/initializer_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/process_group_initializer/process_group_initializer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/process_group_initializer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/random/_helper.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/random/_helper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/context/random/seed_manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/random/seed_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/_base_engine.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/_base_engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_accumulation/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/gradient_handler/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/_base_schedule.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_base_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/_pipeline_schedule.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/global_variables.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/global_variables.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/async_engine.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/async_manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_manager.py`

 * *Files 1% similar despite different names*

```diff
@@ -51,22 +51,22 @@
         has_new_finished = False
         if self.running_batch is None:
             new_batch = self.req_queue.generate_new_batch(self.running_batch)
             if new_batch is not None:
                 self.stats_tool.count_prompt_tokens(new_batch)
                 self.running_batch = new_batch
                 has_new_finished, outputs = self._prefill_batch(self.running_batch)
-                self._filter_running_batch()
+                self._filter_runing_batch()
                 self.has_wait_tokens = 0
 
         else:
             if self.has_wait_tokens < self.max_wait_tokens:
                 self.stats_tool.count_output_tokens(self.running_batch)
                 has_new_finished, outputs = self._decode_batch(self.running_batch)
-                self._filter_running_batch()
+                self._filter_runing_batch()
                 self.has_wait_tokens += 1
 
             else:
                 new_mini_batch = self.req_queue.generate_new_batch(self.running_batch)
                 if new_mini_batch is not None:
                     self.stats_tool.count_prompt_tokens(new_mini_batch)
                     has_new_finished, outputs = self._prefill_batch(new_mini_batch)
@@ -74,15 +74,15 @@
                         self._merge_batch(self.running_batch, new_mini_batch)
                         self.running_batch.merge(new_mini_batch)
                     self.has_wait_tokens = 0
 
                 else:
                     self.stats_tool.count_output_tokens(self.running_batch)
                     has_new_finished, outputs = self._decode_batch(self.running_batch)
-                    self._filter_running_batch()
+                    self._filter_runing_batch()
                     self.has_wait_tokens += 1
 
         if has_new_finished:
             return outputs
         return None
 
     def _prefill_batch(self, batch):
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/infer_batch.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/infer_batch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/ray_init_config.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_init_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/req_queue.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/req_queue.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/sampling_params.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/sampling_params.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/dynamic_batching/stats.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/stats.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/engine.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/engine.py`

 * *Files 0% similar despite different names*

```diff
@@ -129,15 +129,15 @@
 
         Returns:
             out (list): a list of output data, each element is a list of token.
             timestamp (float): the time cost of the inference, only return when verbose is `True`.
         """
         assert isinstance(
             input_list, (BatchEncoding, dict)
-        ), f"Only accept BatchEncoding or dict as input, but got {input_list.__class__.__name__}."
+        ), f"Only accept BatchEncoding or dict as input, but get {input_list.__class__.__name__}."
         if isinstance(input_list, BatchEncoding):
             input_list = input_list.data
         out, timestamp = self.schedule.generate_step(self.model, iter([input_list]))
         if self.verbose:
             return out, timestamp
         else:
             return out
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/modeling/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/modeling/llama.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/hybridengine/polices/llama.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -127,22 +127,22 @@
 
         if self.running_batch is None:
             new_batch = self.req_queue.generate_new_batch(self.running_batch)
             if new_batch is not None:
                 self.stats_tool.count_prompt_tokens(new_batch)
                 self.running_batch = new_batch
                 yield from self._prefill_batch(self.running_batch)
-                self._filter_running_batch()
+                self._filter_runing_batch()
                 self.has_wait_tokens = 0
             return
 
         if self.has_wait_tokens < self.max_wait_tokens:
             self.stats_tool.count_output_tokens(self.running_batch)
             yield from self._decode_batch(self.running_batch)
-            self._filter_running_batch()
+            self._filter_runing_batch()
             self.has_wait_tokens += 1
             return
         else:
             new_mini_batch = self.req_queue.generate_new_batch(self.running_batch)
             if new_mini_batch is not None:
                 self.stats_tool.count_prompt_tokens(new_mini_batch)
                 yield from self._prefill_batch(new_mini_batch)
@@ -150,15 +150,15 @@
                     self._merge_batch(self.running_batch, new_mini_batch)
                     self.running_batch.merge(new_mini_batch)
                 self.has_wait_tokens = 0
 
             else:
                 self.stats_tool.count_output_tokens(self.running_batch)
                 yield from self._decode_batch(self.running_batch)
-                self._filter_running_batch()
+                self._filter_runing_batch()
                 self.has_wait_tokens += 1
 
         return
 
     def _init_batch(self, batch: Batch, dtype="fp16"):
         reqs = [r.to_rpc_obj() for r in batch.reqs]
         batch_id = batch.batch_id
@@ -239,15 +239,15 @@
             finished_reqs = batch.filter_finished()
             if batch.is_clear():
                 self._remove_batch(batch)
             else:
                 self._filter_batch(batch)
             yield from self._output_process(finished_reqs)
 
-    def _filter_running_batch(self):
+    def _filter_runing_batch(self):
         if self.running_batch is not None and self.running_batch.is_clear():
             self.running_batch = None
 
     def _add_token_id_to_req(self, batch: Batch, req_ans):
         for req_id, (new_token_id, new_gen_metadata) in req_ans.items():
             req = batch.id_to_reqs[req_id]
             req.output_ids.append(new_token_id)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/pipeline/microbatch_manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/microbatch_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py` & `colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/batch_infer_state.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/engine.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/modeling/llama.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/initialize.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/initialize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/_ops/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/base_layer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/base_layer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/dropout.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/dropout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/linear.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/colossalai_layer/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_1d/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_1d/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_1d/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2d/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2d/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2d/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_2p5d/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_3d/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_3d/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_3d/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_sequence/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/parallel_sequence/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/utils/common.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/loss_1d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_1d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/loss_2d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/loss_2p5d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/loss/loss_3d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/accuracy_2d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/accuracy_2p5d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/metric/accuracy_3d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/data_parallel.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/data_parallel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/colo_module.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/colo_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/linear.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/layers/module_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/module_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/nn/parallel/reducer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/reducer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/layer_spec.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/layer_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/middleware/adaptor/fx.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/fx.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/middleware/topo.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/topo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/pipelinable.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipelinable.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/pipeline_process_group.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipeline_process_group.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/rpc/_pipeline_base.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/rpc/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/pipeline/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/registry/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/registry/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/registry/registry.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/registry/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/tensor/compute_spec.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/compute_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/tensor/dist_spec_mgr.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/dist_spec_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/tensor/distspec.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/distspec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/tensor/op_wrapper.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/op_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/tensor/process_group.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/process_group.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/tensor/tensor_spec.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/tensor_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/trainer/_trainer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/_trainer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_base_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_base_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_checkpoint_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_checkpoint_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_log_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_log_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/trainer/hooks/_metric_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_metric_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/activation_checkpoint.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/activation_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/checkpoint/module_checkpoint.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/module_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/checkpoint/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/checkpointing.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpointing.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/common.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/memory.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/memory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/legacy/comm_profiler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/comm_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/legacy/prof_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/prof_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/profiler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_monitor.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,134 +1,149 @@
-import os
-import threading
-import time
-from enum import Enum
-from typing import List
+import json
+from abc import abstractmethod
+from concurrent.futures import ThreadPoolExecutor
+from time import sleep, time
 
 import torch
 
-from colossalai.gemini.ophooks import BaseOpHook
-from colossalai.gemini.stateful_tensor import StatefulTensor
-from colossalai.legacy.engine import Engine
-from colossalai.legacy.utils.profiler.extention import ProfilerExtension
+from colossalai.accelerator import get_accelerator
 
 
-class DeviceType(Enum):
-    CPU = 0
-    CUDA = 1
+class MemoryMonitor:
+    """Base class for all types of memory monitor.
+    All monitors should have a list called `time_stamps` and a list called `mem_stats`.
+    """
 
+    def __init__(self):
+        self.time_stamps = []
+        self.mem_stats = []
 
-def get_timestamp_us():
-    return int(time.time() * 1e6)
+    def __len__(self):
+        return len(self.mem_stats)
 
+    @abstractmethod
+    def start(self):
+        pass
 
-def generic_instant_event(name, pid, tid, timestamp, args):
-    return {"ph": "i", "s": "t", "name": name, "pid": pid, "tid": tid, "ts": timestamp, "args": args}
+    @abstractmethod
+    def finish(self):
+        pass
 
+    def state_dict(self):
+        return {
+            "time_stamps": self.time_stamps,
+            "mem_stats": self.mem_stats,
+        }
+
+    def save(self, filename):
+        with open(filename, "w") as f:
+            json.dump(self.state_dict(), f)
+
+    def clear(self):
+        self.mem_stats.clear()
+        self.time_stamps.clear()
+
+
+class AsyncMemoryMonitor(MemoryMonitor):
+    """
+    An Async Memory Monitor running during computing. Sampling memory usage of the current GPU
+    at interval of `1/(10**power)` sec.
+
+    The idea comes from Runtime Memory Tracer of PatrickStar
+    `PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management`_
+
+    Usage::
+
+        async_mem_monitor = AsyncMemoryMonitor()
+        input = torch.randn(2, 20).cuda()
+        OP1 = torch.nn.Linear(20, 30).cuda()
+        OP2 = torch.nn.Linear(30, 40).cuda()
+
+        async_mem_monitor.start()
+        output = OP1(input)
+        async_mem_monitor.finish()
+        async_mem_monitor.start()
+        output = OP2(output)
+        async_mem_monitor.finish()
+        async_mem_monitor.save('log.pkl')
+
+    Args:
+        power (int, optional): the power of time interval. Defaults to 10.
+
+    .. _PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management:
+        https://arxiv.org/abs/2108.05818
+    """
 
-class StatefulTensorMemoryEvent:
-    EVENT_NAME = "[statefulTensorMemory]"
+    def __init__(self, power: int = 10):
+        super().__init__()
+        self.keep_measuring = False
 
-    def __init__(self, timestamp: int, device_type: DeviceType, bytes_: int) -> None:
-        self.pid = os.getpid()
-        self.tid = threading.get_ident()
-        self.timestamp = timestamp
-        self.device_type = device_type
-        self.device_id = torch.cuda.current_device() if device_type == DeviceType.CUDA else -1
-        self.bytes = bytes_
+        current_device = get_accelerator().get_current_device()
 
-    def state_dict(self):
-        return generic_instant_event(
-            StatefulTensorMemoryEvent.EVENT_NAME,
-            self.pid,
-            self.tid,
-            self.timestamp,
-            {"Device Type": self.device_type.value, "Device Id": self.device_id, "Bytes": self.bytes},
-        )
-
-
-class StatefulTensorMemoryTracer:
-    def __init__(self) -> None:
-        self.events: List[StatefulTensorMemoryEvent] = []
-        self._tracing = False
-
-    def sample(self):
-        cuda_mem = StatefulTensor.GST_MGR.total_mem["cuda"]
-        cpu_mem = StatefulTensor.GST_MGR.total_mem["cpu"]
-        timestamp = get_timestamp_us()
-        if self._tracing:
-            self.events.append(StatefulTensorMemoryEvent(timestamp, DeviceType.CUDA, cuda_mem))
-            self.events.append(StatefulTensorMemoryEvent(timestamp, DeviceType.CPU, cpu_mem))
-
-    def start_trace(self):
-        self.events.clear()
-        self._tracing = True
+        def _set_cuda_device():
+            torch.cuda.set_device(current_device)
 
-    def stop_trace(self):
-        self._tracing = False
+        self.executor = ThreadPoolExecutor(max_workers=1, initializer=_set_cuda_device)
+        self.monitor_thread = None
+        self.interval = 1 / (10**power)
+
+    def set_interval(self, power: int):
+        self.clear()
+        self.interval = 1 / (10**power)
+
+    def is_measuring(self):
+        return self.keep_measuring
+
+    def start(self):
+        self.keep_measuring = True
+        self.monitor_thread = self.executor.submit(self._measure_usage)
+
+    def finish(self):
+        if self.keep_measuring is False:
+            return 0
+
+        self.keep_measuring = False
+        max_usage = self.monitor_thread.result()
+
+        self.monitor_thread = None
+        self.time_stamps.append(time())
+        self.mem_stats.append(max_usage)
+        return max_usage
+
+    def _measure_usage(self):
+        from colossalai.legacy.utils import colo_device_memory_used
+
+        max_usage = 0
+        while self.keep_measuring:
+            max_usage = max(
+                max_usage,
+                colo_device_memory_used(get_accelerator().get_current_device()),
+            )
+            sleep(self.interval)
+        return max_usage
 
-    def state_dict(self):
-        return [event.state_dict() for event in self.events]
 
+class SyncCudaMemoryMonitor(MemoryMonitor):
+    """
+    A synchronized cuda memory monitor.
+    It only record the maximum allocated cuda memory from start point to finish point.
+    """
 
-class StatefulTensorMemoryTracerHook(BaseOpHook):
-    def __init__(self, tracer: StatefulTensorMemoryTracer):
+    def __init__(self, power: int = 10):
         super().__init__()
-        self.tracer = tracer
-        self._enable = False
 
-    def pre_fwd_exec(self, module: torch.nn.Module, *args):
-        if self._enable:
-            self.tracer.sample()
-
-    def post_fwd_exec(self, module: torch.nn.Module, *args):
-        if self._enable:
-            self.tracer.sample()
-
-    def pre_bwd_exec(self, module: torch.nn.Module, input_, output):
-        if self._enable:
-            self.tracer.sample()
-
-    def post_bwd_exec(self, module: torch.nn.Module, input_):
-        if self._enable:
-            self.tracer.sample()
-
-    def post_iter(self):
-        if self._enable:
-            self.tracer.sample()
-
-    def enable(self):
-        self._enable = True
-
-    def disable(self):
-        self._enable = False
-
-
-class StatefulTensorMemoryProfilerExtention(ProfilerExtension):
-    def __init__(self, engine: Engine) -> None:
-        self.engine = engine
-        self.tracer = StatefulTensorMemoryTracer()
-        self.hook = StatefulTensorMemoryTracerHook(self.tracer)
-        self.hook_registered = False
-
-    def prepare_trace(self):
-        self.hook.enable()
-        if not self.hook_registered:
-            self.engine.add_hook(self.hook)
-            self.hook_registered = True
-
-    def start_trace(self):
-        self.prepare_trace()
-        self.tracer.start_trace()
-
-    def stop_trace(self):
-        self.tracer.stop_trace()
-        self.hook.disable()
-        if self.hook_registered:
-            self.engine.remove_hook(self.hook)
-            # remove_hook is not implemented now
-            # FIXME(ver217): uncomment below line when remove_hook is implemented
-            # self.hook_registered = False
-
-    def extend_chrome_trace(self, trace: dict) -> dict:
-        trace["traceEvents"].extend(self.tracer.state_dict())
-        return trace
+    def start(self):
+        torch.cuda.synchronize()
+        torch.cuda.reset_peak_memory_stats()
+
+    def finish(self) -> int:
+        """
+        return max gpu memory used since latest `start()`.
+
+        Returns:
+            int: max GPU memory
+        """
+        torch.cuda.synchronize()
+        self.time_stamps.append(time())
+        max_usage = torch.cuda.max_memory_allocated()
+        self.mem_stats.append(max_usage)
+        return max_usage
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/colo_init_context.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/colo_init_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/gemini_context.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/gemini_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/ophooks/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/stateful_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/tensor_placement_policy.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_placement_policy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/gemini/tensor_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/init_ctx/init_context.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/init_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/base_shard_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/base_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/commons.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/commons.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/reduce_scatter.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/reduce_scatter.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/sharded_model_v2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/sharded_model_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_model/zero_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/zero_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_param/sharded_param.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_param.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/legacy/zero/sharded_param/sharded_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/logging/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/logging/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/logging/logger.py` & `colossalai-nightly-2024.5.4/colossalai/logging/logger.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/moe/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/moe/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/checkpoint.py` & `colossalai-nightly-2024.5.4/colossalai/moe/checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/experts.py` & `colossalai-nightly-2024.5.4/colossalai/moe/experts.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/layers.py` & `colossalai-nightly-2024.5.4/colossalai/moe/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/load_balance.py` & `colossalai-nightly-2024.5.4/colossalai/moe/load_balance.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/loss.py` & `colossalai-nightly-2024.5.4/colossalai/moe/loss.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/manager.py` & `colossalai-nightly-2024.5.4/colossalai/moe/manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/routers.py` & `colossalai-nightly-2024.5.4/colossalai/moe/routers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/moe/utils.py` & `colossalai-nightly-2024.5.4/colossalai/moe/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/init.py` & `colossalai-nightly-2024.5.4/colossalai/nn/init.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/layer/layernorm.py` & `colossalai-nightly-2024.5.4/colossalai/nn/layer/layernorm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/layer/scaled_softmax.py` & `colossalai-nightly-2024.5.4/colossalai/nn/layer/scaled_softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/cosine.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/cosine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/delayed.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/delayed.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/linear.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/multistep.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/multistep.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/onecycle.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/onecycle.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/poly.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/poly.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/lr_scheduler/torch.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/torch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/optimizer/cpu_adam.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/cpu_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/optimizer/fused_adam.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/optimizer/fused_lamb.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_lamb.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/optimizer/fused_sgd.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_sgd.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/optimizer/hybrid_adam.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/hybrid_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/optimizer/lamb.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lamb.py`

 * *Files 12% similar despite different names*

```diff
@@ -22,26 +22,24 @@
         adam (bool, optional): always use trust ratio = 1, which turns this into
             Adam. Useful for comparison purposes.
 
     .. _Large Batch Optimization for Deep Learning\: Training BERT in 76 minutes:
         https://arxiv.org/abs/1904.00962
     """
 
-    def __init__(
-        self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0, adam=False, bias_correction=False
-    ):
+    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0, adam=False):
         if not 0.0 <= lr:
             raise ValueError("Invalid learning rate: {}".format(lr))
         if not 0.0 <= eps:
             raise ValueError("Invalid epsilon value: {}".format(eps))
         if not 0.0 <= betas[0] < 1.0:
             raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
         if not 0.0 <= betas[1] < 1.0:
             raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
-        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, bias_correction=bias_correction)
+        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
         self.adam = adam
         super(Lamb, self).__init__(params, defaults)
 
     def step(self, closure=None):
         """Performs a single optimization step.
 
         Arguments:
@@ -77,35 +75,34 @@
 
                 # Decay the first and second moment running average coefficient
                 # m_t
                 exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                 # v_t
                 exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
 
-                # NOTE: Paper v3 does not use debiasing.
-                scaled_lr = group["lr"]
-                if group["bias_correction"]:
-                    bias_correction1 = 1 - beta1 ** state["step"]
-                    bias_correction2 = 1 - beta2 ** state["step"]
-                    # Apply debiasing to lr to avoid broadcast
-                    scaled_lr *= (bias_correction2**0.5) / bias_correction1
-                    # exp_avg.div_(bias_correction1)
-                    # exp_avg_sq.div_(bias_correction2)
+                # Paper v3 does not use debiasing.
+                # bias_correction1 = 1 - beta1 ** state['step']
+                # bias_correction2 = 1 - beta2 ** state['step']
+                # Apply bias to lr to avoid broadcast.
+                # * math.sqrt(bias_correction2) / bias_correction1
+                step_size = group["lr"]
 
                 weight_norm = p.data.pow(2).sum().sqrt()
 
                 adam_step = exp_avg / exp_avg_sq.sqrt().add(group["eps"])
                 if group["weight_decay"] != 0:
                     adam_step.add_(p.data, alpha=group["weight_decay"])
 
                 adam_norm = adam_step.pow(2).sum().sqrt()
                 if weight_norm == 0 or adam_norm == 0:
                     trust_ratio = 1
                 else:
                     trust_ratio = weight_norm / adam_norm
-
+                state["weight_norm"] = weight_norm
+                state["adam_norm"] = adam_norm
+                state["trust_ratio"] = trust_ratio
                 if self.adam:
                     trust_ratio = 1
 
-                p.data.add_(adam_step, alpha=-scaled_lr * trust_ratio)
+                p.data.add_(adam_step, alpha=-step_size * trust_ratio)
 
         return loss
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/optimizer/lars.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lars.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/nn/optimizer/nvme_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/nvme_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/pipeline/p2p.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/p2p.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/base.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/generate.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/generate.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/interleaved_pp.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/interleaved_pp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/pipeline/schedule/one_f_one_b.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/one_f_one_b.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/pipeline/stage_manager.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/stage_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/quantization/bnb.py` & `colossalai-nightly-2024.5.4/colossalai/quantization/bnb.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/quantization/bnb_config.py` & `colossalai-nightly-2024.5.4/colossalai/quantization/bnb_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/attn.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/attn.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/dropout.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/dropout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/embedding.py`

 * *Files 0% similar despite different names*

```diff
@@ -245,14 +245,15 @@
             i.e. it remains as a fixed “pad”, defaults to None.
         dtype (:class:`torch.dtype`, optional): The dtype of parameters, defaults to None.
         weight_initializer (:class:`typing.Callable`, optional):
             he initializer of weight, defaults to normal initializer.
 
     The ``args`` and ``kwargs`` used in :class:``torch.nn.functional.embedding`` should contain:
     ::
+
         max_norm (float, optional): If given, each embedding vector with norm larger than max_norm is
                     renormalized to have norm max_norm. Note: this will modify weight in-place.
         norm_type (float, optional): The p of the p-norm to compute for the max_norm option. Default 2.
         scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse
                     of frequency of the words in the mini-batch. Default False.
         sparse (bool, optional): If True, gradient w.r.t. weight will be a sparse tensor. Default False.
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/linear.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/loss.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/loss.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,28 +18,27 @@
     def forward(
         ctx,
         vocab_logits: torch.Tensor,
         target: torch.Tensor,
         ignore_index: int,
         process_group: ProcessGroup,
         vocab_size: int,
-        dtype=torch.float32,
     ):
         r"""
         Calculate the cross entropy loss before gather, the origin loss function is as follows:
         loss = -log(exp(x[class])/sum(exp(x[i]))
         and can be rewrite as:
         loss = log(sum(exp(x[i])) - x[class]
 
         To avoid the `nan` of log(sum(exp(x[i]))), we minus the max of x[i]
 
         Args:
             vocab_logits (:class:`torch.Tensor`): The logits of the vocabulary, shape is
               [batch_size, seq_len, vocab_size]
-            target (:class:`torch.Tensor`): The labels of the vocabulary, shape is
+            labels (:class:`torch.Tensor`): The labels of the vocabulary, shape is
               [batch_size, seq_len]
 
         Returns:
             :class:`torch.Tensor`: The cross entropy loss
         """
         # get the max
         logits_max = torch.max(vocab_logits, dim=-1)[0]
@@ -83,52 +82,50 @@
         pred_logits = pred_logits_1d.view_as(target)
         pred_logits[mask] = 0.0
 
         # allreduce the get all x(i,y)
         dist.all_reduce(pred_logits, op=dist.ReduceOp.SUM, group=process_group)
         exp_logits = vocab_logits
         torch.exp(vocab_logits, out=exp_logits)
-        sum_exp_logits = torch.sum(exp_logits, dim=-1, dtype=torch.float32)
+        sum_exp_logits = torch.sum(exp_logits, dim=-1)
         dist.all_reduce(sum_exp_logits, op=dist.ReduceOp.SUM, group=process_group)
 
         # calculate the loss
         # loss = log(sum(exp(x[i]))) - x[class]
         loss = torch.where(target == ignore_index, 0.0, torch.log(sum_exp_logits) - pred_logits)
         num_non_zero = torch.sum(loss != 0.0)
         ctx.inv_num_non_zero = 1.0 / num_non_zero
         loss = torch.sum(loss).div_(num_non_zero)
 
         # calculate the softmax
-        exp_logits = exp_logits.div(sum_exp_logits.unsqueeze(dim=-1)).to(dtype)
+        exp_logits.div_(sum_exp_logits.unsqueeze(dim=-1))
         exp_logits[target == ignore_index] = 0.0
         ctx.save_for_backward(exp_logits, mask, masked_target_1d)
-        ctx.dtype = dtype
 
         return loss
 
     @staticmethod
     def backward(ctx, grad_output):
         # retrieve the saved tensors
         grad_output = grad_output * ctx.inv_num_non_zero
         exp_logits, mask, masked_target_1d = ctx.saved_tensors
 
         # use exp logits as the input grad
         grad_logits = exp_logits
         partion_vocab_size = grad_logits.shape[-1]
         grad_logits_2d = grad_logits.view(-1, partion_vocab_size)
 
-        update = 1.0 - mask.view(-1).float().to(ctx.dtype)
+        update = 1.0 - mask.view(-1).float()
         grad_logits_2d[torch.arange(0, grad_logits_2d.shape[0]), masked_target_1d] -= update
 
         grad_logits.mul_(grad_output.unsqueeze(dim=-1))
-        return grad_logits, None, None, None, None, None
+        return grad_logits, None, None, None, None
 
 
 def cross_entropy_1d(
     vocab_logits: torch.Tensor,
     labels: torch.Tensor,
     ignore_index: int = -100,
     process_group: ProcessGroup = None,
     vocab_size: int = None,
-    dtype: torch.dtype = None,
 ) -> torch.Tensor:
-    return DistCrossEntropy.apply(vocab_logits, labels, ignore_index, process_group, vocab_size, dtype)
+    return DistCrossEntropy.apply(vocab_logits, labels, ignore_index, process_group, vocab_size)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/parallel_module.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/parallel_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/qkv_fused_linear.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/qkv_fused_linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/layer/utils.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/bert.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/blip2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bloom.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,15 +6,14 @@
 from torch.distributed import ProcessGroup
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
 from torch.nn import functional as F
 from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
 from transformers.modeling_outputs import (
     BaseModelOutputWithPastAndCrossAttentions,
     CausalLMOutputWithCrossAttentions,
-    CausalLMOutputWithPast,
     QuestionAnsweringModelOutput,
     SequenceClassifierOutputWithPast,
     TokenClassifierOutput,
 )
 from transformers.models.bloom.modeling_bloom import (
     BloomForCausalLM,
     BloomForQuestionAnswering,
@@ -24,16 +23,14 @@
 )
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer.layer._operation import gather_forward_split_backward, split_forward_gather_backward
 from colossalai.shardformer.shard import ShardConfig
 
-from ..layer import cross_entropy_1d
-
 logger = logging.get_logger(__name__)
 
 
 def build_bloom_alibi_tensor_fn(process_group: ProcessGroup) -> torch.Tensor:
     def build_bloom_alibi_tensor(
         self, attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype
     ) -> torch.Tensor:
@@ -353,40 +350,29 @@
             hidden_states=hidden_states,
             stage_index=stage_index,
             shard_config=shard_config,
         )
         past_key_values = None
         if stage_manager.is_last_stage():
             hidden_states = transformer_outputs[0]
-            lm_logits = self.lm_head(hidden_states).contiguous()
+            lm_logits = self.lm_head(hidden_states)
 
             loss = None
             if labels is not None:
                 # move labels to correct device to enable model parallelism
                 labels = labels.to(lm_logits.device)
                 # Shift so that tokens < n predict n
                 shift_logits = lm_logits[..., :-1, :].contiguous()
                 shift_labels = labels[..., 1:].contiguous()
                 batch_size, seq_length, vocab_size = shift_logits.shape
                 # Flatten the tokens
-                if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
-                    new_vocab_size = lm_logits.shape[-1]
-                    shift_logits = shift_logits.view(-1, new_vocab_size)
-                    shift_labels = shift_labels.view(-1)
-                    loss = cross_entropy_1d(
-                        shift_logits,
-                        shift_labels,
-                        process_group=shard_config.tensor_parallel_process_group,
-                        vocab_size=self.lm_head.out_features,
-                        dtype=self.transformer.dtype,
-                    )
-                else:
-                    loss_fct = CrossEntropyLoss()
-                    shift_logits = shift_logits.view(-1, self.config.vocab_size)
-                    loss = loss_fct(shift_logits, shift_labels.view(-1))
+                loss_fct = CrossEntropyLoss()
+                loss = loss_fct(
+                    shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)
+                )
 
             if not return_dict:
                 output = (lm_logits,) + transformer_outputs[1:]
                 return ((loss,) + output) if loss is not None else output
 
             return CausalLMOutputWithCrossAttentions(
                 loss=loss,
@@ -1075,83 +1061,7 @@
             last_hidden_state=hidden_states,
             past_key_values=presents,
             hidden_states=all_hidden_states,
             attentions=all_self_attentions,
         )
 
     return forward
-
-
-def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
-    from transformers import BloomForCausalLM
-
-    def forward(
-        self: BloomForCausalLM,
-        input_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-        labels: Optional[torch.Tensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-    ) -> Union[Tuple, CausalLMOutputWithPast]:
-        r"""
-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
-            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
-            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
-        """
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        transformer_outputs = self.transformer(
-            input_ids=input_ids,
-            past_key_values=past_key_values,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-        )
-        past_key_values = None
-        hidden_states = transformer_outputs[0]
-        lm_logits = self.lm_head(hidden_states)
-
-        loss = None
-        if labels is not None:
-            # move labels to correct device to enable model parallelism
-            labels = labels.to(lm_logits.device)
-            # Shift so that tokens < n predict n
-            shift_logits = lm_logits[..., :-1, :].contiguous()
-            shift_labels = labels[..., 1:].contiguous()
-            # Flatten the tokens
-            new_vocab_size = lm_logits.shape[-1]
-            shift_logits = shift_logits.view(-1, new_vocab_size)
-            shift_labels = shift_labels.view(-1)
-            loss = cross_entropy_1d(
-                shift_logits,
-                shift_labels,
-                process_group=shard_config.tensor_parallel_process_group,
-                vocab_size=self.lm_head.out_features,
-                dtype=self.transformer.dtype,
-            )
-        if not return_dict:
-            output = (lm_logits,) + transformer_outputs[1:]
-            return ((loss,) + output) if loss is not None else output
-
-        return CausalLMOutputWithPast(
-            loss=loss,
-            logits=lm_logits,
-            past_key_values=transformer_outputs.past_key_values,
-            hidden_states=transformer_outputs.hidden_states,
-            attentions=transformer_outputs.attentions,
-        )
-
-    return forward
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/falcon.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/falcon.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,15 +10,14 @@
     AttentionMaskConverter,
     _prepare_4d_causal_attention_mask,
     _prepare_4d_causal_attention_mask_for_sdpa,
 )
 from transformers.modeling_outputs import (
     BaseModelOutputWithPastAndCrossAttentions,
     CausalLMOutputWithCrossAttentions,
-    CausalLMOutputWithPast,
     QuestionAnsweringModelOutput,
     SequenceClassifierOutputWithPast,
     TokenClassifierOutput,
 )
 from transformers.models.falcon.modeling_falcon import (
     FalconForCausalLM,
     FalconForQuestionAnswering,
@@ -28,16 +27,14 @@
     build_alibi_tensor,
 )
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer.shard import ShardConfig
 
-from ..layer import cross_entropy_1d
-
 
 def build_falcon_alibi_tensor_fn(process_group: ProcessGroup) -> torch.Tensor:
     def build_falcon_alibi_tensor(
         self, attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype
     ) -> torch.Tensor:
         """
         Link to paper: https://arxiv.org/abs/2108.12409 Alibi tensor is not causal as the original paper mentions, it
@@ -436,36 +433,22 @@
         if stage_manager.is_last_stage():
             hidden_states = transformer_outputs[0]
             lm_logits = self.lm_head(hidden_states)
 
             loss = None
             if labels is not None:
                 # Shift so that tokens < n predict n
-                labels = labels.to(lm_logits.device)
                 shift_logits = lm_logits[..., :-1, :].contiguous()
                 shift_labels = labels[..., 1:].contiguous()
                 batch_size, seq_length, vocab_size = shift_logits.shape
                 # Flatten the tokens
                 loss_fct = CrossEntropyLoss()
-                if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
-                    new_vocab_size = shift_logits.shape[-1]
-                    shift_logits = shift_logits.view(-1, new_vocab_size)
-                    shift_labels = shift_labels.view(-1)
-                    loss = cross_entropy_1d(
-                        shift_logits,
-                        shift_labels,
-                        process_group=shard_config.tensor_parallel_process_group,
-                        vocab_size=self.lm_head.out_features,
-                        dtype=self.transformer.dtype,
-                    )
-                else:
-                    loss = loss_fct(
-                        shift_logits.view(batch_size * seq_length, vocab_size),
-                        shift_labels.view(batch_size * seq_length),
-                    )
+                loss = loss_fct(
+                    shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)
+                )
 
             if not return_dict:
                 output = (lm_logits,) + transformer_outputs[1:]
                 return ((loss,) + output) if loss is not None else output
 
             return CausalLMOutputWithCrossAttentions(
                 loss=loss,
@@ -760,83 +743,7 @@
                 end_logits=end_logits,
                 hidden_states=outputs.hidden_states,
                 attentions=outputs.attentions,
             )
         else:
             hidden_states = outputs.get("hidden_states")
             return {"hidden_states": hidden_states}
-
-
-def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
-    from transformers import FalconForCausalLM
-
-    def forward(
-        self: FalconForCausalLM,
-        input_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-        labels: Optional[torch.Tensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-    ) -> Union[Tuple, CausalLMOutputWithPast]:
-        r"""
-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
-            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
-            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
-        """
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        transformer_outputs = self.transformer(
-            input_ids,
-            past_key_values=past_key_values,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-        )
-        past_key_values = None
-        hidden_states = transformer_outputs[0]
-        lm_logits = self.lm_head(hidden_states)
-        loss = None
-        if labels is not None:
-            # Shift so that tokens < n predict n
-            labels = labels.to(lm_logits.device)
-            shift_logits = lm_logits[..., :-1, :].contiguous()
-            shift_labels = labels[..., 1:].contiguous()
-            batch_size, seq_length, vocab_size = shift_logits.shape
-            # Flatten the tokens
-            new_vocab_size = shift_logits.shape[-1]
-            shift_logits = shift_logits.view(-1, new_vocab_size)
-            shift_labels = shift_labels.view(-1)
-            loss = cross_entropy_1d(
-                shift_logits,
-                shift_labels,
-                process_group=shard_config.tensor_parallel_process_group,
-                vocab_size=self.lm_head.out_features,
-                dtype=self.transformer.dtype,
-            )
-
-        if not return_dict:
-            output = (lm_logits,) + transformer_outputs[1:]
-            return ((loss,) + output) if loss is not None else output
-
-        return CausalLMOutputWithPast(
-            loss=loss,
-            logits=lm_logits,
-            past_key_values=transformer_outputs.past_key_values,
-            hidden_states=transformer_outputs.hidden_states,
-            attentions=transformer_outputs.attentions,
-        )
-
-    return forward
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/gpt2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gpt2.py`

 * *Files 0% similar despite different names*

```diff
@@ -385,15 +385,14 @@
             shift_labels = shift_labels.view(-1)
             if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
                 loss = cross_entropy_1d(
                     shift_logits,
                     shift_labels,
                     process_group=shard_config.tensor_parallel_process_group,
                     vocab_size=self.lm_head.out_features,
-                    dtype=self.transformer.dtype,
                 )
             else:
                 loss = loss_fct(shift_logits, shift_labels)
 
         if not return_dict:
             output = (lm_logits,) + outputs[1:]
             return ((loss,) + output) if loss is not None else output
@@ -1291,15 +1290,14 @@
             shift_logits = shift_logits.view(-1, shift_logits.size(-1))
             shift_labels = shift_labels.view(-1)
             loss = cross_entropy_1d(
                 shift_logits,
                 shift_labels,
                 process_group=shard_config.tensor_parallel_process_group,
                 vocab_size=self.lm_head.out_features,
-                dtype=self.transformer.dtype,
             )
 
         if not return_dict:
             output = (lm_logits,) + transformer_outputs[1:]
             return ((loss,) + output) if loss is not None else output
 
         return CausalLMOutputWithCrossAttentions(
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/gptj.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gptj.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/jit.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/jit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/llama.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/llama.py`

 * *Files 0% similar despite different names*

```diff
@@ -328,15 +328,14 @@
                     new_vocab_size = logits.shape[-1]
                     shift_logits = shift_logits.view(-1, new_vocab_size)
                     loss = cross_entropy_1d(
                         shift_logits,
                         shift_labels,
                         process_group=shard_config.tensor_parallel_process_group,
                         vocab_size=self.lm_head.out_features,
-                        dtype=self.model.dtype,
                     )
                 else:
                     shift_logits = shift_logits.view(-1, self.config.vocab_size)
                     loss = loss_fct(shift_logits, shift_labels)
 
             if not return_dict:
                 output = (logits,) + outputs[1:]
@@ -765,15 +764,14 @@
             new_vocab_size = logits.shape[-1]
             shift_logits = shift_logits.view(-1, new_vocab_size)
             loss = cross_entropy_1d(
                 shift_logits,
                 shift_labels,
                 process_group=shard_config.tensor_parallel_process_group,
                 vocab_size=self.lm_head.out_features,
-                dtype=self.model.dtype,
             )
 
         if not return_dict:
             output = (logits,) + outputs[1:]
             return (loss,) + output if loss is not None else output
 
         return CausalLMOutputWithPast(
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/mistral.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/opt.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,296 +1,347 @@
-import warnings
+import random
 from typing import List, Optional, Tuple, Union
 
 import torch
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
-from transformers.cache_utils import Cache, DynamicCache
 from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
 from transformers.modeling_outputs import (
     BaseModelOutputWithPast,
     CausalLMOutputWithPast,
+    QuestionAnsweringModelOutput,
     SequenceClassifierOutputWithPast,
 )
-from transformers.models.mistral.modeling_mistral import MistralForCausalLM, MistralModel
+from transformers.models.opt.modeling_opt import (
+    OPTForCausalLM,
+    OPTForQuestionAnswering,
+    OPTForSequenceClassification,
+    OPTModel,
+)
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer.layer import ColoAttention
 from colossalai.shardformer.shard import ShardConfig
 
-from ..layer import ColoAttention, cross_entropy_1d
-
 logger = logging.get_logger(__name__)
 
 
-class MistralForwards:
+def _get_attention_mask(
+    self: OPTModel,
+    shard_config: ShardConfig,
+    hidden_states: torch.Tensor,
+    past_key_values_length: int,
+    attention_mask: Optional[torch.FloatTensor],
+):
+    batch_size, seq_length = hidden_states.shape[:2]
+    mask_seq_length = past_key_values_length + seq_length
+    if shard_config.enable_flash_attention:
+        attention_mask = ColoAttention.prepare_attn_kwargs(
+            (batch_size, 1, seq_length, mask_seq_length),
+            hidden_states.dtype,
+            hidden_states.device,
+            attention_mask,
+            is_causal=True,
+        )
+    else:
+        attention_mask = _prepare_4d_causal_attention_mask(
+            attention_mask,
+            (batch_size, seq_length),
+            hidden_states,
+            past_key_values_length,
+        )
+    return attention_mask
+
+
+class OPTPipelineForwards:
+    """
+    This class serves as a micro library for forward function substitution of OPT models
+    under pipeline setting.
+    """
+
     @staticmethod
-    def mistral_model_forward(
-        self: MistralModel,
+    def opt_model_forward(
+        self: OPTModel,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
-        shard_config: ShardConfig = None,
+        shard_config: Optional[ShardConfig] = None,
     ) -> Union[Tuple, BaseModelOutputWithPast]:
-        if use_cache:
-            logger.warning_once("use_cache=True is not supported for Mistral models at the moment.")
-            use_cache = False
+        """
+        This forward method is modified based on transformers.models.opt.modeling_opt.OPTModel.forward
+        """
+
+        from transformers.modeling_outputs import BaseModelOutputWithPast
+        from transformers.utils import logging
+
+        logger = logging.get_logger(__name__)
+
         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
         output_hidden_states = (
             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
-
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        # retrieve input_ids and inputs_embeds
+        decoder = self.decoder
         if stage_manager.is_first_stage():
+            # retrieve input_ids and inputs_embeds
             if input_ids is not None and inputs_embeds is not None:
                 raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
             elif input_ids is not None:
-                batch_size, seq_length = input_ids.shape
+                input_shape = input_ids.size()
+                input_ids = input_ids.view(-1, input_shape[-1])
             elif inputs_embeds is not None:
-                batch_size, seq_length, _ = inputs_embeds.shape
+                input_shape = inputs_embeds.size()[:-1]
             else:
-                raise ValueError("You have to specify either input_ids or inputs_embeds")
-            inputs_embeds = self.embed_tokens(input_ids)
-            hidden_states = inputs_embeds
-        else:
-            input_shape = hidden_states.shape[:-1]
+                raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
+
             batch_size, seq_length = input_shape
-            device = hidden_states.device
 
-        past_key_values_length = 0
+            if inputs_embeds is None:
+                inputs_embeds = decoder.embed_tokens(input_ids)
 
-        if position_ids is None:
+            if decoder.project_in is not None:
+                inputs_embeds = decoder.project_in(inputs_embeds)
             device = input_ids.device if input_ids is not None else inputs_embeds.device
-            position_ids = torch.arange(
-                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
-            )
-            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
+            inputs_embeds.dtype
+            hidden_states = inputs_embeds
         else:
-            position_ids = position_ids.view(-1, seq_length).long()
+            if hidden_states is None:
+                raise ValueError("hidden_states shouldn't be None for intermediate stages.")
+            input_shape = hidden_states.size()[:-1]
+            batch_size, seq_length = input_shape[0], input_shape[1]
+            device = hidden_states.device
+            hidden_states.dtype
 
-        if attention_mask is not None and self._use_flash_attention_2 and use_cache:
-            is_padding_right = attention_mask[:, -1].sum().item() != batch_size
-            if is_padding_right:
+        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
+        # required mask seq length can be calculated via length of past
+        mask_seq_length = past_key_values_length + seq_length
+        # embed positions
+        if self.decoder._use_flash_attention_2:
+            # 2d mask is passed through the layers
+            causal_attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+            attention_mask = (
+                torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
+                if attention_mask is None
+                else attention_mask
+            )
+        else:
+            # 4d mask is passed through the layers
+            if attention_mask is None:
+                attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
+            elif attention_mask.shape[1] != mask_seq_length:
                 raise ValueError(
-                    "You are attempting to perform batched generation with padding_side='right'"
-                    " this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to "
-                    " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
+                    f"The provided attention mask has length {attention_mask.shape[1]}, but its length should be "
+                    f"{mask_seq_length} (sum of the lengths of current and past inputs)"
                 )
+            causal_attention_mask = _prepare_4d_causal_attention_mask(
+                attention_mask, input_shape, hidden_states, past_key_values_length
+            )
 
-        if shard_config.enable_flash_attention:
-            # in this case, attention_mask is a dict rather than a tensor
-            mask_shape = (batch_size, 1, seq_length, seq_length)
-            attention_mask = ColoAttention.prepare_attn_kwargs(
-                mask_shape,
-                hidden_states.dtype,
-                hidden_states.device,
-                q_padding_mask=attention_mask,
-                is_causal=True,
+        if stage_manager.is_first_stage():
+            causal_attention_mask = _get_attention_mask(
+                self,
+                shard_config,
+                inputs_embeds,
+                past_key_values_length,
+                attention_mask,
             )
+            pos_embeds = decoder.embed_positions(attention_mask, past_key_values_length)
+            hidden_states = inputs_embeds + pos_embeds
         else:
-            if self._use_flash_attention_2:
-                # 2d mask is passed through the layers
-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
-            else:
-                # 4d mask is passed through the layers
-                attention_mask = _prepare_4d_causal_attention_mask(
-                    attention_mask,
-                    (batch_size, seq_length),
-                    hidden_states,
-                    past_key_values_length,
-                    sliding_window=self.config.sliding_window,
-                )
+            causal_attention_mask = _get_attention_mask(
+                self,
+                shard_config,
+                hidden_states,
+                past_key_values_length,
+                attention_mask,
+            )
 
-        if self.gradient_checkpointing and self.training:
+        if decoder.gradient_checkpointing and decoder.training:
             if use_cache:
                 logger.warning_once(
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
                 use_cache = False
 
+        # TODO(baizhou): left the recording kv-value tensors as () or None type, this feature may be added in the future.
+        if past_key_values:
+            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
+            past_key_values = None
+        if output_attentions:
+            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
+            output_attentions = False
+        if output_hidden_states:
+            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
+            output_hidden_states = False
+        if use_cache:
+            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
+            use_cache = False
+
         # decoder layers
         all_hidden_states = () if output_hidden_states else None
         all_self_attns = () if output_attentions else None
+        next_decoder_cache = () if use_cache else None
+
+        # check if head_mask has a correct number of layers specified if desired
+        for attn_mask, mask_name in zip([head_mask], ["head_mask"]):
+            if attn_mask is not None:
+                if attn_mask.size()[0] != (len(decoder.layers)):
+                    raise ValueError(
+                        f"The `{mask_name}` should be specified for {len(decoder.layers)} layers, but it is for"
+                        f" {head_mask.size()[0]}."
+                    )
 
         start_idx, end_idx = stage_index[0], stage_index[1]
-        num_ckpt_layers = 0
-        if self.gradient_checkpointing and self.training:
-            num_ckpt_layers = end_idx - start_idx
-            # TODO: We can replace `gradient_checkpointing_enable` fn and initialize a gradient_checkpointing (List[bool]) for each layer
-            if shard_config.gradient_checkpoint_config is not None:
-                num_ckpt_layers = shard_config.gradient_checkpoint_config.get_num_ckpt_layers(
-                    stage=stage_manager.stage,
-                    num_stages=stage_manager.num_stages,
-                    num_layers=end_idx - start_idx,
-                    model_chunk_id=(stage_manager.model_chunk_id if stage_manager.is_interleave else 0),
-                    num_model_chunks=stage_manager.num_model_chunks,
-                )
-            assert num_ckpt_layers <= end_idx - start_idx
 
-        for idx, decoder_layer in enumerate(self.layers[start_idx:end_idx], start=start_idx):
+        torch.cuda.set_device(device)
+
+        for idx in range(start_idx, end_idx):
+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
+            decoder_layer = decoder.layers[idx]
+
             if output_hidden_states:
                 all_hidden_states += (hidden_states,)
 
-            if idx - start_idx < num_ckpt_layers:
+            dropout_probability = random.uniform(0, 1)
+            if decoder.training and (dropout_probability < decoder.layerdrop):
+                continue
+
+            past_key_value = past_key_values[idx] if past_key_values is not None else None
+
+            if decoder.gradient_checkpointing and decoder.training:
                 layer_outputs = self._gradient_checkpointing_func(
                     decoder_layer.__call__,
                     hidden_states,
-                    attention_mask,
-                    position_ids,
-                    past_key_values,
+                    causal_attention_mask,
+                    head_mask[idx] if head_mask is not None else None,
+                    None,
                     output_attentions,
                     use_cache,
                 )
             else:
                 layer_outputs = decoder_layer(
                     hidden_states,
-                    attention_mask=attention_mask,
-                    position_ids=position_ids,
-                    past_key_value=past_key_values,
+                    attention_mask=causal_attention_mask,
+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                    past_key_value=past_key_value,
                     output_attentions=output_attentions,
                     use_cache=use_cache,
                 )
 
             hidden_states = layer_outputs[0]
 
             if use_cache:
-                layer_outputs[2 if output_attentions else 1]
+                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
 
             if output_attentions:
                 all_self_attns += (layer_outputs[1],)
 
         if stage_manager.is_last_stage():
-            hidden_states = self.norm(hidden_states)
+            if decoder.final_layer_norm is not None:
+                hidden_states = decoder.final_layer_norm(hidden_states)
+            if decoder.project_out is not None:
+                hidden_states = decoder.project_out(hidden_states)
 
         # add hidden states from the last decoder layer
         if output_hidden_states:
             all_hidden_states += (hidden_states,)
 
-        next_cache = None
+        next_cache = next_decoder_cache if use_cache else None
+
         if stage_manager.is_last_stage():
             if not return_dict:
-                return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+                return tuple(
+                    v
+                    for v in [
+                        hidden_states,
+                        next_cache,
+                        all_hidden_states,
+                        all_self_attns,
+                    ]
+                    if v is not None
+                )
+
             return BaseModelOutputWithPast(
                 last_hidden_state=hidden_states,
                 past_key_values=next_cache,
                 hidden_states=all_hidden_states,
                 attentions=all_self_attns,
             )
         else:
             return {"hidden_states": hidden_states}
 
     @staticmethod
-    def mistral_for_causal_lm_forward(
-        self: MistralForCausalLM,
+    def opt_for_causal_lm_forward(
+        self: OPTForCausalLM,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         labels: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
-        shard_config: ShardConfig = None,
+        shard_config: Optional[ShardConfig] = None,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
         r"""
-        Args:
-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
-
-        Returns:
-
-        Example:
-
-        ```python
-        >>> from transformers import AutoTokenizer, MistralForCausalLM
-
-        >>> model = MistralForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
-        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)
-
-        >>> prompt = "Hey, are you conscious? Can you talk to me?"
-        >>> inputs = tokenizer(prompt, return_tensors="pt")
-
-        >>> # Generate
-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
-        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
-        ```"""
+        This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForCausalLM.forward.
+        Please refer to original code of transformers for more details.
+        """
 
         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
         output_hidden_states = (
             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        outputs = MistralForwards.mistral_model_forward(
+        outputs = OPTPipelineForwards.opt_model_forward(
             self.model,
             input_ids=input_ids,
             attention_mask=attention_mask,
-            position_ids=position_ids,
+            head_mask=head_mask,
             past_key_values=past_key_values,
             inputs_embeds=inputs_embeds,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
             shard_config=shard_config,
         )
-
-        past_key_values = None
-
         if stage_manager.is_last_stage():
-            hidden_states = outputs[0]
-            logits = self.lm_head(hidden_states)
-            logits = logits.float()
-
+            logits = self.lm_head(outputs[0]).contiguous()
             loss = None
             if labels is not None:
+                # move labels to correct device to enable model parallelism
+                labels = labels.to(logits.device)
                 # Shift so that tokens < n predict n
                 shift_logits = logits[..., :-1, :].contiguous()
                 shift_labels = labels[..., 1:].contiguous()
                 # Flatten the tokens
                 loss_fct = CrossEntropyLoss()
-                shift_labels = shift_labels.view(-1)
-                # Enable model parallelism
-                shift_labels = shift_labels.to(shift_logits.device)
-                if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
-                    new_vocab_size = logits.shape[-1]
-                    shift_logits = shift_logits.view(-1, new_vocab_size)
-                    loss = cross_entropy_1d(
-                        shift_logits,
-                        shift_labels,
-                        process_group=shard_config.tensor_parallel_process_group,
-                        vocab_size=self.lm_head.out_features,
-                        dtype=self.model.dtype,
-                    )
-                else:
-                    shift_logits = shift_logits.view(-1, self.config.vocab_size)
-                    loss = loss_fct(shift_logits, shift_labels)
-
+                loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
             if not return_dict:
                 output = (logits,) + outputs[1:]
                 return (loss,) + output if loss is not None else output
 
             return CausalLMOutputWithPast(
                 loss=loss,
                 logits=logits,
@@ -299,83 +350,79 @@
                 attentions=outputs.attentions,
             )
         else:
             hidden_states = outputs.get("hidden_states")
             return {"hidden_states": hidden_states}
 
     @staticmethod
-    def mistral_for_sequence_classification_forward(
-        self,
-        input_ids: torch.LongTensor = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
+    def opt_for_sequence_classification_forward(
+        self: OPTForSequenceClassification,
+        input_ids: Optional[torch.LongTensor] = None,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         labels: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
-        shard_config: ShardConfig = None,
+        shard_config: Optional[ShardConfig] = None,
     ) -> Union[Tuple, SequenceClassifierOutputWithPast]:
         r"""
-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
+        This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForSequenceClassification.forward.
+        Please refer to original code of transformers for more details.
         """
+
+        logger = logging.get_logger(__name__)
+
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        transformer_outputs = MistralForwards.mistral_model_forward(
+        transformer_outputs = OPTPipelineForwards.opt_model_forward(
             self.model,
             input_ids,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
             past_key_values=past_key_values,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
             inputs_embeds=inputs_embeds,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
             shard_config=shard_config,
         )
 
-        if input_ids is not None:
-            batch_size = input_ids.shape[0]
-        elif inputs_embeds is not None:
-            batch_size = inputs_embeds.shape[0]
-        else:
-            batch_size = hidden_states.shape[0]
-
         if stage_manager.is_last_stage():
             hidden_states = transformer_outputs[0]
             logits = self.score(hidden_states)
-            if self.config.pad_token_id is None and batch_size != 1:
-                raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
+
+            batch_size = input_ids.shape[0] if input_ids is not None else hidden_states.shape[0]
+
             if self.config.pad_token_id is None:
                 sequence_lengths = -1
             else:
                 if input_ids is not None:
-                    sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1).to(
-                        logits.device
-                    )
+                    sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)
                 else:
                     sequence_lengths = -1
+                    logger.warning(
+                        f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
+                        "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
+                    )
 
             pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]
 
             loss = None
             if labels is not None:
-                labels = labels.to(logits.device)
                 if self.config.problem_type is None:
                     if self.num_labels == 1:
                         self.config.problem_type = "regression"
                     elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                         self.config.problem_type = "single_label_classification"
                     else:
                         self.config.problem_type = "multi_label_classification"
@@ -388,39 +435,197 @@
                         loss = loss_fct(pooled_logits, labels)
                 elif self.config.problem_type == "single_label_classification":
                     loss_fct = CrossEntropyLoss()
                     loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
                 elif self.config.problem_type == "multi_label_classification":
                     loss_fct = BCEWithLogitsLoss()
                     loss = loss_fct(pooled_logits, labels)
+
             if not return_dict:
                 output = (pooled_logits,) + transformer_outputs[1:]
                 return ((loss,) + output) if loss is not None else output
+
+            return SequenceClassifierOutputWithPast(
+                loss=loss,
+                logits=pooled_logits,
+                past_key_values=transformer_outputs.past_key_values,
+                hidden_states=transformer_outputs.hidden_states,
+                attentions=transformer_outputs.attentions,
+            )
         else:
             hidden_states = transformer_outputs.get("hidden_states")
             return {"hidden_states": hidden_states}
 
-        return SequenceClassifierOutputWithPast(
-            loss=loss,
-            logits=pooled_logits,
-            past_key_values=transformer_outputs.past_key_values,
-            hidden_states=transformer_outputs.hidden_states,
-            attentions=transformer_outputs.attentions,
+    @staticmethod
+    def opt_for_question_answering_forward(
+        self: OPTForQuestionAnswering,
+        input_ids: Optional[torch.LongTensor] = None,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        start_positions: Optional[torch.LongTensor] = None,
+        end_positions: Optional[torch.LongTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        stage_manager: Optional[PipelineStageManager] = None,
+        hidden_states: Optional[torch.FloatTensor] = None,
+        stage_index: Optional[List[int]] = None,
+        shard_config: Optional[ShardConfig] = None,
+    ) -> Union[Tuple, QuestionAnsweringModelOutput]:
+        r"""
+        This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForQuestionAnswering.forward.
+        Please refer to original code of transformers for more details.
+        """
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        transformer_outputs = OPTPipelineForwards.opt_model_forward(
+            self.model,
+            input_ids,
+            past_key_values=past_key_values,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+            stage_manager=stage_manager,
+            hidden_states=hidden_states,
+            stage_index=stage_index,
+            shard_config=shard_config,
         )
+        if stage_manager.is_last_stage():
+            hidden_states = transformer_outputs[0]
 
+            logits = self.qa_outputs(hidden_states)
+            start_logits, end_logits = logits.split(1, dim=-1)
+            start_logits = start_logits.squeeze(-1).contiguous()
+            end_logits = end_logits.squeeze(-1).contiguous()
+
+            total_loss = None
+            if start_positions is not None and end_positions is not None:
+                # If we are on multi-GPU, split add a dimension
+                if len(start_positions.size()) > 1:
+                    start_positions = start_positions.squeeze(-1)
+                if len(end_positions.size()) > 1:
+                    end_positions = end_positions.squeeze(-1)
+                # sometimes the start/end positions are outside our model inputs, we ignore these terms
+                ignored_index = start_logits.size(1)
+                start_positions = start_positions.clamp(0, ignored_index)
+                end_positions = end_positions.clamp(0, ignored_index)
+
+                loss_fct = CrossEntropyLoss(ignore_index=ignored_index)
+                start_loss = loss_fct(start_logits, start_positions)
+                end_loss = loss_fct(end_logits, end_positions)
+                total_loss = (start_loss + end_loss) / 2
 
-def get_mistral_model_forward_for_flash_attn(shard_config: ShardConfig):
-    logger = logging.get_logger(__name__)
-    assert shard_config.enable_flash_attention, "Flash Attention is not enabled."
+            if not return_dict:
+                output = (start_logits, end_logits) + transformer_outputs[2:]
+                return ((total_loss,) + output) if total_loss is not None else output
+
+            return QuestionAnsweringModelOutput(
+                loss=total_loss,
+                start_logits=start_logits,
+                end_logits=end_logits,
+                hidden_states=transformer_outputs.hidden_states,
+                attentions=transformer_outputs.attentions,
+            )
+        else:
+            hidden_states = transformer_outputs.get("hidden_states")
+            return {"hidden_states": hidden_states}
+
+
+def get_opt_flash_attention_forward(shard_config: ShardConfig):
+    from transformers.models.opt.modeling_opt import OPTAttention
+
+    def forward(
+        self: OPTAttention,
+        hidden_states: torch.Tensor,
+        key_value_states: Optional[torch.Tensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        attention_mask: Optional[dict] = None,
+        layer_head_mask: Optional[torch.Tensor] = None,
+        output_attentions: bool = False,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+        """Input shape: Batch x Time x Channel"""
+        assert layer_head_mask is None, "layer_head_mask is not supported for FlashAttention"
+        # if key_value_states are provided this layer is used as a cross-attention layer
+        # for the decoder
+        is_cross_attention = key_value_states is not None
+
+        bsz, tgt_len, _ = hidden_states.size()
+
+        # get query proj
+        query_states = self.q_proj(hidden_states)
+        # get key, value proj
+        if is_cross_attention and past_key_value is not None:
+            # reuse k,v, cross_attentions
+            key_states = past_key_value[0]
+            value_states = past_key_value[1]
+        elif is_cross_attention:
+            # cross_attentions
+            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
+            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
+        elif past_key_value is not None:
+            # reuse k, v, self_attention
+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
+            key_states = torch.cat([past_key_value[0], key_states], dim=2)
+            value_states = torch.cat([past_key_value[1], value_states], dim=2)
+        else:
+            # self_attention
+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
+
+        if self.is_decoder:
+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
+            # Further calls to cross_attention layer can then reuse all cross-attention
+            # key/value_states (first "if" case)
+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
+            # all previous decoder key/value_states. Further calls to uni-directional self-attention
+            # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
+            # if encoder bi-directional self-attention `past_key_value` is always `None`
+            past_key_value = (key_states, value_states)
+
+        query_states = self._shape(query_states, tgt_len, bsz)
+
+        dropout_p = self.dropout if self.training else 0.0
+        attn_output = ColoAttention.attention(
+            query_states,
+            key_states,
+            value_states,
+            **attention_mask,
+            dropout_p=dropout_p,
+            scale=self.scaling,
+        )
+
+        attn_output = attn_output.transpose(1, 2)
+
+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
+        # partitioned aross GPUs when using tensor-parallelism.
+        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
+
+        attn_output = self.out_proj(attn_output)
+
+        return attn_output, None, past_key_value
+
+    return forward
+
+
+def get_opt_decoder_forward_for_flash_attention(shard_config: ShardConfig):
+    from transformers.models.opt.modeling_opt import OPTDecoder
 
     def forward(
-        self: MistralModel,
+        self: OPTDecoder,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
     ) -> Union[Tuple, BaseModelOutputWithPast]:
@@ -432,289 +637,210 @@
 
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
         # retrieve input_ids and inputs_embeds
         if input_ids is not None and inputs_embeds is not None:
             raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
         elif input_ids is not None:
-            batch_size, seq_length = input_ids.shape
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
         elif inputs_embeds is not None:
-            batch_size, seq_length, _ = inputs_embeds.shape
+            input_shape = inputs_embeds.size()[:-1]
         else:
             raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
 
-        past_key_values_length = 0
-
-        if use_cache:
-            use_legacy_cache = not isinstance(past_key_values, Cache)
-            if use_legacy_cache:
-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)
-            past_key_values_length = past_key_values.get_usable_length(seq_length)
-
-        if position_ids is None:
-            device = input_ids.device if input_ids is not None else inputs_embeds.device
-            position_ids = torch.arange(
-                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
-            )
-            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
-        else:
-            position_ids = position_ids.view(-1, seq_length).long()
-
         if inputs_embeds is None:
             inputs_embeds = self.embed_tokens(input_ids)
 
-        if attention_mask is not None and self._use_flash_attention_2 and use_cache:
-            is_padding_right = attention_mask[:, -1].sum().item() != batch_size
-            if is_padding_right:
-                raise ValueError(
-                    "You are attempting to perform batched generation with padding_side='right'"
-                    " this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to "
-                    " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
-                )
-        if shard_config.enable_flash_attention:
-            # in this case, attention_mask is a dict rather than a tensor
-            mask_shape = (batch_size, 1, seq_length, seq_length)
-            attention_mask = ColoAttention.prepare_attn_kwargs(
-                mask_shape,
-                inputs_embeds.dtype,
-                inputs_embeds.device,
-                q_padding_mask=attention_mask,
-                is_causal=True,
+        batch_size, seq_length = input_shape
+        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
+        # required mask seq length can be calculated via length of past
+        mask_seq_length = past_key_values_length + seq_length
+
+        # embed positions
+        if attention_mask is None:
+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
+        elif attention_mask.shape[1] != mask_seq_length:
+            raise ValueError(
+                f"The provided attention mask has length {attention_mask.shape[1]}, but its length should be "
+                f"{mask_seq_length} (sum of the lengths of current and past inputs)"
             )
-        else:
-            if self._use_flash_attention_2:
-                # 2d mask is passed through the layers
-                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
-            else:
-                # 4d mask is passed through the layers
-                attention_mask = _prepare_4d_causal_attention_mask(
-                    attention_mask,
-                    (batch_size, seq_length),
-                    inputs_embeds,
-                    past_key_values_length,
-                    sliding_window=self.config.sliding_window,
-                )
+        causal_attention_mask = _get_attention_mask(
+            self, shard_config, inputs_embeds, past_key_values_length, attention_mask
+        )
+        pos_embeds = self.embed_positions(attention_mask, past_key_values_length)
+
+        if self.project_in is not None:
+            inputs_embeds = self.project_in(inputs_embeds)
 
-        hidden_states = inputs_embeds
+        hidden_states = inputs_embeds + pos_embeds
 
         if self.gradient_checkpointing and self.training:
             if use_cache:
                 logger.warning_once(
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
                 use_cache = False
 
         # decoder layers
         all_hidden_states = () if output_hidden_states else None
         all_self_attns = () if output_attentions else None
-        next_decoder_cache = None
+        next_decoder_cache = () if use_cache else None
 
-        for decoder_layer in self.layers:
+        # check if head_mask has a correct number of layers specified if desired
+        for attn_mask, mask_name in zip([head_mask], ["head_mask"]):
+            if attn_mask is not None:
+                if attn_mask.size()[0] != (len(self.layers)):
+                    raise ValueError(
+                        f"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for"
+                        f" {head_mask.size()[0]}."
+                    )
+
+        for idx, decoder_layer in enumerate(self.layers):
+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
             if output_hidden_states:
                 all_hidden_states += (hidden_states,)
 
+            if self.training:
+                dropout_probability = torch.rand([])
+                if dropout_probability < self.layerdrop:
+                    continue
+
+            past_key_value = past_key_values[idx] if past_key_values is not None else None
+
             if self.gradient_checkpointing and self.training:
-                layer_outputs = self._gradient_checkpointing_func(
-                    decoder_layer.__call__,
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        # None for past_key_value
+                        return module(*inputs, output_attentions, None)
+
+                    return custom_forward
+
+                layer_outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(decoder_layer),
                     hidden_states,
-                    attention_mask,
-                    position_ids,
-                    past_key_values,
-                    output_attentions,
-                    use_cache,
+                    causal_attention_mask,
+                    head_mask[idx] if head_mask is not None else None,
+                    None,
                 )
             else:
                 layer_outputs = decoder_layer(
                     hidden_states,
-                    attention_mask=attention_mask,
-                    position_ids=position_ids,
-                    past_key_value=past_key_values,
+                    attention_mask=causal_attention_mask,
+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                    past_key_value=past_key_value,
                     output_attentions=output_attentions,
                     use_cache=use_cache,
                 )
 
             hidden_states = layer_outputs[0]
 
             if use_cache:
-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]
+                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
 
             if output_attentions:
                 all_self_attns += (layer_outputs[1],)
 
-        hidden_states = self.norm(hidden_states)
+        if self.final_layer_norm is not None:
+            hidden_states = self.final_layer_norm(hidden_states)
+
+        if self.project_out is not None:
+            hidden_states = self.project_out(hidden_states)
 
         # add hidden states from the last decoder layer
         if output_hidden_states:
             all_hidden_states += (hidden_states,)
 
-        next_cache = None
-        if use_cache:
-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache
-
+        next_cache = next_decoder_cache if use_cache else None
         if not return_dict:
             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
         return BaseModelOutputWithPast(
             last_hidden_state=hidden_states,
             past_key_values=next_cache,
             hidden_states=all_hidden_states,
             attentions=all_self_attns,
         )
 
     return forward
 
 
-def get_mistral_flash_attention_forward(shard_config: ShardConfig):
-    from transformers.models.mistral.modeling_mistral import MistralAttention, apply_rotary_pos_emb, repeat_kv
+def get_jit_fused_opt_decoder_layer_forward():
+    from transformers.models.opt.modeling_opt import OPTDecoderLayer
 
     def forward(
-        self: MistralAttention,
+        self: OPTDecoderLayer,
         hidden_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Cache] = None,
-        output_attentions: bool = False,
-        use_cache: bool = False,
-        **kwargs,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-        if "padding_mask" in kwargs:
-            warnings.warn(
-                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
-            )
-        bsz, q_len, _ = hidden_states.size()
+        layer_head_mask: Optional[torch.Tensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        output_attentions: Optional[bool] = False,
+        use_cache: Optional[bool] = False,
+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
+        """
+        Args:
+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
+            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
+            layer_head_mask (`torch.FloatTensor`, *optional*): mask for attention heads in a given layer of size
+                `(encoder_attention_heads,)`.
+            output_attentions (`bool`, *optional*):
+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
+                returned tensors for more detail.
+            use_cache (`bool`, *optional*):
+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
+                (see `past_key_values`).
+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
+        """
 
-        query_states = self.q_proj(hidden_states)
-        key_states = self.k_proj(hidden_states)
-        value_states = self.v_proj(hidden_states)
+        residual = hidden_states
 
-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
-
-        kv_seq_len = key_states.shape[-2]
-        if past_key_value is not None:
-            if self.layer_idx is None:
-                raise ValueError(
-                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
-                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
-                    "with a layer index."
-                )
-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
+        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
+        if self.do_layer_norm_before:
+            hidden_states = self.self_attn_layer_norm(hidden_states)
 
-        if past_key_value is not None:
-            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
+        # Self Attention
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            past_key_value=past_key_value,
+            attention_mask=attention_mask,
+            layer_head_mask=layer_head_mask,
+            output_attentions=output_attentions,
+        )
 
-        # repeat k/v heads if n_kv_heads < n_heads
-        key_states = repeat_kv(key_states, self.num_key_value_groups)
-        value_states = repeat_kv(value_states, self.num_key_value_groups)
+        hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training)
 
-        assert isinstance(attention_mask, dict), "Flash Attention Error: attention_mask should be a dict."
-        attn_output = ColoAttention.attention(query_states, key_states, value_states, **attention_mask)
+        # 350m applies layer norm AFTER attention
+        if not self.do_layer_norm_before:
+            hidden_states = self.self_attn_layer_norm(hidden_states)
 
-        attn_output = attn_output.transpose(1, 2).contiguous()
-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
+        # Fully Connected
+        hidden_states_shape = hidden_states.shape
+        hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))
+        residual = hidden_states
 
-        attn_output = self.o_proj(attn_output)
+        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
+        if self.do_layer_norm_before:
+            hidden_states = self.final_layer_norm(hidden_states)
 
-        return attn_output, None, past_key_value
+        hidden_states = self.fc1(hidden_states)
+        hidden_states = self.activation_fn(hidden_states)
 
-    return forward
+        hidden_states = self.fc2(hidden_states)
 
+        hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training).view(hidden_states_shape)
 
-def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
-    from transformers import MistralForCausalLM
+        # 350m applies layer norm AFTER attention
+        if not self.do_layer_norm_before:
+            hidden_states = self.final_layer_norm(hidden_states)
 
-    def forward(
-        self: MistralForCausalLM,
-        input_ids: torch.LongTensor = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        labels: Optional[torch.LongTensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-    ) -> Union[Tuple, CausalLMOutputWithPast]:
-        r"""
-        Args:
-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
+        outputs = (hidden_states,)
 
-        Returns:
+        if output_attentions:
+            outputs += (self_attn_weights,)
 
-        Example:
-
-        ```python
-        >>> from transformers import AutoTokenizer, MistralForCausalLM
-
-        >>> model = MistralForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
-        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)
-
-        >>> prompt = "Hey, are you conscious? Can you talk to me?"
-        >>> inputs = tokenizer(prompt, return_tensors="pt")
-
-        >>> # Generate
-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
-        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
-        ```"""
-
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        outputs = self.model(
-            input_ids=input_ids,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-        )
-
-        hidden_states = outputs[0]
-        logits = self.lm_head(hidden_states)
-        logits = logits.float()
-
-        loss = None
-        if labels is not None:
-            # Shift so that tokens < n predict n
-            shift_logits = logits[..., :-1, :].contiguous()
-            shift_labels = labels[..., 1:].contiguous()
-            shift_labels = shift_labels.view(-1)
-            # Enable model parallelism
-            shift_labels = shift_labels.to(shift_logits.device)
-            new_vocab_size = logits.shape[-1]
-            shift_logits = shift_logits.view(-1, new_vocab_size)
-            loss = cross_entropy_1d(
-                shift_logits,
-                shift_labels,
-                process_group=shard_config.tensor_parallel_process_group,
-                vocab_size=self.lm_head.out_features,
-                dtype=self.model.dtype,
-            )
-
-        if not return_dict:
-            output = (logits,) + outputs[1:]
-            return (loss,) + output if loss is not None else output
+        if use_cache:
+            outputs += (present_key_value,)
 
-        return CausalLMOutputWithPast(
-            loss=loss,
-            logits=logits,
-            past_key_values=outputs.past_key_values,
-            hidden_states=outputs.hidden_states,
-            attentions=outputs.attentions,
-        )
+        return outputs
 
     return forward
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/opt.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/t5.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,1007 +1,785 @@
-import random
-from typing import List, Optional, Tuple, Union
+import warnings
+from typing import Dict, List, Optional, Tuple, Union
 
 import torch
-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
-from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
+from torch.nn import CrossEntropyLoss
 from transformers.modeling_outputs import (
-    BaseModelOutputWithPast,
-    CausalLMOutputWithPast,
-    QuestionAnsweringModelOutput,
-    SequenceClassifierOutputWithPast,
-)
-from transformers.models.opt.modeling_opt import (
-    OPTForCausalLM,
-    OPTForQuestionAnswering,
-    OPTForSequenceClassification,
-    OPTModel,
+    BaseModelOutput,
+    BaseModelOutputWithPastAndCrossAttentions,
+    Seq2SeqLMOutput,
+    Seq2SeqModelOutput,
 )
+from transformers.models.t5.modeling_t5 import T5EncoderModel, T5ForConditionalGeneration, T5Model, T5Stack
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
-from colossalai.shardformer.layer import ColoAttention
-from colossalai.shardformer.shard import ShardConfig
-
-from ..layer import cross_entropy_1d
-
-logger = logging.get_logger(__name__)
-
-
-def _get_attention_mask(
-    self: OPTModel,
-    shard_config: ShardConfig,
-    hidden_states: torch.Tensor,
-    past_key_values_length: int,
-    attention_mask: Optional[torch.FloatTensor],
-):
-    batch_size, seq_length = hidden_states.shape[:2]
-    mask_seq_length = past_key_values_length + seq_length
-    if shard_config.enable_flash_attention:
-        attention_mask = ColoAttention.prepare_attn_kwargs(
-            (batch_size, 1, seq_length, mask_seq_length),
-            hidden_states.dtype,
-            hidden_states.device,
-            attention_mask,
-            is_causal=True,
-        )
-    else:
-        attention_mask = _prepare_4d_causal_attention_mask(
-            attention_mask,
-            (batch_size, seq_length),
-            hidden_states,
-            past_key_values_length,
-        )
-    return attention_mask
 
 
-class OPTPipelineForwards:
+class T5PipelineForwards:
     """
-    This class serves as a micro library for forward function substitution of OPT models
-    under pipeline setting.
+    This class serves as a micro library for forward function substitution of
+    T5 models under pipeline setting.
     """
 
     @staticmethod
-    def opt_model_forward(
-        self: OPTModel,
-        input_ids: torch.LongTensor = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
+    def t5_stack_forward(
+        self: T5Stack,
+        input_ids: Optional[torch.LongTensor] = None,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        encoder_hidden_states: Optional[torch.Tensor] = None,
+        encoder_attention_mask: Optional[torch.FloatTensor] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        cross_attn_head_mask: Optional[torch.Tensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        use_cache: Optional[bool] = False,
+        output_attentions: Optional[bool] = False,
+        output_hidden_states: Optional[bool] = False,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
+        position_bias: Optional[torch.Tensor] = None,
+        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
         stage_index: Optional[List[int]] = None,
-        shard_config: Optional[ShardConfig] = None,
-    ) -> Union[Tuple, BaseModelOutputWithPast]:
-        """
-        This forward method is modified based on transformers.models.opt.modeling_opt.OPTModel.forward
-        """
-
-        from transformers.modeling_outputs import BaseModelOutputWithPast
-        from transformers.utils import logging
+        decoder_starting_stage: Optional[int] = None,
+    ) -> Union[Dict, Tuple, BaseModelOutputWithPastAndCrossAttentions]:
+        # This function is modified on the basis of transformers.models.t5.modeling_t5.T5Stack.forward.
+        # Please refer to original code of transformers for more details.
 
         logger = logging.get_logger(__name__)
 
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        # TODO(baizhou): left the recording kv-value tensors as () or None type, this feature may be added in the future.
+        if past_key_values:
+            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
+            past_key_values = None
+        if output_attentions:
+            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
+            output_attentions = False
+        if output_hidden_states:
+            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
+            output_hidden_states = False
+        if use_cache:
+            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
+            use_cache = False
+        if use_cache is True:
+            if not in_decoder:
+                raise ValueError(f"`use_cache` can only be set to `True` if {self} is used as a decoder")
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
+
+        stage = stage_manager.stage
+        in_decoder = self.is_decoder
+        if in_decoder != (stage >= decoder_starting_stage):
+            raise ValueError("Config in T5Stack is not aligned with pipeline setting.")
+
+        # at_first_stage: current stage is the first stage of encoder/decoder, taking input_ids/input_embeds
+        # at_last_stage: current stage is the last stage of encoder/decoder, making outputs the same form as huggingface
+        at_first_stage = (stage == 0) or (stage == decoder_starting_stage)
+        at_last_stage = (stage == decoder_starting_stage - 1) or (stage == stage_manager.num_stages - 1)
 
-        decoder = self.decoder
-        if stage_manager.is_first_stage():
-            # retrieve input_ids and inputs_embeds
+        # Process inputs if at the first stage of encoder/decoder.
+        if at_first_stage:
             if input_ids is not None and inputs_embeds is not None:
-                raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+                err_msg_prefix = "decoder_" if in_decoder else ""
+                raise ValueError(
+                    f"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time"
+                )
             elif input_ids is not None:
                 input_shape = input_ids.size()
                 input_ids = input_ids.view(-1, input_shape[-1])
             elif inputs_embeds is not None:
                 input_shape = inputs_embeds.size()[:-1]
             else:
-                raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
-
-            batch_size, seq_length = input_shape
-
+                err_msg_prefix = "decoder_" if in_decoder else ""
+                raise ValueError(
+                    f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds"
+                )
             if inputs_embeds is None:
-                inputs_embeds = decoder.embed_tokens(input_ids)
-
-            if decoder.project_in is not None:
-                inputs_embeds = decoder.project_in(inputs_embeds)
-            device = input_ids.device if input_ids is not None else inputs_embeds.device
-            inputs_embeds.dtype
-            hidden_states = inputs_embeds
+                if self.embed_tokens is None:
+                    raise ValueError("You have to initialize the model with valid token embeddings")
+                inputs_embeds = self.embed_tokens(input_ids)
+            batch_size, seq_length = input_shape
+            device = inputs_embeds.device
+            hidden_states = self.dropout(inputs_embeds)
         else:
             if hidden_states is None:
-                raise ValueError("hidden_states shouldn't be None for intermediate stages.")
+                raise ValueError(
+                    "hidden_states shouldn't be None for stages other than the first stage of encoder/decoder."
+                )
             input_shape = hidden_states.size()[:-1]
             batch_size, seq_length = input_shape[0], input_shape[1]
             device = hidden_states.device
-            hidden_states.dtype
 
-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
         # required mask seq length can be calculated via length of past
-        mask_seq_length = past_key_values_length + seq_length
-        # embed positions
-        if self.decoder._use_flash_attention_2:
-            # 2d mask is passed through the layers
-            causal_attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
-            attention_mask = (
-                torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
-                if attention_mask is None
-                else attention_mask
-            )
-        else:
-            # 4d mask is passed through the layers
-            if attention_mask is None:
-                attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
-            elif attention_mask.shape[1] != mask_seq_length:
-                raise ValueError(
-                    f"The provided attention mask has length {attention_mask.shape[1]}, but its length should be "
-                    f"{mask_seq_length} (sum of the lengths of current and past inputs)"
-                )
-            causal_attention_mask = _prepare_4d_causal_attention_mask(
-                attention_mask, input_shape, hidden_states, past_key_values_length
-            )
+        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length
 
-        if stage_manager.is_first_stage():
-            causal_attention_mask = _get_attention_mask(
-                self,
-                shard_config,
-                inputs_embeds,
-                past_key_values_length,
-                attention_mask,
-            )
-            pos_embeds = decoder.embed_positions(attention_mask, past_key_values_length)
-            hidden_states = inputs_embeds + pos_embeds
-        else:
-            causal_attention_mask = _get_attention_mask(
-                self,
-                shard_config,
-                hidden_states,
-                past_key_values_length,
-                attention_mask,
-            )
+        # initialize past_key_values with `None` if past does not exist
+        if past_key_values is None:
+            past_key_values = [None] * len(self.block)
 
-        if decoder.gradient_checkpointing and decoder.training:
-            if use_cache:
-                logger.warning_once(
-                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
-                )
-                use_cache = False
+        if attention_mask is None:
+            attention_mask = torch.ones(batch_size, mask_seq_length, device=device)
 
-        # TODO(baizhou): left the recording kv-value tensors as () or None type, this feature may be added in the future.
-        if past_key_values:
-            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
-            past_key_values = None
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-        if use_cache:
-            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
-            use_cache = False
+        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
+        # ourselves in which case we just need to make it broadcastable to all heads.
+        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)
+
+        # If a 2D or 3D attention mask is provided for the cross-attention
+        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
+        if self.is_decoder and encoder_hidden_states is not None:
+            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
+            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
+            if encoder_attention_mask is None:
+                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device, dtype=torch.long)
+            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)
+        else:
+            encoder_extended_attention_mask = None
 
-        # decoder layers
+        # Prepare head mask if needed
+        head_mask = self.get_head_mask(head_mask, self.config.num_layers)
+        cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)
+        present_key_value_states = () if use_cache else None
         all_hidden_states = () if output_hidden_states else None
-        all_self_attns = () if output_attentions else None
-        next_decoder_cache = () if use_cache else None
-
-        # check if head_mask has a correct number of layers specified if desired
-        for attn_mask, mask_name in zip([head_mask], ["head_mask"]):
-            if attn_mask is not None:
-                if attn_mask.size()[0] != (len(decoder.layers)):
-                    raise ValueError(
-                        f"The `{mask_name}` should be specified for {len(decoder.layers)} layers, but it is for"
-                        f" {head_mask.size()[0]}."
-                    )
+        all_attentions = () if output_attentions else None
+        all_cross_attentions = () if (output_attentions and self.is_decoder) else None
 
+        # Going through held blocks.
         start_idx, end_idx = stage_index[0], stage_index[1]
 
-        torch.cuda.set_device(device)
-
-        for idx in range(start_idx, end_idx):
-            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
-            decoder_layer = decoder.layers[idx]
-
-            if output_hidden_states:
-                all_hidden_states += (hidden_states,)
+        for i in range(start_idx, end_idx):
+            past_key_value = past_key_values[i]
+            layer_module = self.block[i]
+            layer_head_mask = head_mask[i]
+            cross_attn_layer_head_mask = cross_attn_head_mask[i]
+            torch.cuda.set_device(hidden_states.device)
 
-            dropout_probability = random.uniform(0, 1)
-            if decoder.training and (dropout_probability < decoder.layerdrop):
-                continue
-
-            past_key_value = past_key_values[idx] if past_key_values is not None else None
-
-            if decoder.gradient_checkpointing and decoder.training:
+            if self.gradient_checkpointing and self.training:
                 layer_outputs = self._gradient_checkpointing_func(
-                    decoder_layer.__call__,
+                    layer_module.forward,
                     hidden_states,
-                    causal_attention_mask,
-                    head_mask[idx] if head_mask is not None else None,
-                    None,
-                    output_attentions,
+                    extended_attention_mask,
+                    position_bias,
+                    encoder_hidden_states,
+                    encoder_extended_attention_mask,
+                    encoder_decoder_position_bias,
+                    layer_head_mask,
+                    cross_attn_layer_head_mask,
+                    None,  # past_key_value is always None with gradient checkpointing
                     use_cache,
+                    output_attentions,
                 )
             else:
-                layer_outputs = decoder_layer(
+                layer_outputs = layer_module(
                     hidden_states,
-                    attention_mask=causal_attention_mask,
-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                    attention_mask=extended_attention_mask,
+                    position_bias=position_bias,
+                    encoder_hidden_states=encoder_hidden_states,
+                    encoder_attention_mask=encoder_extended_attention_mask,
+                    encoder_decoder_position_bias=encoder_decoder_position_bias,
+                    layer_head_mask=layer_head_mask,
+                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,
                     past_key_value=past_key_value,
-                    output_attentions=output_attentions,
                     use_cache=use_cache,
+                    output_attentions=output_attentions,
                 )
 
-            hidden_states = layer_outputs[0]
+            # layer_outputs is a tuple with:
+            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)
 
+            if use_cache is False or use_cache is None:
+                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]
+            hidden_states, present_key_value_state = layer_outputs[:2]
+
+            # We share the position biases between the layers - the first layer store them
+            # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),
+            # (cross-attention position bias), (cross-attention weights)
+            position_bias = layer_outputs[2]
+
+            if in_decoder and encoder_hidden_states is not None:
+                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]
+            # append next layer key value states
             if use_cache:
-                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
-
-            if output_attentions:
-                all_self_attns += (layer_outputs[1],)
-
-        if stage_manager.is_last_stage():
-            if decoder.final_layer_norm is not None:
-                hidden_states = decoder.final_layer_norm(hidden_states)
-            if decoder.project_out is not None:
-                hidden_states = decoder.project_out(hidden_states)
-
-        # add hidden states from the last decoder layer
-        if output_hidden_states:
-            all_hidden_states += (hidden_states,)
+                present_key_value_states = present_key_value_states + (present_key_value_state,)
 
-        next_cache = next_decoder_cache if use_cache else None
+        # last layer
+        if at_last_stage:
+            hidden_states = self.final_layer_norm(hidden_states)
+            hidden_states = self.dropout(hidden_states)
 
-        if stage_manager.is_last_stage():
             if not return_dict:
                 return tuple(
                     v
                     for v in [
                         hidden_states,
-                        next_cache,
+                        present_key_value_states,
                         all_hidden_states,
-                        all_self_attns,
+                        all_attentions,
+                        all_cross_attentions,
                     ]
                     if v is not None
                 )
-
-            return BaseModelOutputWithPast(
+            return BaseModelOutputWithPastAndCrossAttentions(
                 last_hidden_state=hidden_states,
-                past_key_values=next_cache,
+                past_key_values=present_key_value_states,
                 hidden_states=all_hidden_states,
-                attentions=all_self_attns,
+                attentions=all_attentions,
+                cross_attentions=all_cross_attentions,
             )
         else:
-            return {"hidden_states": hidden_states}
+            return {
+                "hidden_states": hidden_states,
+                "position_bias": position_bias,
+                "encoder_decoder_position_bias": encoder_decoder_position_bias,
+                "backward_tensor_keys": ["hidden_states"],
+            }
 
     @staticmethod
-    def opt_for_causal_lm_forward(
-        self: OPTForCausalLM,
-        input_ids: torch.LongTensor = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        labels: Optional[torch.LongTensor] = None,
+    def t5_model_forward(
+        self: T5Model,
+        input_ids: Optional[torch.LongTensor] = None,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        decoder_input_ids: Optional[torch.LongTensor] = None,
+        decoder_attention_mask: Optional[torch.BoolTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        decoder_head_mask: Optional[torch.FloatTensor] = None,
+        cross_attn_head_mask: Optional[torch.Tensor] = None,
+        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        decoder_inputs_embeds: Optional[torch.Tensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
+        encoder_hidden_states: Optional[torch.FloatTensor] = None,
+        position_bias: Optional[torch.Tensor] = None,
+        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
+        backward_tensor_keys: Optional[List[str]] = None,
         stage_index: Optional[List[int]] = None,
-        shard_config: Optional[ShardConfig] = None,
-    ) -> Union[Tuple, CausalLMOutputWithPast]:
-        r"""
-        This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForCausalLM.forward.
-        Please refer to original code of transformers for more details.
+        decoder_starting_stage: Optional[int] = None,
+    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:
+        # This function is modified on the basis of transformers.models.t5.modeling_t5.T5Model.forward.
+        # Please refer to original code of transformers for more details.
+
+        __HEAD_MASK_WARNING_MSG = """
+        The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,
+        `decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.
+        If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,
+        num_heads)`.
         """
 
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        outputs = OPTPipelineForwards.opt_model_forward(
-            self.model,
-            input_ids=input_ids,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
+        logger = logging.get_logger(__name__)
+
+        # TODO(baizhou): left the recording kv-value tensors as () or None type, this feature may be added in the future.
+        if past_key_values:
+            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
+            past_key_values = None
+        if output_attentions:
+            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
+            output_attentions = False
+        if output_hidden_states:
+            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
+            output_hidden_states = False
+        if use_cache:
+            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
+            use_cache = False
+
+        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask
+        if head_mask is not None and decoder_head_mask is None:
+            if self.config.num_layers == self.config.num_decoder_layers:
+                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)
+                decoder_head_mask = head_mask
+
+        in_decoder = stage_manager.stage >= decoder_starting_stage
+        # Stage is in encoder, directly return the output of t5_stack_forward
+        if not in_decoder:
+            encoder_outputs = T5PipelineForwards.t5_stack_forward(
+                self.encoder,
+                input_ids=input_ids,
+                attention_mask=attention_mask,
+                inputs_embeds=inputs_embeds,
+                head_mask=head_mask,
+                output_attentions=output_attentions,
+                output_hidden_states=output_hidden_states,
+                return_dict=return_dict,
+                stage_manager=stage_manager,
+                hidden_states=hidden_states,
+                position_bias=position_bias,
+                encoder_decoder_position_bias=encoder_decoder_position_bias,
+                stage_index=stage_index,
+                decoder_starting_stage=decoder_starting_stage,
+            )
+            if stage_manager.stage == decoder_starting_stage - 1:
+                # last stage of encoder
+                return {"encoder_hidden_states": encoder_outputs[0]}
+            else:
+                return encoder_outputs
+
+        at_last_decoder_stage = stage_manager.is_last_stage()
+        at_first_decoder_stage = stage_manager.stage == decoder_starting_stage
+
+        if encoder_outputs is not None:
+            encoder_hidden_states = encoder_outputs[0]
+        elif encoder_hidden_states is None:
+            raise ValueError("Non-empty encoder_hidden_states should be passed in at decoder stages.")
+
+        if not at_first_decoder_stage and hidden_states is None:
+            raise ValueError("If not at the first layer of decoder, non-empty hidden_states must be provided.")
+
+        # Decode
+        decoder_outputs = T5PipelineForwards.t5_stack_forward(
+            self.decoder,
+            input_ids=decoder_input_ids,
+            attention_mask=decoder_attention_mask,
+            inputs_embeds=decoder_inputs_embeds,
             past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
+            encoder_hidden_states=encoder_hidden_states,
+            encoder_attention_mask=attention_mask,
+            head_mask=decoder_head_mask,
+            cross_attn_head_mask=cross_attn_head_mask,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
+            position_bias=position_bias,
+            encoder_decoder_position_bias=encoder_decoder_position_bias,
             stage_index=stage_index,
-            shard_config=shard_config,
+            decoder_starting_stage=decoder_starting_stage,
         )
-        if stage_manager.is_last_stage():
-            logits = self.lm_head(outputs[0]).contiguous()
-            loss = None
-            if labels is not None:
-                # move labels to correct device to enable model parallelism
-                labels = labels.to(logits.device)
-                # Shift so that tokens < n predict n
-                shift_logits = logits[..., :-1, :].contiguous()
-                shift_labels = labels[..., 1:].contiguous()
-                # Flatten the tokens
-
-                if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
-                    new_vocab_size = logits.shape[-1]
-                    shift_logits = shift_logits.view(-1, new_vocab_size)
-                    shift_labels = shift_labels.view(-1)
-                    loss = cross_entropy_1d(
-                        shift_logits,
-                        shift_labels,
-                        process_group=shard_config.tensor_parallel_process_group,
-                        vocab_size=self.lm_head.out_features,
-                        dtype=self.model.decoder.dtype,
-                    )
-                else:
-                    loss_fct = CrossEntropyLoss()
-                    shift_logits = shift_logits.view(-1, self.config.vocab_size)
-                    loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
 
-            if not return_dict:
-                output = (logits,) + outputs[1:]
-                return (loss,) + output if loss is not None else output
+        # Directly return outputs of overloaded T5Stack forward if not at last stage.
+        if not at_last_decoder_stage:
+            # encoder_hidden_states should be passed to the next stage
+            decoder_outputs["encoder_hidden_states"] = encoder_hidden_states
+            return decoder_outputs
 
-            return CausalLMOutputWithPast(
-                loss=loss,
-                logits=logits,
-                past_key_values=outputs.past_key_values,
-                hidden_states=outputs.hidden_states,
-                attentions=outputs.attentions,
-            )
+        if not return_dict:
+            return decoder_outputs + encoder_hidden_states
         else:
-            hidden_states = outputs.get("hidden_states")
-            return {"hidden_states": hidden_states}
+            return Seq2SeqModelOutput(
+                last_hidden_state=decoder_outputs.last_hidden_state,
+                past_key_values=decoder_outputs.past_key_values,
+                decoder_hidden_states=decoder_outputs.hidden_states,
+                decoder_attentions=decoder_outputs.attentions,
+                cross_attentions=decoder_outputs.cross_attentions,
+                encoder_last_hidden_state=encoder_hidden_states,
+            )
 
     @staticmethod
-    def opt_for_sequence_classification_forward(
-        self: OPTForSequenceClassification,
+    def t5_for_conditional_generation_forward(
+        self: T5ForConditionalGeneration,
         input_ids: Optional[torch.LongTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
+        decoder_input_ids: Optional[torch.LongTensor] = None,
+        decoder_attention_mask: Optional[torch.BoolTensor] = None,
         head_mask: Optional[torch.FloatTensor] = None,
+        decoder_head_mask: Optional[torch.FloatTensor] = None,
+        cross_attn_head_mask: Optional[torch.Tensor] = None,
+        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,
         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
         labels: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
+        encoder_hidden_states: Optional[torch.FloatTensor] = None,
+        position_bias: Optional[torch.Tensor] = None,
+        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
+        backward_tensor_keys: Optional[List[str]] = None,
         stage_index: Optional[List[int]] = None,
-        shard_config: Optional[ShardConfig] = None,
-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:
-        r"""
-        This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForSequenceClassification.forward.
-        Please refer to original code of transformers for more details.
+        decoder_starting_stage: Optional[int] = None,
+    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:
+        # This function is modified on the basis of transformers.models.t5.modeling_t5.T5ForConditionalGeneration.forward.
+        # Please refer to original code of transformers for more details.
+
+        __HEAD_MASK_WARNING_MSG = """
+        The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,
+        `decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.
+        If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,
+        num_heads)`.
         """
 
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
         logger = logging.get_logger(__name__)
 
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        # TODO(baizhou): left the recording kv-value tensors as () or None type, this feature may be added in the future.
+        if past_key_values:
+            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
+            past_key_values = None
+        if output_attentions:
+            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
+            output_attentions = False
+        if output_hidden_states:
+            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
+            output_hidden_states = False
+        if use_cache:
+            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
+            use_cache = False
+
+        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask
+        if head_mask is not None and decoder_head_mask is None:
+            if self.config.num_layers == self.config.num_decoder_layers:
+                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)
+                decoder_head_mask = head_mask
+
+        in_decoder = stage_manager.stage >= decoder_starting_stage
+
+        # Stage is in encoder, directly return the output of t5_stack_forward
+        if not in_decoder:
+            encoder_outputs = T5PipelineForwards.t5_stack_forward(
+                self.encoder,
+                input_ids=input_ids,
+                attention_mask=attention_mask,
+                inputs_embeds=inputs_embeds,
+                head_mask=head_mask,
+                output_attentions=output_attentions,
+                output_hidden_states=output_hidden_states,
+                return_dict=return_dict,
+                stage_manager=stage_manager,
+                hidden_states=hidden_states,
+                position_bias=position_bias,
+                encoder_decoder_position_bias=encoder_decoder_position_bias,
+                stage_index=stage_index,
+                decoder_starting_stage=decoder_starting_stage,
+            )
+            if stage_manager.stage == decoder_starting_stage - 1:
+                # last stage of encoder
+                return {"encoder_hidden_states": encoder_outputs[0]}
+            else:
+                return encoder_outputs
+
+        at_last_decoder_stage = stage_manager.is_last_stage()
+        at_first_decoder_stage = stage_manager.stage == decoder_starting_stage
 
-        transformer_outputs = OPTPipelineForwards.opt_model_forward(
-            self.model,
-            input_ids,
+        if encoder_outputs is not None:
+            encoder_hidden_states = encoder_outputs[0]
+        elif encoder_hidden_states is None:
+            raise ValueError("Non-empty encoder_hidden_states should be passed in at decoder stages.")
+
+        if not at_first_decoder_stage and hidden_states is None:
+            raise ValueError("If not at the first layer of decoder, non-empty hidden_states must be provided.")
+
+        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
+            # get decoder inputs from shifting lm labels to the right
+            decoder_input_ids = self._shift_right(labels)
+
+        # Decode
+        decoder_outputs = T5PipelineForwards.t5_stack_forward(
+            self.decoder,
+            input_ids=decoder_input_ids,
+            attention_mask=decoder_attention_mask,
+            inputs_embeds=decoder_inputs_embeds,
             past_key_values=past_key_values,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
-            inputs_embeds=inputs_embeds,
+            encoder_hidden_states=encoder_hidden_states,
+            encoder_attention_mask=attention_mask,
+            head_mask=decoder_head_mask,
+            cross_attn_head_mask=cross_attn_head_mask,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
+            position_bias=position_bias,
+            encoder_decoder_position_bias=encoder_decoder_position_bias,
             stage_index=stage_index,
-            shard_config=shard_config,
+            decoder_starting_stage=decoder_starting_stage,
         )
 
-        if stage_manager.is_last_stage():
-            hidden_states = transformer_outputs[0]
-            logits = self.score(hidden_states)
+        # Directly return outputs of overloaded T5Stack forward if not at last stage.
+        if not at_last_decoder_stage:
+            # encoder_hidden_states should be passed to the next stage
+            decoder_outputs["encoder_hidden_states"] = encoder_hidden_states
+            return decoder_outputs
 
-            batch_size = input_ids.shape[0] if input_ids is not None else hidden_states.shape[0]
+        sequence_output = decoder_outputs[0]
 
-            if self.config.pad_token_id is None:
-                sequence_lengths = -1
-            else:
-                if input_ids is not None:
-                    sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)
-                else:
-                    sequence_lengths = -1
-                    logger.warning(
-                        f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
-                        "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
-                    )
-
-            pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]
-
-            loss = None
-            if labels is not None:
-                if self.config.problem_type is None:
-                    if self.num_labels == 1:
-                        self.config.problem_type = "regression"
-                    elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
-                        self.config.problem_type = "single_label_classification"
-                    else:
-                        self.config.problem_type = "multi_label_classification"
-
-                if self.config.problem_type == "regression":
-                    loss_fct = MSELoss()
-                    if self.num_labels == 1:
-                        loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())
-                    else:
-                        loss = loss_fct(pooled_logits, labels)
-                elif self.config.problem_type == "single_label_classification":
-                    loss_fct = CrossEntropyLoss()
-                    loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
-                elif self.config.problem_type == "multi_label_classification":
-                    loss_fct = BCEWithLogitsLoss()
-                    loss = loss_fct(pooled_logits, labels)
+        if self.config.tie_word_embeddings:
+            # Rescale output before projecting on vocab
+            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586
+            sequence_output = sequence_output * (self.model_dim**-0.5)
 
-            if not return_dict:
-                output = (pooled_logits,) + transformer_outputs[1:]
-                return ((loss,) + output) if loss is not None else output
+        lm_logits = self.lm_head(sequence_output)
 
-            return SequenceClassifierOutputWithPast(
-                loss=loss,
-                logits=pooled_logits,
-                past_key_values=transformer_outputs.past_key_values,
-                hidden_states=transformer_outputs.hidden_states,
-                attentions=transformer_outputs.attentions,
-            )
-        else:
-            hidden_states = transformer_outputs.get("hidden_states")
-            return {"hidden_states": hidden_states}
+        loss = None
+        if labels is not None:
+            loss_fct = CrossEntropyLoss(ignore_index=-100)
+            # move labels to correct device to enable PP
+            labels = labels.to(lm_logits.device)
+            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))
+
+        if not return_dict:
+            output = (lm_logits,) + decoder_outputs[1:] + encoder_hidden_states
+            return ((loss,) + output) if loss is not None else output
+
+        return Seq2SeqLMOutput(
+            loss=loss,
+            logits=lm_logits,
+            past_key_values=decoder_outputs.past_key_values,
+            decoder_hidden_states=decoder_outputs.hidden_states,
+            decoder_attentions=decoder_outputs.attentions,
+            cross_attentions=decoder_outputs.cross_attentions,
+            encoder_last_hidden_state=encoder_hidden_states,
+        )
 
     @staticmethod
-    def opt_for_question_answering_forward(
-        self: OPTForQuestionAnswering,
+    def t5_encoder_model_forward(
+        self: T5EncoderModel,
         input_ids: Optional[torch.LongTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         head_mask: Optional[torch.FloatTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
-        start_positions: Optional[torch.LongTensor] = None,
-        end_positions: Optional[torch.LongTensor] = None,
-        use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
+        position_bias: Optional[torch.Tensor] = None,
+        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
+        backward_tensor_keys: Optional[List[str]] = None,
         stage_index: Optional[List[int]] = None,
-        shard_config: Optional[ShardConfig] = None,
-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:
+        decoder_starting_stage: Optional[int] = None,
+    ) -> Union[Tuple[torch.FloatTensor], BaseModelOutput]:
         r"""
-        This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForQuestionAnswering.forward.
+        This function is modified on the basis of transformers.models.t5.modeling_gpt2.T5EncoderModel.forward.
         Please refer to original code of transformers for more details.
-        """
+        ```"""
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        transformer_outputs = OPTPipelineForwards.opt_model_forward(
-            self.model,
-            input_ids,
-            past_key_values=past_key_values,
+        outputs = T5PipelineForwards.t5_stack_forward(
+            self.encoder,
+            input_ids=input_ids,
             attention_mask=attention_mask,
-            head_mask=head_mask,
             inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
+            head_mask=head_mask,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
+            position_bias=position_bias,
+            encoder_decoder_position_bias=encoder_decoder_position_bias,
             stage_index=stage_index,
-            shard_config=shard_config,
+            decoder_starting_stage=decoder_starting_stage,
         )
-        if stage_manager.is_last_stage():
-            hidden_states = transformer_outputs[0]
-
-            logits = self.qa_outputs(hidden_states)
-            start_logits, end_logits = logits.split(1, dim=-1)
-            start_logits = start_logits.squeeze(-1).contiguous()
-            end_logits = end_logits.squeeze(-1).contiguous()
-
-            total_loss = None
-            if start_positions is not None and end_positions is not None:
-                # If we are on multi-GPU, split add a dimension
-                if len(start_positions.size()) > 1:
-                    start_positions = start_positions.squeeze(-1)
-                if len(end_positions.size()) > 1:
-                    end_positions = end_positions.squeeze(-1)
-                # sometimes the start/end positions are outside our model inputs, we ignore these terms
-                ignored_index = start_logits.size(1)
-                start_positions = start_positions.clamp(0, ignored_index)
-                end_positions = end_positions.clamp(0, ignored_index)
-
-                loss_fct = CrossEntropyLoss(ignore_index=ignored_index)
-                start_loss = loss_fct(start_logits, start_positions)
-                end_loss = loss_fct(end_logits, end_positions)
-                total_loss = (start_loss + end_loss) / 2
 
-            if not return_dict:
-                output = (start_logits, end_logits) + transformer_outputs[2:]
-                return ((total_loss,) + output) if total_loss is not None else output
-
-            return QuestionAnsweringModelOutput(
-                loss=total_loss,
-                start_logits=start_logits,
-                end_logits=end_logits,
-                hidden_states=transformer_outputs.hidden_states,
-                attentions=transformer_outputs.attentions,
-            )
-        else:
-            hidden_states = transformer_outputs.get("hidden_states")
-            return {"hidden_states": hidden_states}
+        return outputs
 
 
-def get_opt_flash_attention_forward(shard_config: ShardConfig):
-    from transformers.models.opt.modeling_opt import OPTAttention
+def get_t5_flash_attention_forward():
+    from transformers.models.t5.modeling_t5 import T5Attention
 
     def forward(
-        self: OPTAttention,
+        self: T5Attention,
         hidden_states: torch.Tensor,
+        mask: Optional[torch.Tensor] = None,
         key_value_states: Optional[torch.Tensor] = None,
+        position_bias: Optional[torch.Tensor] = None,
         past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        attention_mask: Optional[dict] = None,
         layer_head_mask: Optional[torch.Tensor] = None,
+        query_length: Optional[int] = None,
+        use_cache: bool = False,
         output_attentions: bool = False,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-        """Input shape: Batch x Time x Channel"""
-        assert layer_head_mask is None, "layer_head_mask is not supported for FlashAttention"
-        # if key_value_states are provided this layer is used as a cross-attention layer
-        # for the decoder
-        is_cross_attention = key_value_states is not None
-
-        bsz, tgt_len, _ = hidden_states.size()
-
-        # get query proj
-        query_states = self.q_proj(hidden_states)
-        # get key, value proj
-        if is_cross_attention and past_key_value is not None:
-            # reuse k,v, cross_attentions
-            key_states = past_key_value[0]
-            value_states = past_key_value[1]
-        elif is_cross_attention:
-            # cross_attentions
-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
-        elif past_key_value is not None:
-            # reuse k, v, self_attention
-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
-            key_states = torch.cat([past_key_value[0], key_states], dim=2)
-            value_states = torch.cat([past_key_value[1], value_states], dim=2)
-        else:
-            # self_attention
-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
-
-        if self.is_decoder:
-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
-            # Further calls to cross_attention layer can then reuse all cross-attention
-            # key/value_states (first "if" case)
-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
-            # all previous decoder key/value_states. Further calls to uni-directional self-attention
-            # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
-            # if encoder bi-directional self-attention `past_key_value` is always `None`
-            past_key_value = (key_states, value_states)
-
-        query_states = self._shape(query_states, tgt_len, bsz)
-
-        dropout_p = self.dropout if self.training else 0.0
-        attn_output = ColoAttention.attention(
-            query_states,
-            key_states,
-            value_states,
-            **attention_mask,
-            dropout_p=dropout_p,
-            scale=self.scaling,
-        )
-
-        attn_output = attn_output.transpose(1, 2)
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
+        """
+        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).
+        """
+        # Input is (batch_size, seq_length, dim)
+        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)
+        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)
+        batch_size, seq_length = hidden_states.shape[:2]
 
-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
-        # partitioned aross GPUs when using tensor-parallelism.
-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
+        real_seq_length = seq_length
 
-        attn_output = self.out_proj(attn_output)
+        if past_key_value is not None:
+            if len(past_key_value) != 2:
+                raise ValueError(
+                    f"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states"
+                )
+            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length
 
-        return attn_output, None, past_key_value
-
-    return forward
+        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]
 
+        def shape(states):
+            """projection"""
+            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)
+
+        def unshape(states):
+            """reshape"""
+            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)
+
+        def project(hidden_states, proj_layer, key_value_states, past_key_value):
+            """projects hidden states correctly to key/query states"""
+            if key_value_states is None:
+                # self-attn
+                # (batch_size, n_heads, seq_length, dim_per_head)
+                hidden_states = shape(proj_layer(hidden_states))
+            elif past_key_value is None:
+                # cross-attn
+                # (batch_size, n_heads, seq_length, dim_per_head)
+                hidden_states = shape(proj_layer(key_value_states))
+
+            if past_key_value is not None:
+                if key_value_states is None:
+                    # self-attn
+                    # (batch_size, n_heads, key_length, dim_per_head)
+                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)
+                elif past_key_value.shape[2] != key_value_states.shape[1]:
+                    # checking that the `sequence_length` of the `past_key_value` is the same as
+                    # the provided `key_value_states` to support prefix tuning
+                    # cross-attn
+                    # (batch_size, n_heads, seq_length, dim_per_head)
+                    hidden_states = shape(proj_layer(key_value_states))
+                else:
+                    # cross-attn
+                    hidden_states = past_key_value
+            return hidden_states
 
-def get_opt_decoder_forward_for_flash_attention(shard_config: ShardConfig):
-    from transformers.models.opt.modeling_opt import OPTDecoder
+        # get query states
+        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
 
-    def forward(
-        self: OPTDecoder,
-        input_ids: torch.LongTensor = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-    ) -> Union[Tuple, BaseModelOutputWithPast]:
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        # get key/value states
+        key_states = project(
+            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None
         )
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        # retrieve input_ids and inputs_embeds
-        if input_ids is not None and inputs_embeds is not None:
-            raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
-        elif input_ids is not None:
-            input_shape = input_ids.size()
-            input_ids = input_ids.view(-1, input_shape[-1])
-        elif inputs_embeds is not None:
-            input_shape = inputs_embeds.size()[:-1]
-        else:
-            raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
-
-        if inputs_embeds is None:
-            inputs_embeds = self.embed_tokens(input_ids)
-
-        batch_size, seq_length = input_shape
-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
-        # required mask seq length can be calculated via length of past
-        mask_seq_length = past_key_values_length + seq_length
-
-        # embed positions
-        if attention_mask is None:
-            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
-        elif attention_mask.shape[1] != mask_seq_length:
-            raise ValueError(
-                f"The provided attention mask has length {attention_mask.shape[1]}, but its length should be "
-                f"{mask_seq_length} (sum of the lengths of current and past inputs)"
-            )
-        causal_attention_mask = _get_attention_mask(
-            self, shard_config, inputs_embeds, past_key_values_length, attention_mask
+        value_states = project(
+            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None
         )
-        pos_embeds = self.embed_positions(attention_mask, past_key_values_length)
 
-        if self.project_in is not None:
-            inputs_embeds = self.project_in(inputs_embeds)
-
-        hidden_states = inputs_embeds + pos_embeds
-
-        if self.gradient_checkpointing and self.training:
-            if use_cache:
-                logger.warning_once(
-                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+        if position_bias is None:
+            if not self.has_relative_attention_bias:
+                position_bias = torch.zeros(
+                    (1, self.n_heads, real_seq_length, key_length), device=query_states.device, dtype=query_states.dtype
                 )
-                use_cache = False
-
-        # decoder layers
-        all_hidden_states = () if output_hidden_states else None
-        all_self_attns = () if output_attentions else None
-        next_decoder_cache = () if use_cache else None
-
-        # check if head_mask has a correct number of layers specified if desired
-        for attn_mask, mask_name in zip([head_mask], ["head_mask"]):
-            if attn_mask is not None:
-                if attn_mask.size()[0] != (len(self.layers)):
-                    raise ValueError(
-                        f"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for"
-                        f" {head_mask.size()[0]}."
-                    )
-
-        for idx, decoder_layer in enumerate(self.layers):
-            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
-            if output_hidden_states:
-                all_hidden_states += (hidden_states,)
-
-            if self.training:
-                dropout_probability = torch.rand([])
-                if dropout_probability < self.layerdrop:
-                    continue
-
-            past_key_value = past_key_values[idx] if past_key_values is not None else None
-
-            if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, output_attentions, None)
+                if self.gradient_checkpointing and self.training:
+                    position_bias.requires_grad = True
+            else:
+                position_bias = self.compute_bias(real_seq_length, key_length, device=query_states.device)
 
-                    return custom_forward
+            # if key and values are already calculated
+            # we want only the last query position bias
+            if past_key_value is not None:
+                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]
+
+            if mask is not None:
+                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)
+
+        if self.pruned_heads:
+            mask = torch.ones(position_bias.shape[1])
+            mask[list(self.pruned_heads)] = 0
+            position_bias_masked = position_bias[:, mask.bool()]
+        else:
+            position_bias_masked = position_bias
 
-                layer_outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(decoder_layer),
-                    hidden_states,
-                    causal_attention_mask,
-                    head_mask[idx] if head_mask is not None else None,
-                    None,
-                )
-            else:
-                layer_outputs = decoder_layer(
-                    hidden_states,
-                    attention_mask=causal_attention_mask,
-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
-                    past_key_value=past_key_value,
-                    output_attentions=output_attentions,
-                    use_cache=use_cache,
-                )
+        with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True):
+            attn_output = torch.nn.functional.scaled_dot_product_attention(
+                query_states,
+                key_states,
+                value_states,
+                attn_mask=position_bias_masked,
+                dropout_p=self.dropout,
+                scale=1.0,
+            )
+        attn_output = unshape(attn_output)
+        attn_output = self.o(attn_output)
 
-            hidden_states = layer_outputs[0]
+        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None
 
-            if use_cache:
-                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
+        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)
 
-            if output_attentions:
-                all_self_attns += (layer_outputs[1],)
+        return outputs
 
-        if self.final_layer_norm is not None:
-            hidden_states = self.final_layer_norm(hidden_states)
+    return forward
 
-        if self.project_out is not None:
-            hidden_states = self.project_out(hidden_states)
 
-        # add hidden states from the last decoder layer
-        if output_hidden_states:
-            all_hidden_states += (hidden_states,)
+def get_jit_fused_T5_layer_ff_forward():
+    from transformers.models.t5.modeling_t5 import T5LayerFF
 
-        next_cache = next_decoder_cache if use_cache else None
-        if not return_dict:
-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
-        return BaseModelOutputWithPast(
-            last_hidden_state=hidden_states,
-            past_key_values=next_cache,
-            hidden_states=all_hidden_states,
-            attentions=all_self_attns,
-        )
+    def forward(self: T5LayerFF, hidden_states: torch.Tensor) -> torch.Tensor:
+        forwarded_states = self.layer_norm(hidden_states)
+        forwarded_states = self.DenseReluDense(forwarded_states)
+        hidden_states = self.dropout_add(forwarded_states, hidden_states, self.dropout.p, self.dropout.training)
+        return hidden_states
 
     return forward
 
 
-def get_jit_fused_opt_decoder_layer_forward():
-    from transformers.models.opt.modeling_opt import OPTDecoderLayer
+def get_T5_layer_self_attention_forward():
+    from transformers.models.t5.modeling_t5 import T5LayerSelfAttention
 
     def forward(
-        self: OPTDecoderLayer,
+        self: T5LayerSelfAttention,
         hidden_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
+        position_bias: Optional[torch.Tensor] = None,
         layer_head_mask: Optional[torch.Tensor] = None,
         past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        output_attentions: Optional[bool] = False,
-        use_cache: Optional[bool] = False,
-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
-        """
-        Args:
-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
-            layer_head_mask (`torch.FloatTensor`, *optional*): mask for attention heads in a given layer of size
-                `(encoder_attention_heads,)`.
-            output_attentions (`bool`, *optional*):
-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
-                returned tensors for more detail.
-            use_cache (`bool`, *optional*):
-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
-                (see `past_key_values`).
-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
-        """
-
-        residual = hidden_states
-
-        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
-        if self.do_layer_norm_before:
-            hidden_states = self.self_attn_layer_norm(hidden_states)
-
-        # Self Attention
-        hidden_states, self_attn_weights, present_key_value = self.self_attn(
-            hidden_states=hidden_states,
-            past_key_value=past_key_value,
-            attention_mask=attention_mask,
+        use_cache: bool = False,
+        output_attentions: bool = False,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
+        normed_hidden_states = self.layer_norm(hidden_states)
+        attention_output = self.SelfAttention(
+            normed_hidden_states,
+            mask=attention_mask,
+            position_bias=position_bias,
             layer_head_mask=layer_head_mask,
+            past_key_value=past_key_value,
+            use_cache=use_cache,
             output_attentions=output_attentions,
         )
-
-        hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training)
-
-        # 350m applies layer norm AFTER attention
-        if not self.do_layer_norm_before:
-            hidden_states = self.self_attn_layer_norm(hidden_states)
-
-        # Fully Connected
-        hidden_states_shape = hidden_states.shape
-        hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))
-        residual = hidden_states
-
-        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
-        if self.do_layer_norm_before:
-            hidden_states = self.final_layer_norm(hidden_states)
-
-        hidden_states = self.fc1(hidden_states)
-        hidden_states = self.activation_fn(hidden_states)
-
-        hidden_states = self.fc2(hidden_states)
-
-        hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training).view(hidden_states_shape)
-
-        # 350m applies layer norm AFTER attention
-        if not self.do_layer_norm_before:
-            hidden_states = self.final_layer_norm(hidden_states)
-
-        outputs = (hidden_states,)
-
-        if output_attentions:
-            outputs += (self_attn_weights,)
-
-        if use_cache:
-            outputs += (present_key_value,)
-
+        hidden_states = self.dropout_add(attention_output[0], hidden_states, self.dropout.p, self.dropout.training)
+        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them
         return outputs
 
     return forward
 
 
-def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
+def get_T5_layer_cross_attention_forward():
+    from transformers.models.t5.modeling_t5 import T5LayerCrossAttention
+
     def forward(
-        self: OPTForCausalLM,
-        input_ids: torch.LongTensor = None,
+        self: T5LayerCrossAttention,
+        hidden_states: torch.Tensor,
+        key_value_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        labels: Optional[torch.LongTensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-    ) -> Union[Tuple, CausalLMOutputWithPast]:
-        r"""
-        Args:
-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
-                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
-                provide it.
-
-                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
-                [`PreTrainedTokenizer.__call__`] for details.
-
-                [What are input IDs?](../glossary#input-ids)
-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
-
-                - 1 for tokens that are **not masked**,
-                - 0 for tokens that are **masked**.
-
-                [What are attention masks?](../glossary#attention-mask)
-            head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):
-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:
-
-                - 1 indicates the head is **not masked**,
-                - 0 indicates the head is **masked**.
-
-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional
-                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.
-
-                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
-                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
-
-                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
-                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
-                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors
-                than the model's internal embedding lookup matrix.
-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
-            use_cache (`bool`, *optional*):
-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
-                (see `past_key_values`).
-            output_attentions (`bool`, *optional*):
-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
-                returned tensors for more detail.
-            output_hidden_states (`bool`, *optional*):
-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
-                for more detail.
-            return_dict (`bool`, *optional*):
-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
-
-        Returns:
-
-        Example:
-
-        ```python
-        >>> from transformers import AutoTokenizer, OPTForCausalLM
-
-        >>> model = OPTForCausalLM.from_pretrained("facebook/opt-350m")
-        >>> tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
-
-        >>> prompt = "Hey, are you conscious? Can you talk to me?"
-        >>> inputs = tokenizer(prompt, return_tensors="pt")
-
-        >>> # Generate
-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
-        "Hey, are you conscious? Can you talk to me?\nI'm not conscious. I'm just a little bit of a weirdo."
-        ```"""
-
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        outputs = self.model.decoder(
-            input_ids=input_ids,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
+        position_bias: Optional[torch.Tensor] = None,
+        layer_head_mask: Optional[torch.Tensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        use_cache: bool = False,
+        query_length: Optional[int] = None,
+        output_attentions: bool = False,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
+        normed_hidden_states = self.layer_norm(hidden_states)
+        attention_output = self.EncDecAttention(
+            normed_hidden_states,
+            mask=attention_mask,
+            key_value_states=key_value_states,
+            position_bias=position_bias,
+            layer_head_mask=layer_head_mask,
+            past_key_value=past_key_value,
             use_cache=use_cache,
+            query_length=query_length,
             output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-        )
-
-        logits = self.lm_head(outputs[0]).contiguous()
-
-        loss = None
-        if labels is not None:
-            # move labels to correct device to enable model parallelism
-            labels = labels.to(logits.device)
-            # Shift so that tokens < n predict n
-            shift_logits = logits[..., :-1, :].contiguous()
-            shift_labels = labels[..., 1:].contiguous()
-            shift_labels = shift_labels.view(-1)
-            # Enable model parallelism
-            shift_labels = shift_labels.to(shift_logits.device)
-            new_vocab_size = logits.shape[-1]
-            shift_logits = shift_logits.view(-1, new_vocab_size)
-            loss = cross_entropy_1d(
-                shift_logits,
-                shift_labels,
-                process_group=shard_config.tensor_parallel_process_group,
-                vocab_size=self.lm_head.out_features,
-                dtype=self.model.decoder.dtype,
-            )
-
-        if not return_dict:
-            output = (logits,) + outputs[1:]
-            return (loss,) + output if loss is not None else output
-
-        return CausalLMOutputWithPast(
-            loss=loss,
-            logits=logits,
-            past_key_values=outputs.past_key_values,
-            hidden_states=outputs.hidden_states,
-            attentions=outputs.attentions,
         )
+        layer_output = self.dropout_add(attention_output[0], hidden_states, self.dropout.p, self.dropout.training)
+        outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them
+        return outputs
 
     return forward
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/sam.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/t5.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/whisper.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,785 +1,1169 @@
-import warnings
-from typing import Dict, List, Optional, Tuple, Union
+import logging
+import random
+from typing import List, Optional, Tuple, Union
 
 import torch
+from torch import nn
 from torch.nn import CrossEntropyLoss
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask,
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
 from transformers.modeling_outputs import (
     BaseModelOutput,
     BaseModelOutputWithPastAndCrossAttentions,
     Seq2SeqLMOutput,
     Seq2SeqModelOutput,
+    SequenceClassifierOutput,
+)
+from transformers.models.whisper.modeling_whisper import (
+    WhisperDecoder,
+    WhisperEncoder,
+    WhisperForAudioClassification,
+    WhisperForConditionalGeneration,
+    WhisperModel,
+    shift_tokens_right,
 )
-from transformers.models.t5.modeling_t5 import T5EncoderModel, T5ForConditionalGeneration, T5Model, T5Stack
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer.layer import ColoAttention
+from colossalai.shardformer.shard import ShardConfig
+
+logger = logging.get_logger(__name__)
+
+
+def _get_attention_mask(
+    self: WhisperDecoder,
+    shard_config: ShardConfig,
+    hidden_states: torch.Tensor,
+    past_key_values_length: int,
+    attention_mask: Optional[torch.FloatTensor],
+    head_mask: Optional[torch.Tensor] = None,
+    output_attentions: bool = False,
+):
+    batch_size, seq_length = hidden_states.shape[:2]
+    mask_seq_length = past_key_values_length + seq_length
+    if shard_config.enable_flash_attention:
+        attention_mask = ColoAttention.prepare_attn_kwargs(
+            (batch_size, 1, seq_length, mask_seq_length),
+            hidden_states.dtype,
+            hidden_states.device,
+            attention_mask,
+            is_causal=True,
+        )
+    else:
+        input_shape = (batch_size, seq_length)
+        if self._use_flash_attention_2:
+            # 2d mask is passed through the layers
+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+        elif self._use_sdpa and head_mask is None and not output_attentions:
+            # output_attentions=True & head_mask can not be supported when using SDPA.
+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+                attention_mask, input_shape, hidden_states, past_key_values_length
+            )
+        else:
+            # 4d mask is passed through the layers
+            attention_mask = _prepare_4d_causal_attention_mask(
+                attention_mask, input_shape, hidden_states, past_key_values_length
+            )
+    return attention_mask
+
+
+def get_whisper_flash_attention_forward():
+    from transformers.models.whisper.modeling_whisper import WhisperAttention
+
+    def forward(
+        self: WhisperAttention,
+        hidden_states: torch.Tensor,
+        key_value_states: Optional[torch.Tensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        attention_mask: Optional[dict] = None,
+        layer_head_mask: Optional[torch.Tensor] = None,
+        output_attentions: bool = False,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+        """Input shape: Batch x Time x Channel"""
+        assert layer_head_mask is None, "layer_head_mask is not supported for FlashAttention"
+        # for encoder, attention_mask is None
+        if attention_mask is None:
+            attention_mask = {}
+        # if key_value_states are provided this layer is used as a cross-attention layer
+        # for the decoder
+        is_cross_attention = key_value_states is not None
+
+        bsz, tgt_len, _ = hidden_states.size()
+
+        # get query proj
+        query_states = self.q_proj(hidden_states)
+        # get key, value proj
+        # `past_key_value[0].shape[2] == key_value_states.shape[1]`
+        # is checking that the `sequence_length` of the `past_key_value` is the same as
+        # the provided `key_value_states` to support prefix tuning
+        if (
+            is_cross_attention
+            and past_key_value is not None
+            and past_key_value[0].shape[2] == key_value_states.shape[1]
+        ):
+            # reuse k,v, cross_attentions
+            key_states = past_key_value[0]
+            value_states = past_key_value[1]
+        elif is_cross_attention:
+            # cross_attentions
+            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
+            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
+        elif past_key_value is not None:
+            # reuse k, v, self_attention
+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
+            key_states = torch.cat([past_key_value[0], key_states], dim=2)
+            value_states = torch.cat([past_key_value[1], value_states], dim=2)
+        else:
+            # self_attention
+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
+
+        if self.is_decoder:
+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
+            # Further calls to cross_attention layer can then reuse all cross-attention
+            # key/value_states (first "if" case)
+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
+            # all previous decoder key/value_states. Further calls to uni-directional self-attention
+            # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
+            # if encoder bi-directional self-attention `past_key_value` is always `None`
+            past_key_value = (key_states, value_states)
+
+        query_states = self._shape(query_states, tgt_len, bsz)
+
+        dropout_p = self.dropout if self.training else 0.0
+        attn_output = ColoAttention.attention(
+            query_states,
+            key_states,
+            value_states,
+            **attention_mask,
+            dropout_p=dropout_p,
+            scale=self.scaling,
+        )
+        attn_output = attn_output.transpose(1, 2)
+
+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
+        # partitioned across GPUs when using tensor-parallelism.
+        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
+
+        attn_output = self.out_proj(attn_output)
+
+        return attn_output, None, past_key_value
+
+    return forward
+
+
+def get_whisper_decoder_forward_for_flash_attention(shard_config: ShardConfig):
+    def forward(
+        self: WhisperDecoder,
+        input_ids=None,
+        attention_mask=None,
+        encoder_hidden_states=None,
+        head_mask=None,
+        cross_attn_head_mask=None,
+        past_key_values=None,
+        inputs_embeds=None,
+        use_cache=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+    ):
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # retrieve input_ids and inputs_embeds
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+        else:
+            raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
+
+        # past_key_values_length
+        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
+
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+
+        attention_mask = _get_attention_mask(self, shard_config, inputs_embeds, past_key_values_length, attention_mask)
+
+        # embed positions
+        if input_ids is not None:
+            positions = self.embed_positions(input_ids, past_key_values_length=past_key_values_length)
+        else:
+            positions = self.embed_positions(inputs_embeds, past_key_values_length=past_key_values_length)
+
+        hidden_states = inputs_embeds + positions
+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
+
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`..."
+                )
+                use_cache = False
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None
+        next_decoder_cache = () if use_cache else None
+
+        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired
+        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], ["head_mask", "cross_attn_head_mask"]):
+            if attn_mask is not None:
+                assert attn_mask.size()[0] == (len(self.layers)), (
+                    f"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for"
+                    f" {head_mask.size()[0]}."
+                )
+        for idx, decoder_layer in enumerate(self.layers):
+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
+            if self.training:
+                dropout_probability = torch.rand([])
+                if dropout_probability < self.layerdrop:
+                    continue
+
+            past_key_value = past_key_values[idx] if past_key_values is not None else None
+
+            if self.gradient_checkpointing and self.training:
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        # None for past_key_value
+                        return module(*inputs, output_attentions, use_cache)
 
+                    return custom_forward
 
-class T5PipelineForwards:
+                layer_outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(decoder_layer),
+                    hidden_states,
+                    attention_mask,
+                    encoder_hidden_states,
+                    None,  # encoder attention mask
+                    head_mask[idx] if head_mask is not None else None,
+                    (cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),
+                    None,  # past_key_value
+                )
+            else:
+                layer_outputs = decoder_layer(
+                    hidden_states,
+                    attention_mask=attention_mask,
+                    encoder_hidden_states=encoder_hidden_states,
+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                    cross_attn_layer_head_mask=(
+                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None
+                    ),
+                    past_key_value=past_key_value,
+                    output_attentions=output_attentions,
+                    use_cache=use_cache,
+                )
+            hidden_states = layer_outputs[0]
+
+            if use_cache:
+                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)
+
+            if output_attentions:
+                all_self_attns += (layer_outputs[1],)
+
+                if encoder_hidden_states is not None:
+                    all_cross_attentions += (layer_outputs[2],)
+
+        hidden_states = self.layer_norm(hidden_states)
+        # add hidden states from the last decoder layer
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
+
+        next_cache = next_decoder_cache if use_cache else None
+        if not return_dict:
+            return tuple(
+                v
+                for v in [
+                    hidden_states,
+                    next_cache,
+                    all_hidden_states,
+                    all_self_attns,
+                    all_cross_attentions,
+                ]
+                if v is not None
+            )
+        return BaseModelOutputWithPastAndCrossAttentions(
+            last_hidden_state=hidden_states,
+            past_key_values=next_cache,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attns,
+            cross_attentions=all_cross_attentions,
+        )
+
+    return forward
+
+
+def get_jit_fused_whisper_encoder_layer_forward():
+    from transformers.models.whisper.modeling_whisper import WhisperEncoderLayer
+
+    def forward(
+        self: WhisperEncoderLayer,
+        hidden_states: torch.Tensor,
+        attention_mask: torch.Tensor,
+        layer_head_mask: torch.Tensor,
+        output_attentions: bool = False,
+    ) -> torch.Tensor:
+        """
+        Args:
+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
+            attention_mask (`torch.FloatTensor`): attention mask of size
+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
+            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
+                `(encoder_attention_heads,)`.
+            output_attentions (`bool`, *optional*):
+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
+                returned tensors for more detail.
+        """
+        residual = hidden_states
+        hidden_states = self.self_attn_layer_norm(hidden_states)
+        hidden_states, attn_weights, _ = self.self_attn(
+            hidden_states=hidden_states,
+            attention_mask=attention_mask,
+            layer_head_mask=layer_head_mask,
+            output_attentions=output_attentions,
+        )
+        hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training)
+
+        residual = hidden_states
+        hidden_states = self.final_layer_norm(hidden_states)
+        hidden_states = self.activation_fn(self.fc1(hidden_states))
+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
+        hidden_states = self.fc2(hidden_states)
+        hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training)
+
+        if hidden_states.dtype == torch.float16 and (
+            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()
+        ):
+            clamp_value = torch.finfo(hidden_states.dtype).max - 1000
+            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)
+
+        outputs = (hidden_states,)
+
+        if output_attentions:
+            outputs += (attn_weights,)
+
+        return outputs
+
+    return forward
+
+
+def get_jit_fused_whisper_decoder_layer_forward():
+    from transformers.models.whisper.modeling_whisper import WhisperDecoderLayer
+
+    def forward(
+        self: WhisperDecoderLayer,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.Tensor] = None,
+        encoder_hidden_states: Optional[torch.Tensor] = None,
+        encoder_attention_mask: Optional[torch.Tensor] = None,
+        layer_head_mask: Optional[torch.Tensor] = None,
+        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        output_attentions: Optional[bool] = False,
+        use_cache: Optional[bool] = True,
+    ) -> torch.Tensor:
+        """
+        Args:
+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
+            attention_mask (`torch.FloatTensor`): attention mask of size
+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
+            encoder_hidden_states (`torch.FloatTensor`):
+                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`
+            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size
+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
+            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
+                `(encoder_attention_heads,)`.
+            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of
+                size `(decoder_attention_heads,)`.
+            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states
+            output_attentions (`bool`, *optional*):
+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
+                returned tensors for more detail.
+        """
+        residual = hidden_states
+        hidden_states = self.self_attn_layer_norm(hidden_states)
+
+        # Self Attention
+        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2
+        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None
+        # add present self-attn cache to positions 1,2 of present_key_value tuple
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            past_key_value=self_attn_past_key_value,
+            attention_mask=attention_mask,
+            layer_head_mask=layer_head_mask,
+            output_attentions=output_attentions,
+        )
+        hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training)
+
+        # Cross-Attention Block
+        cross_attn_present_key_value = None
+        cross_attn_weights = None
+        if encoder_hidden_states is not None:
+            residual = hidden_states
+            hidden_states = self.encoder_attn_layer_norm(hidden_states)
+
+            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple
+            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None
+            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
+                hidden_states=hidden_states,
+                key_value_states=encoder_hidden_states,
+                attention_mask=encoder_attention_mask,
+                layer_head_mask=cross_attn_layer_head_mask,
+                past_key_value=cross_attn_past_key_value,
+                output_attentions=output_attentions,
+            )
+            hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training)
+
+            # add cross-attn to positions 3,4 of present_key_value tuple
+            present_key_value = present_key_value + cross_attn_present_key_value
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = self.final_layer_norm(hidden_states)
+        hidden_states = self.activation_fn(self.fc1(hidden_states))
+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
+        hidden_states = self.fc2(hidden_states)
+        hidden_states = self.dropout_add(hidden_states, residual, self.dropout, self.training)
+
+        outputs = (hidden_states,)
+
+        if output_attentions:
+            outputs += (self_attn_weights, cross_attn_weights)
+
+        if use_cache:
+            outputs += (present_key_value,)
+
+        return outputs
+
+    return forward
+
+
+class WhisperPipelineForwards:
     """
-    This class serves as a micro library for forward function substitution of
-    T5 models under pipeline setting.
+    This class serves as a micro library for forward function substitution of Llama models
+    under pipeline setting.
     """
 
     @staticmethod
-    def t5_stack_forward(
-        self: T5Stack,
-        input_ids: Optional[torch.LongTensor] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
-        encoder_hidden_states: Optional[torch.Tensor] = None,
-        encoder_attention_mask: Optional[torch.FloatTensor] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        head_mask: Optional[torch.FloatTensor] = None,
-        cross_attn_head_mask: Optional[torch.Tensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
-        use_cache: Optional[bool] = False,
-        output_attentions: Optional[bool] = False,
-        output_hidden_states: Optional[bool] = False,
-        return_dict: Optional[bool] = None,
+    def whisper_encoder_forward(
+        self: WhisperEncoder,
+        input_features,
+        attention_mask=None,
+        head_mask=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
-        position_bias: Optional[torch.Tensor] = None,
-        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
+        encoder_states=None,
+        all_attentions=None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
-    ) -> Union[Dict, Tuple, BaseModelOutputWithPastAndCrossAttentions]:
-        # This function is modified on the basis of transformers.models.t5.modeling_t5.T5Stack.forward.
-        # Please refer to original code of transformers for more details.
+        shard_config: Optional[ShardConfig] = None,
+    ):
+        r"""
+        Args:
+            input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):
+                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be
+                obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a
+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into
+                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding
+                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]
+            attention_mask (`torch.Tensor`)`, *optional*):
+                Whisper does not support masking of the `input_features`, this argument is preserved for compatibility,
+                but it is not used. By default the silence in the input log mel spectrogram are ignored.
+            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
+                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:
+
+                - 1 indicates the head is **not masked**,
+                - 0 indicates the head is **masked**.
+            output_attentions (`bool`, *optional*):
+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
+                returned tensors for more detail.
+            output_hidden_states (`bool`, *optional*):
+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
+                for more detail.
+            return_dict (`bool`, *optional*):
+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
+        """
+        logging.get_logger(__name__)
 
-        logger = logging.get_logger(__name__)
+        stage = stage_manager.stage
+        at_first_stage = stage == 0
+        at_last_stage = stage == decoder_starting_stage - 1
 
-        # TODO(baizhou): left the recording kv-value tensors as () or None type, this feature may be added in the future.
-        if past_key_values:
-            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
-            past_key_values = None
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-        if use_cache:
-            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
-            use_cache = False
-        if use_cache is True:
-            if not in_decoder:
-                raise ValueError(f"`use_cache` can only be set to `True` if {self} is used as a decoder")
-        if self.gradient_checkpointing and self.training:
-            if use_cache:
-                logger.warning_once(
-                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # Process inputs if at the first stage of encoder.
+        if at_first_stage:
+            inputs_embeds = nn.functional.gelu(self.conv1(input_features))
+            inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))
+
+            inputs_embeds = inputs_embeds.permute(0, 2, 1)
+            embed_pos = self.embed_positions.weight
+
+            hidden_states = inputs_embeds + embed_pos
+            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
+
+            encoder_states = () if output_hidden_states else None
+            all_attentions = () if output_attentions else None
+
+            # check if head_mask has a correct number of layers specified if desired
+            if head_mask is not None:
+                assert head_mask.size()[0] == (
+                    len(self.layers)
+                ), f"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}."
+
+        else:
+            if hidden_states is None:
+                raise ValueError(
+                    "hidden_states shouldn't be None for stages other than the first stage of encoder/decoder."
                 )
-                use_cache = False
 
+        start_idx, end_idx = stage_index[0], stage_index[1]
+
+        for idx in range(start_idx, end_idx):
+            encoder_layer = self.layers[idx]
+
+            if output_hidden_states:
+                encoder_states = encoder_states + (hidden_states,)
+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
+            dropout_probability = random.uniform(0, 1)
+            if self.training and (dropout_probability < self.layerdrop):  # skip the layer
+                layer_outputs = (None, None)
+            else:
+                if self.gradient_checkpointing and self.training:
+                    layer_outputs = self._gradient_checkpointing_func(
+                        encoder_layer.__call__,
+                        hidden_states,
+                        None,
+                        (head_mask[idx] if head_mask is not None else None),
+                        output_attentions,
+                    )
+                else:
+                    layer_outputs = encoder_layer(
+                        hidden_states,
+                        None,
+                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                        output_attentions=output_attentions,
+                    )
+
+                hidden_states = layer_outputs[0]
+
+            if output_attentions:
+                all_attentions = all_attentions + (layer_outputs[1],)
+
+        if at_last_stage:
+            hidden_states = self.layer_norm(hidden_states)
+            if output_hidden_states:
+                encoder_states = encoder_states + (hidden_states,)
+
+            if not return_dict:
+                return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)
+            return BaseModelOutput(
+                last_hidden_state=hidden_states,
+                hidden_states=encoder_states,
+                attentions=all_attentions,
+            )
+
+        else:
+            return {"hidden_states": hidden_states, "head_mask": head_mask}
+
+    @staticmethod
+    def whisper_decoder_forward(
+        self: WhisperDecoder,
+        input_ids=None,
+        attention_mask=None,
+        encoder_hidden_states=None,
+        head_mask=None,
+        cross_attn_head_mask=None,
+        past_key_values=None,
+        inputs_embeds=None,
+        use_cache=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+        stage_manager: Optional[PipelineStageManager] = None,
+        hidden_states: Optional[torch.FloatTensor] = None,
+        stage_index: Optional[List[int]] = None,
+        decoder_starting_stage: Optional[int] = None,
+        shard_config: Optional[ShardConfig] = None,
+    ):
+        r"""
+        Args:
+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
+                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
+                provide it.
+
+                Indices can be obtained using [`WhisperTokenizer`]. See [`PreTrainedTokenizer.encode`] and
+                [`PreTrainedTokenizer.__call__`] for details.
+
+                [What are input IDs?](../glossary#input-ids)
+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
+
+                - 1 for tokens that are **not masked**,
+                - 0 for tokens that are **masked**.
+
+                [What are attention masks?](../glossary#attention-mask)
+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):
+                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
+                of the decoder.
+            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
+                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:
+
+                - 1 indicates the head is **not masked**,
+                - 0 indicates the head is **masked**.
+
+            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
+                Mask to nullify selected heads of the attention modules in encoder to avoid performing cross-attention
+                on hidden heads. Mask values selected in `[0, 1]`:
+
+                - 1 indicates the head is **not masked**,
+                - 0 indicates the head is **masked**.
+
+            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
+                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
+                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
+                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
+
+                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
+                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
+
+                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
+                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
+                all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of
+                shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing
+                `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more
+                control over how to convert `input_ids` indices into associated vectors than the model's internal
+                embedding lookup matrix.
+            output_attentions (`bool`, *optional*):
+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
+                returned tensors for more detail.
+            output_hidden_states (`bool`, *optional*):
+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
+                for more detail.
+            return_dict (`bool`, *optional*):
+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
+        """
+        logger = logging.get_logger(__name__)
         stage = stage_manager.stage
-        in_decoder = self.is_decoder
-        if in_decoder != (stage >= decoder_starting_stage):
-            raise ValueError("Config in T5Stack is not aligned with pipeline setting.")
-
-        # at_first_stage: current stage is the first stage of encoder/decoder, taking input_ids/input_embeds
-        # at_last_stage: current stage is the last stage of encoder/decoder, making outputs the same form as huggingface
-        at_first_stage = (stage == 0) or (stage == decoder_starting_stage)
-        at_last_stage = (stage == decoder_starting_stage - 1) or (stage == stage_manager.num_stages - 1)
+        at_first_stage = stage == decoder_starting_stage
+        at_last_stage = stage == stage_manager.num_stages - 1
+
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None
+        next_decoder_cache = () if use_cache else None
+
+        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired
+        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], ["head_mask", "cross_attn_head_mask"]):
+            if attn_mask is not None:
+                assert attn_mask.size()[0] == (len(self.layers)), (
+                    f"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for"
+                    f" {head_mask.size()[0]}."
+                )
+
+        # past_key_values_length
+        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
 
-        # Process inputs if at the first stage of encoder/decoder.
         if at_first_stage:
+            # retrieve input_ids and inputs_embeds
             if input_ids is not None and inputs_embeds is not None:
-                err_msg_prefix = "decoder_" if in_decoder else ""
-                raise ValueError(
-                    f"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time"
-                )
+                raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
             elif input_ids is not None:
                 input_shape = input_ids.size()
                 input_ids = input_ids.view(-1, input_shape[-1])
             elif inputs_embeds is not None:
                 input_shape = inputs_embeds.size()[:-1]
             else:
-                err_msg_prefix = "decoder_" if in_decoder else ""
-                raise ValueError(
-                    f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds"
-                )
+                raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
+
             if inputs_embeds is None:
-                if self.embed_tokens is None:
-                    raise ValueError("You have to initialize the model with valid token embeddings")
                 inputs_embeds = self.embed_tokens(input_ids)
-            batch_size, seq_length = input_shape
-            device = inputs_embeds.device
-            hidden_states = self.dropout(inputs_embeds)
+
+            attention_mask = _get_attention_mask(
+                self, shard_config, inputs_embeds, past_key_values_length, attention_mask
+            )
+
+            # embed positions
+            if input_ids is not None:
+                positions = self.embed_positions(input_ids, past_key_values_length=past_key_values_length)
+            else:
+                positions = self.embed_positions(inputs_embeds, past_key_values_length=past_key_values_length)
+
+            hidden_states = inputs_embeds + positions
+            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
+
+            if self.gradient_checkpointing and self.training:
+                if use_cache:
+                    logger.warning_once(
+                        "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`..."
+                    )
+                    use_cache = False
+
         else:
             if hidden_states is None:
                 raise ValueError(
                     "hidden_states shouldn't be None for stages other than the first stage of encoder/decoder."
                 )
             input_shape = hidden_states.size()[:-1]
-            batch_size, seq_length = input_shape[0], input_shape[1]
-            device = hidden_states.device
-
-        # required mask seq length can be calculated via length of past
-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length
-
-        # initialize past_key_values with `None` if past does not exist
-        if past_key_values is None:
-            past_key_values = [None] * len(self.block)
-
-        if attention_mask is None:
-            attention_mask = torch.ones(batch_size, mask_seq_length, device=device)
-
-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
-        # ourselves in which case we just need to make it broadcastable to all heads.
-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)
-
-        # If a 2D or 3D attention mask is provided for the cross-attention
-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
-        if self.is_decoder and encoder_hidden_states is not None:
-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
-            if encoder_attention_mask is None:
-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device, dtype=torch.long)
-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)
-        else:
-            encoder_extended_attention_mask = None
-
-        # Prepare head mask if needed
-        head_mask = self.get_head_mask(head_mask, self.config.num_layers)
-        cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)
-        present_key_value_states = () if use_cache else None
-        all_hidden_states = () if output_hidden_states else None
-        all_attentions = () if output_attentions else None
-        all_cross_attentions = () if (output_attentions and self.is_decoder) else None
+            attention_mask = _get_attention_mask(
+                self,
+                shard_config,
+                hidden_states,
+                past_key_values_length,
+                attention_mask,
+            )
 
-        # Going through held blocks.
         start_idx, end_idx = stage_index[0], stage_index[1]
 
-        for i in range(start_idx, end_idx):
-            past_key_value = past_key_values[i]
-            layer_module = self.block[i]
-            layer_head_mask = head_mask[i]
-            cross_attn_layer_head_mask = cross_attn_head_mask[i]
-            torch.cuda.set_device(hidden_states.device)
+        for idx in range(start_idx, end_idx):
+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
+            decoder_layer = self.layers[idx]
+
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
+            dropout_probability = random.uniform(0, 1)
+            if self.training and (dropout_probability < self.layerdrop):
+                continue
+
+            past_key_value = past_key_values[idx] if past_key_values is not None else None
 
             if self.gradient_checkpointing and self.training:
                 layer_outputs = self._gradient_checkpointing_func(
-                    layer_module.forward,
+                    decoder_layer.__call__,
                     hidden_states,
-                    extended_attention_mask,
-                    position_bias,
+                    attention_mask,
                     encoder_hidden_states,
-                    encoder_extended_attention_mask,
-                    encoder_decoder_position_bias,
-                    layer_head_mask,
-                    cross_attn_layer_head_mask,
-                    None,  # past_key_value is always None with gradient checkpointing
-                    use_cache,
+                    None,  # encoder attention mask
+                    head_mask[idx] if head_mask is not None else None,
+                    (cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),
+                    None,  # past_key_value
                     output_attentions,
+                    use_cache,
                 )
             else:
-                layer_outputs = layer_module(
+                layer_outputs = decoder_layer(
                     hidden_states,
-                    attention_mask=extended_attention_mask,
-                    position_bias=position_bias,
+                    attention_mask=attention_mask,
                     encoder_hidden_states=encoder_hidden_states,
-                    encoder_attention_mask=encoder_extended_attention_mask,
-                    encoder_decoder_position_bias=encoder_decoder_position_bias,
-                    layer_head_mask=layer_head_mask,
-                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,
+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                    cross_attn_layer_head_mask=(
+                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None
+                    ),
                     past_key_value=past_key_value,
-                    use_cache=use_cache,
                     output_attentions=output_attentions,
+                    use_cache=use_cache,
                 )
+            hidden_states = layer_outputs[0]
 
-            # layer_outputs is a tuple with:
-            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)
-
-            if use_cache is False or use_cache is None:
-                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]
-            hidden_states, present_key_value_state = layer_outputs[:2]
-
-            # We share the position biases between the layers - the first layer store them
-            # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),
-            # (cross-attention position bias), (cross-attention weights)
-            position_bias = layer_outputs[2]
-
-            if in_decoder and encoder_hidden_states is not None:
-                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]
-            # append next layer key value states
             if use_cache:
-                present_key_value_states = present_key_value_states + (present_key_value_state,)
+                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)
 
-        # last layer
-        if at_last_stage:
-            hidden_states = self.final_layer_norm(hidden_states)
-            hidden_states = self.dropout(hidden_states)
+            if output_attentions:
+                all_self_attns += (layer_outputs[1],)
 
+                if encoder_hidden_states is not None:
+                    all_cross_attentions += (layer_outputs[2],)
+
+        if at_last_stage:
+            hidden_states = self.layer_norm(hidden_states)
+            # add hidden states from the last decoder layer
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
+            next_cache = next_decoder_cache if use_cache else None
             if not return_dict:
                 return tuple(
                     v
                     for v in [
                         hidden_states,
-                        present_key_value_states,
+                        next_cache,
                         all_hidden_states,
-                        all_attentions,
+                        all_self_attns,
                         all_cross_attentions,
                     ]
                     if v is not None
                 )
             return BaseModelOutputWithPastAndCrossAttentions(
                 last_hidden_state=hidden_states,
-                past_key_values=present_key_value_states,
+                past_key_values=next_cache,
                 hidden_states=all_hidden_states,
-                attentions=all_attentions,
+                attentions=all_self_attns,
                 cross_attentions=all_cross_attentions,
             )
+
         else:
             return {
+                "head_mask": head_mask,
+                "cross_attn_head_mask": cross_attn_head_mask,
                 "hidden_states": hidden_states,
-                "position_bias": position_bias,
-                "encoder_decoder_position_bias": encoder_decoder_position_bias,
-                "backward_tensor_keys": ["hidden_states"],
             }
 
     @staticmethod
-    def t5_model_forward(
-        self: T5Model,
-        input_ids: Optional[torch.LongTensor] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
+    def whisper_model_forward(
+        self: WhisperModel,
+        input_features: Optional[torch.FloatTensor] = None,
+        attention_mask: Optional[torch.LongTensor] = None,
         decoder_input_ids: Optional[torch.LongTensor] = None,
-        decoder_attention_mask: Optional[torch.BoolTensor] = None,
-        head_mask: Optional[torch.FloatTensor] = None,
-        decoder_head_mask: Optional[torch.FloatTensor] = None,
+        decoder_attention_mask: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
+        decoder_head_mask: Optional[torch.Tensor] = None,
         cross_attn_head_mask: Optional[torch.Tensor] = None,
         encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
         past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-        decoder_inputs_embeds: Optional[torch.Tensor] = None,
+        decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
-        position_bias: Optional[torch.Tensor] = None,
-        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
-        backward_tensor_keys: Optional[List[str]] = None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
-    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:
-        # This function is modified on the basis of transformers.models.t5.modeling_t5.T5Model.forward.
-        # Please refer to original code of transformers for more details.
-
-        __HEAD_MASK_WARNING_MSG = """
-        The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,
-        `decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.
-        If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,
-        num_heads)`.
-        """
-
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        logger = logging.get_logger(__name__)
+        shard_config: Optional[ShardConfig] = None,
+    ):
+        r"""
+        Returns:
 
-        # TODO(baizhou): left the recording kv-value tensors as () or None type, this feature may be added in the future.
+        Example:
+         ```python
+         >>> import torch
+         >>> from transformers import AutoFeatureExtractor, WhisperModel
+         >>> from datasets import load_dataset
+
+         >>> model = WhisperModel.from_pretrained("openai/whisper-base")
+         >>> feature_extractor = AutoFeatureExtractor.from_pretrained("openai/whisper-base")
+         >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
+         >>> inputs = feature_extractor(ds[0]["audio"]["array"], return_tensors="pt")
+         >>> input_features = inputs.input_features
+         >>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id
+         >>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state
+         >>> list(last_hidden_state.shape)
+         [1, 2, 512]
+         ```"""
+        # TODO: left the recording kv-value tensors as () or None type, this feature may be added in the future.
         if past_key_values:
             logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
             past_key_values = None
         if output_attentions:
             logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
             output_attentions = False
         if output_hidden_states:
             logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
             output_hidden_states = False
         if use_cache:
             logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
             use_cache = False
 
-        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask
-        if head_mask is not None and decoder_head_mask is None:
-            if self.config.num_layers == self.config.num_decoder_layers:
-                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)
-                decoder_head_mask = head_mask
+        logging.get_logger(__name__)
 
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
         in_decoder = stage_manager.stage >= decoder_starting_stage
-        # Stage is in encoder, directly return the output of t5_stack_forward
         if not in_decoder:
-            encoder_outputs = T5PipelineForwards.t5_stack_forward(
-                self.encoder,
-                input_ids=input_ids,
-                attention_mask=attention_mask,
-                inputs_embeds=inputs_embeds,
-                head_mask=head_mask,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-                return_dict=return_dict,
-                stage_manager=stage_manager,
-                hidden_states=hidden_states,
-                position_bias=position_bias,
-                encoder_decoder_position_bias=encoder_decoder_position_bias,
-                stage_index=stage_index,
-                decoder_starting_stage=decoder_starting_stage,
-            )
-            if stage_manager.stage == decoder_starting_stage - 1:
-                # last stage of encoder
-                return {"encoder_hidden_states": encoder_outputs[0]}
-            else:
-                return encoder_outputs
+            if encoder_outputs is None:
+                input_features = self._mask_input_features(input_features, attention_mask=attention_mask)
+
+                encoder_outputs = WhisperPipelineForwards.whisper_encoder_forward(
+                    self.encoder,
+                    input_features,
+                    head_mask=head_mask,
+                    output_attentions=output_attentions,
+                    output_hidden_states=output_hidden_states,
+                    return_dict=return_dict,
+                    stage_manager=stage_manager,
+                    hidden_states=hidden_states,
+                    stage_index=stage_index,
+                    decoder_starting_stage=decoder_starting_stage,
+                )
+
+                if stage_manager.stage == decoder_starting_stage - 1:
+                    # last stage of encoder
+                    return {"encoder_hidden_states": encoder_outputs[0]}
+                else:
+                    return encoder_outputs
+            # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True
+            elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):
+                encoder_outputs = BaseModelOutput(
+                    last_hidden_state=encoder_outputs[0],
+                    hidden_states=(encoder_outputs[1] if len(encoder_outputs) > 1 else None),
+                    attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,
+                )
 
         at_last_decoder_stage = stage_manager.is_last_stage()
         at_first_decoder_stage = stage_manager.stage == decoder_starting_stage
-
         if encoder_outputs is not None:
             encoder_hidden_states = encoder_outputs[0]
         elif encoder_hidden_states is None:
             raise ValueError("Non-empty encoder_hidden_states should be passed in at decoder stages.")
 
         if not at_first_decoder_stage and hidden_states is None:
             raise ValueError("If not at the first layer of decoder, non-empty hidden_states must be provided.")
 
-        # Decode
-        decoder_outputs = T5PipelineForwards.t5_stack_forward(
+        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)
+        decoder_outputs = WhisperPipelineForwards.whisper_decoder_forward(
             self.decoder,
             input_ids=decoder_input_ids,
             attention_mask=decoder_attention_mask,
-            inputs_embeds=decoder_inputs_embeds,
-            past_key_values=past_key_values,
             encoder_hidden_states=encoder_hidden_states,
-            encoder_attention_mask=attention_mask,
             head_mask=decoder_head_mask,
             cross_attn_head_mask=cross_attn_head_mask,
+            past_key_values=past_key_values,
+            inputs_embeds=decoder_inputs_embeds,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
-            position_bias=position_bias,
-            encoder_decoder_position_bias=encoder_decoder_position_bias,
             stage_index=stage_index,
             decoder_starting_stage=decoder_starting_stage,
+            shard_config=shard_config,
         )
 
-        # Directly return outputs of overloaded T5Stack forward if not at last stage.
+        # Directly return outputs of overloaded Whisper forward if not at last stage.
         if not at_last_decoder_stage:
             # encoder_hidden_states should be passed to the next stage
             decoder_outputs["encoder_hidden_states"] = encoder_hidden_states
             return decoder_outputs
 
         if not return_dict:
-            return decoder_outputs + encoder_hidden_states
-        else:
-            return Seq2SeqModelOutput(
-                last_hidden_state=decoder_outputs.last_hidden_state,
-                past_key_values=decoder_outputs.past_key_values,
-                decoder_hidden_states=decoder_outputs.hidden_states,
-                decoder_attentions=decoder_outputs.attentions,
-                cross_attentions=decoder_outputs.cross_attentions,
-                encoder_last_hidden_state=encoder_hidden_states,
-            )
+            return decoder_outputs + encoder_outputs
+
+        return Seq2SeqModelOutput(
+            last_hidden_state=decoder_outputs.last_hidden_state,
+            past_key_values=decoder_outputs.past_key_values,
+            decoder_hidden_states=decoder_outputs.hidden_states,
+            decoder_attentions=decoder_outputs.attentions,
+            cross_attentions=decoder_outputs.cross_attentions,
+            encoder_last_hidden_state=encoder_hidden_states,
+        )
 
     @staticmethod
-    def t5_for_conditional_generation_forward(
-        self: T5ForConditionalGeneration,
-        input_ids: Optional[torch.LongTensor] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
+    def whisper_for_conditional_generation_forward(
+        self: WhisperForConditionalGeneration,
+        input_features: Optional[torch.FloatTensor] = None,
+        attention_mask: Optional[torch.LongTensor] = None,
         decoder_input_ids: Optional[torch.LongTensor] = None,
-        decoder_attention_mask: Optional[torch.BoolTensor] = None,
-        head_mask: Optional[torch.FloatTensor] = None,
-        decoder_head_mask: Optional[torch.FloatTensor] = None,
+        decoder_attention_mask: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
+        decoder_head_mask: Optional[torch.Tensor] = None,
         cross_attn_head_mask: Optional[torch.Tensor] = None,
-        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
+        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,
         labels: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
-        position_bias: Optional[torch.Tensor] = None,
-        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
-        backward_tensor_keys: Optional[List[str]] = None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
-    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:
-        # This function is modified on the basis of transformers.models.t5.modeling_t5.T5ForConditionalGeneration.forward.
-        # Please refer to original code of transformers for more details.
-
-        __HEAD_MASK_WARNING_MSG = """
-        The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,
-        `decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.
-        If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,
-        num_heads)`.
-        """
+        shard_config: Optional[ShardConfig] = None,
+    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`
+            or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is
+            only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
 
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        Returns:
 
-        logger = logging.get_logger(__name__)
+        Example:
 
-        # TODO(baizhou): left the recording kv-value tensors as () or None type, this feature may be added in the future.
-        if past_key_values:
-            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
-            past_key_values = None
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-        if use_cache:
-            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
-            use_cache = False
+        ```python
+        >>> import torch
+        >>> from transformers import AutoProcessor, WhisperForConditionalGeneration
+        >>> from datasets import load_dataset
 
-        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask
-        if head_mask is not None and decoder_head_mask is None:
-            if self.config.num_layers == self.config.num_decoder_layers:
-                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)
-                decoder_head_mask = head_mask
+        >>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
+        >>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")
 
-        in_decoder = stage_manager.stage >= decoder_starting_stage
+        >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
 
-        # Stage is in encoder, directly return the output of t5_stack_forward
-        if not in_decoder:
-            encoder_outputs = T5PipelineForwards.t5_stack_forward(
-                self.encoder,
-                input_ids=input_ids,
-                attention_mask=attention_mask,
-                inputs_embeds=inputs_embeds,
-                head_mask=head_mask,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-                return_dict=return_dict,
-                stage_manager=stage_manager,
-                hidden_states=hidden_states,
-                position_bias=position_bias,
-                encoder_decoder_position_bias=encoder_decoder_position_bias,
-                stage_index=stage_index,
-                decoder_starting_stage=decoder_starting_stage,
-            )
-            if stage_manager.stage == decoder_starting_stage - 1:
-                # last stage of encoder
-                return {"encoder_hidden_states": encoder_outputs[0]}
-            else:
-                return encoder_outputs
+        >>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")
+        >>> input_features = inputs.input_features
 
-        at_last_decoder_stage = stage_manager.is_last_stage()
-        at_first_decoder_stage = stage_manager.stage == decoder_starting_stage
-
-        if encoder_outputs is not None:
-            encoder_hidden_states = encoder_outputs[0]
-        elif encoder_hidden_states is None:
-            raise ValueError("Non-empty encoder_hidden_states should be passed in at decoder stages.")
-
-        if not at_first_decoder_stage and hidden_states is None:
-            raise ValueError("If not at the first layer of decoder, non-empty hidden_states must be provided.")
+        >>> generated_ids = model.generate(inputs=input_features)
 
-        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
-            # get decoder inputs from shifting lm labels to the right
-            decoder_input_ids = self._shift_right(labels)
+        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
+        >>> transcription
+        ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'
+        ```"""
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        # Decode
-        decoder_outputs = T5PipelineForwards.t5_stack_forward(
-            self.decoder,
-            input_ids=decoder_input_ids,
-            attention_mask=decoder_attention_mask,
-            inputs_embeds=decoder_inputs_embeds,
-            past_key_values=past_key_values,
-            encoder_hidden_states=encoder_hidden_states,
-            encoder_attention_mask=attention_mask,
-            head_mask=decoder_head_mask,
+        if labels is not None:
+            if decoder_input_ids is None and decoder_inputs_embeds is None:
+                decoder_input_ids = shift_tokens_right(
+                    labels, self.config.pad_token_id, self.config.decoder_start_token_id
+                )
+        in_decoder = stage_manager.stage >= decoder_starting_stage
+        at_last_decoder_stage = stage_manager.is_last_stage()
+        outputs = WhisperPipelineForwards.whisper_model_forward(
+            self.model,
+            input_features,
+            attention_mask=attention_mask,
+            decoder_input_ids=decoder_input_ids,
+            encoder_outputs=encoder_outputs,
+            decoder_attention_mask=decoder_attention_mask,
+            head_mask=head_mask,
+            decoder_head_mask=decoder_head_mask,
             cross_attn_head_mask=cross_attn_head_mask,
+            past_key_values=past_key_values,
+            decoder_inputs_embeds=decoder_inputs_embeds,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
-            position_bias=position_bias,
-            encoder_decoder_position_bias=encoder_decoder_position_bias,
+            encoder_hidden_states=encoder_hidden_states,
             stage_index=stage_index,
             decoder_starting_stage=decoder_starting_stage,
+            shard_config=shard_config,
         )
+        if not in_decoder:
+            return outputs
 
-        # Directly return outputs of overloaded T5Stack forward if not at last stage.
         if not at_last_decoder_stage:
             # encoder_hidden_states should be passed to the next stage
-            decoder_outputs["encoder_hidden_states"] = encoder_hidden_states
-            return decoder_outputs
-
-        sequence_output = decoder_outputs[0]
-
-        if self.config.tie_word_embeddings:
-            # Rescale output before projecting on vocab
-            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586
-            sequence_output = sequence_output * (self.model_dim**-0.5)
+            outputs["encoder_hidden_states"] = encoder_hidden_states
+            return outputs
 
-        lm_logits = self.lm_head(sequence_output)
+        lm_logits = self.proj_out(outputs[0])
 
         loss = None
         if labels is not None:
-            loss_fct = CrossEntropyLoss(ignore_index=-100)
+            loss_fct = CrossEntropyLoss()
             # move labels to correct device to enable PP
             labels = labels.to(lm_logits.device)
-            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))
+            loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.reshape(-1))
 
         if not return_dict:
-            output = (lm_logits,) + decoder_outputs[1:] + encoder_hidden_states
+            output = (lm_logits,) + outputs[1:]
             return ((loss,) + output) if loss is not None else output
 
         return Seq2SeqLMOutput(
             loss=loss,
             logits=lm_logits,
-            past_key_values=decoder_outputs.past_key_values,
-            decoder_hidden_states=decoder_outputs.hidden_states,
-            decoder_attentions=decoder_outputs.attentions,
-            cross_attentions=decoder_outputs.cross_attentions,
-            encoder_last_hidden_state=encoder_hidden_states,
+            past_key_values=outputs.past_key_values,
+            decoder_hidden_states=outputs.decoder_hidden_states,
+            decoder_attentions=outputs.decoder_attentions,
+            cross_attentions=outputs.cross_attentions,
+            encoder_last_hidden_state=outputs.encoder_last_hidden_state,
+            encoder_hidden_states=outputs.encoder_hidden_states,
+            encoder_attentions=outputs.encoder_attentions,
         )
 
     @staticmethod
-    def t5_encoder_model_forward(
-        self: T5EncoderModel,
-        input_ids: Optional[torch.LongTensor] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
-        head_mask: Optional[torch.FloatTensor] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
+    def whisper_for_audio_classification_forward(
+        self: WhisperForAudioClassification,
+        input_features: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
+        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        labels: Optional[torch.LongTensor] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
-        position_bias: Optional[torch.Tensor] = None,
-        encoder_decoder_position_bias: Optional[torch.Tensor] = None,
-        backward_tensor_keys: Optional[List[str]] = None,
+        encoder_states=None,
+        all_attentions=None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
-    ) -> Union[Tuple[torch.FloatTensor], BaseModelOutput]:
+        shard_config: Optional[ShardConfig] = None,
+    ):
         r"""
-        This function is modified on the basis of transformers.models.t5.modeling_gpt2.T5EncoderModel.forward.
+        This function is modified on the basis of transformers.models.whisper.modeling_whisper.WhisperForAudioClassification.forward.
         Please refer to original code of transformers for more details.
-        ```"""
+        """
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        outputs = T5PipelineForwards.t5_stack_forward(
+        # audio_classification only holds encoder
+        encoder_outputs = WhisperPipelineForwards.whisper_encoder_forward(
             self.encoder,
-            input_ids=input_ids,
-            attention_mask=attention_mask,
-            inputs_embeds=inputs_embeds,
+            input_features,
             head_mask=head_mask,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
-            position_bias=position_bias,
-            encoder_decoder_position_bias=encoder_decoder_position_bias,
             stage_index=stage_index,
             decoder_starting_stage=decoder_starting_stage,
         )
 
-        return outputs
-
-
-def get_t5_flash_attention_forward():
-    from transformers.models.t5.modeling_t5 import T5Attention
-
-    def forward(
-        self: T5Attention,
-        hidden_states: torch.Tensor,
-        mask: Optional[torch.Tensor] = None,
-        key_value_states: Optional[torch.Tensor] = None,
-        position_bias: Optional[torch.Tensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        layer_head_mask: Optional[torch.Tensor] = None,
-        query_length: Optional[int] = None,
-        use_cache: bool = False,
-        output_attentions: bool = False,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
-        """
-        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).
-        """
-        # Input is (batch_size, seq_length, dim)
-        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)
-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)
-        batch_size, seq_length = hidden_states.shape[:2]
-
-        real_seq_length = seq_length
-
-        if past_key_value is not None:
-            if len(past_key_value) != 2:
-                raise ValueError(
-                    f"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states"
-                )
-            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length
-
-        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]
-
-        def shape(states):
-            """projection"""
-            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)
-
-        def unshape(states):
-            """reshape"""
-            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)
-
-        def project(hidden_states, proj_layer, key_value_states, past_key_value):
-            """projects hidden states correctly to key/query states"""
-            if key_value_states is None:
-                # self-attn
-                # (batch_size, n_heads, seq_length, dim_per_head)
-                hidden_states = shape(proj_layer(hidden_states))
-            elif past_key_value is None:
-                # cross-attn
-                # (batch_size, n_heads, seq_length, dim_per_head)
-                hidden_states = shape(proj_layer(key_value_states))
-
-            if past_key_value is not None:
-                if key_value_states is None:
-                    # self-attn
-                    # (batch_size, n_heads, key_length, dim_per_head)
-                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)
-                elif past_key_value.shape[2] != key_value_states.shape[1]:
-                    # checking that the `sequence_length` of the `past_key_value` is the same as
-                    # the provided `key_value_states` to support prefix tuning
-                    # cross-attn
-                    # (batch_size, n_heads, seq_length, dim_per_head)
-                    hidden_states = shape(proj_layer(key_value_states))
-                else:
-                    # cross-attn
-                    hidden_states = past_key_value
-            return hidden_states
-
-        # get query states
-        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
-
-        # get key/value states
-        key_states = project(
-            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None
-        )
-        value_states = project(
-            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None
-        )
-
-        if position_bias is None:
-            if not self.has_relative_attention_bias:
-                position_bias = torch.zeros(
-                    (1, self.n_heads, real_seq_length, key_length), device=query_states.device, dtype=query_states.dtype
-                )
-                if self.gradient_checkpointing and self.training:
-                    position_bias.requires_grad = True
-            else:
-                position_bias = self.compute_bias(real_seq_length, key_length, device=query_states.device)
+        if not stage_manager.is_last_stage():
+            return encoder_outputs
 
-            # if key and values are already calculated
-            # we want only the last query position bias
-            if past_key_value is not None:
-                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]
-
-            if mask is not None:
-                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)
-
-        if self.pruned_heads:
-            mask = torch.ones(position_bias.shape[1])
-            mask[list(self.pruned_heads)] = 0
-            position_bias_masked = position_bias[:, mask.bool()]
+        if self.config.use_weighted_layer_sum:
+            hidden_states = torch.stack(encoder_outputs, dim=1)
+            norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)
+            hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)
         else:
-            position_bias_masked = position_bias
-
-        with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True):
-            attn_output = torch.nn.functional.scaled_dot_product_attention(
-                query_states,
-                key_states,
-                value_states,
-                attn_mask=position_bias_masked,
-                dropout_p=self.dropout,
-                scale=1.0,
-            )
-        attn_output = unshape(attn_output)
-        attn_output = self.o(attn_output)
-
-        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None
-
-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)
-
-        return outputs
-
-    return forward
-
-
-def get_jit_fused_T5_layer_ff_forward():
-    from transformers.models.t5.modeling_t5 import T5LayerFF
-
-    def forward(self: T5LayerFF, hidden_states: torch.Tensor) -> torch.Tensor:
-        forwarded_states = self.layer_norm(hidden_states)
-        forwarded_states = self.DenseReluDense(forwarded_states)
-        hidden_states = self.dropout_add(forwarded_states, hidden_states, self.dropout.p, self.dropout.training)
-        return hidden_states
-
-    return forward
+            hidden_states = encoder_outputs[0]
 
+        hidden_states = self.projector(hidden_states)
+        pooled_output = hidden_states.mean(dim=1)
 
-def get_T5_layer_self_attention_forward():
-    from transformers.models.t5.modeling_t5 import T5LayerSelfAttention
+        logits = self.classifier(pooled_output)
 
-    def forward(
-        self: T5LayerSelfAttention,
-        hidden_states: torch.Tensor,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_bias: Optional[torch.Tensor] = None,
-        layer_head_mask: Optional[torch.Tensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        use_cache: bool = False,
-        output_attentions: bool = False,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
-        normed_hidden_states = self.layer_norm(hidden_states)
-        attention_output = self.SelfAttention(
-            normed_hidden_states,
-            mask=attention_mask,
-            position_bias=position_bias,
-            layer_head_mask=layer_head_mask,
-            past_key_value=past_key_value,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-        )
-        hidden_states = self.dropout_add(attention_output[0], hidden_states, self.dropout.p, self.dropout.training)
-        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them
-        return outputs
-
-    return forward
+        loss = None
 
+        if labels is not None:
+            loss_fct = CrossEntropyLoss()
+            # move labels to correct device to enable PP
+            labels = labels.to(logits.device)
+            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
 
-def get_T5_layer_cross_attention_forward():
-    from transformers.models.t5.modeling_t5 import T5LayerCrossAttention
+        if not return_dict:
+            output = (logits,) + encoder_outputs[1:]
+            return ((loss,) + output) if loss is not None else output
 
-    def forward(
-        self: T5LayerCrossAttention,
-        hidden_states: torch.Tensor,
-        key_value_states: torch.Tensor,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_bias: Optional[torch.Tensor] = None,
-        layer_head_mask: Optional[torch.Tensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        use_cache: bool = False,
-        query_length: Optional[int] = None,
-        output_attentions: bool = False,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
-        normed_hidden_states = self.layer_norm(hidden_states)
-        attention_output = self.EncDecAttention(
-            normed_hidden_states,
-            mask=attention_mask,
-            key_value_states=key_value_states,
-            position_bias=position_bias,
-            layer_head_mask=layer_head_mask,
-            past_key_value=past_key_value,
-            use_cache=use_cache,
-            query_length=query_length,
-            output_attentions=output_attentions,
+        return SequenceClassifierOutput(
+            loss=loss,
+            logits=logits,
+            hidden_states=encoder_outputs.hidden_states,
+            attentions=encoder_outputs.attentions,
         )
-        layer_output = self.dropout_add(attention_output[0], hidden_states, self.dropout.p, self.dropout.training)
-        outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them
-        return outputs
-
-    return forward
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/modeling/vit.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/vit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/auto_policy.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/auto_policy.py`

 * *Files 2% similar despite different names*

```diff
@@ -178,24 +178,14 @@
     ),
     "transformers.models.mistral.modeling_mistral.MistralForCausalLM": PolicyLocation(
         file_name="mistral", class_name="MistralForCausalLMPolicy"
     ),
     "transformers.models.mistral.modeling_mistral.MistralForSequenceClassification": PolicyLocation(
         file_name="mistral", class_name="MistralForSequenceClassificationPolicy"
     ),
-    # Qwen2
-    "transformers.models.qwen2.modeling_qwen2.Qwen2Model": PolicyLocation(
-        file_name="qwen2", class_name="Qwen2ModelPolicy"
-    ),
-    "transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM": PolicyLocation(
-        file_name="qwen2", class_name="Qwen2ForCausalLMPolicy"
-    ),
-    "transformers.models.qwen2.modeling_qwen2.Qwen2ForSequenceClassification": PolicyLocation(
-        file_name="qwen2", class_name="Qwen2ForSequenceClassificationPolicy"
-    ),
 }
 
 
 def import_policy(policy_location: PolicyLocation) -> Policy:
     """
     Dynamically import a Policy class based on the policy location.
     """
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/base_policy.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/base_policy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/bert.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/blip2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bloom.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,15 +12,14 @@
     BloomPipelineForwards,
     build_bloom_alibi_tensor_fn,
     get_bloom_flash_attention_forward,
     get_bloom_sequence_parallel_forward_fn,
     get_jit_fused_bloom_attention_forward,
     get_jit_fused_bloom_gelu_forward,
     get_jit_fused_bloom_mlp_forward,
-    get_lm_forward_with_dist_cross_entropy,
 )
 from ..modeling.jit import get_dropout_add_func, get_jit_fused_dropout_add_func, get_jit_fused_gelu_forward_func
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 
 class BloomPolicy(Policy):
     def __init__(self) -> None:
@@ -284,26 +283,20 @@
         # handle tensor parallelism
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=col_nn.VocabParallelLMHead1D,
                     kwargs=dict(
-                        gather_output=not self.shard_config.parallel_output,
-                        make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
+                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
                     ),
                 ),
                 policy=policy,
                 target_key=BloomForCausalLM,
             )
-            if self.shard_config.parallel_output:
-                method_replacement = {"forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)}
-                self.append_or_create_method_replacement(
-                    description=method_replacement, policy=policy, target_key=BloomForCausalLM
-                )
         else:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=col_nn.PaddingLMHead,
                     kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/falcon.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/falcon.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,20 +3,15 @@
 from typing import Callable, Dict, List
 
 from torch import Tensor, nn
 from torch.nn import Module
 
 import colossalai.shardformer.layer as col_nn
 
-from ..modeling.falcon import (
-    FalconPipelineForwards,
-    build_falcon_alibi_tensor_fn,
-    get_lm_forward_with_dist_cross_entropy,
-    get_tp_falcon_decoder_layer_forward,
-)
+from ..modeling.falcon import FalconPipelineForwards, build_falcon_alibi_tensor_fn, get_tp_falcon_decoder_layer_forward
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = ["FalconPolicy"]
 
 
 class FalconPolicy(Policy):
     def __init__(self) -> None:
@@ -234,27 +229,20 @@
         # handle tensor parallelism
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=col_nn.VocabParallelLMHead1D,
                     kwargs=dict(
-                        gather_output=not self.shard_config.parallel_output,
-                        make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
+                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
                     ),
                 ),
                 policy=policy,
                 target_key=FalconForCausalLM,
             )
-            if self.shard_config.parallel_output:
-                method_replacement = {"forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)}
-                self.append_or_create_method_replacement(
-                    description=method_replacement, policy=policy, target_key=FalconForCausalLM
-                )
-
         else:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=col_nn.PaddingLMHead,
                     kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/gpt2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gpt2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/gptj.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gptj.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/llama.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/llama.py`

 * *Files 2% similar despite different names*

```diff
@@ -137,19 +137,17 @@
                 target_key=LlamaModel,
             )
 
         if self.shard_config.enable_tensor_parallelism:
             assert (
                 self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
             ), f"The number of attention heads must be divisible by tensor parallel size."
-            if hasattr(self.model.config, "num_key_value_heads"):
-                assert (
-                    self.model.config.num_key_value_heads >= self.shard_config.tensor_parallel_size
-                    and self.model.config.num_key_value_heads % self.shard_config.tensor_parallel_size == 0
-                ), f"The number of key_value heads must be divisible by, and must not be less than tensor parallel size."
+            assert (
+                self.model.config.num_key_value_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of key_value heads must be divisible by tensor parallel size."
             decoder_attribute_replacement = {
                 "self_attn.hidden_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
                 "self_attn.num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
             }
             if getattr(self.model.config, "num_key_value_heads", False):
                 decoder_attribute_replacement["self_attn.num_key_value_heads"] = (
                     self.model.config.num_key_value_heads // self.shard_config.tensor_parallel_size
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/mistral.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/mistral.py`

 * *Files 6% similar despite different names*

```diff
@@ -14,15 +14,14 @@
     PaddingLMHead,
     VocabParallelEmbedding1D,
     VocabParallelLMHead1D,
 )
 
 from ..modeling.mistral import (
     MistralForwards,
-    get_lm_forward_with_dist_cross_entropy,
     get_mistral_flash_attention_forward,
     get_mistral_model_forward_for_flash_attn,
 )
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = ["MistralPolicy", "MistralModelPolicy", "MistralForCausalLMPolicy", "MistralForSequenceClassificationPolicy"]
 
@@ -272,26 +271,22 @@
             # add a new item for casual lm
             new_item = {
                 MistralForCausalLM: ModulePolicyDescription(
                     sub_module_replacement=[
                         SubModuleReplacementDescription(
                             suffix="lm_head",
                             target_module=VocabParallelLMHead1D,
-                            kwargs={
-                                "gather_output": not self.shard_config.parallel_output,
-                                "make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by,
-                            },
+                            kwargs=dict(
+                                gather_output=True,
+                                make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
+                            ),
                         )
                     ]
                 )
             }
-            if self.shard_config.parallel_output:
-                new_item[MistralForCausalLM].method_replacement = {
-                    "forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)
-                }
         else:
             new_item = {
                 MistralForCausalLM: ModulePolicyDescription(
                     sub_module_replacement=[
                         SubModuleReplacementDescription(
                             suffix="lm_head",
                             target_module=PaddingLMHead,
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/opt.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/opt.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,15 +17,14 @@
 )
 
 from .._utils import getattr_
 from ..modeling.jit import get_jit_fused_dropout_add_func
 from ..modeling.opt import (
     OPTPipelineForwards,
     get_jit_fused_opt_decoder_layer_forward,
-    get_lm_forward_with_dist_cross_entropy,
     get_opt_decoder_forward_for_flash_attention,
     get_opt_flash_attention_forward,
 )
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = [
     "OPTPolicy",
@@ -266,26 +265,20 @@
         policy = super().module_policy()
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=VocabParallelLMHead1D,
                     kwargs=dict(
-                        gather_output=not self.shard_config.parallel_output,
-                        make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
+                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
                     ),
                 ),
                 policy=policy,
                 target_key=OPTForCausalLM,
             )
-            if self.shard_config.parallel_output:
-                method_replacement = {"forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)}
-                self.append_or_create_method_replacement(
-                    description=method_replacement, policy=policy, target_key=OPTForCausalLM
-                )
         else:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=PaddingLMHead,
                     kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/sam.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/t5.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/t5.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/vit.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/vit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/policies/whisper.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/whisper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/shard/grad_ckpt_config.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/grad_ckpt_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/shard/shard_config.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shard_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -121,7 +121,13 @@
         self.enable_fused_normalization = apex_avail
         self.enable_flash_attention = True
         self.enable_jit_fused = True
         # This can cause non-in-place param sharding when used without ZeRO.
         # It may also slow down training when seq len is small. Plz enable manually.
         # self.enable_sequence_parallelism = True
         # self.enable_sequence_overlap = True
+
+    def _infer(self):
+        """
+        Set default params for inference.
+        """
+        # assert self.pipeline_stage_manager is None, "pipeline parallelism is not supported in inference for now"
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/shard/sharder.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/sharder.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/shard/shardformer.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shardformer.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 import os
 from typing import Dict, List, Tuple
 
-import torch.distributed as dist
 import torch.nn as nn
 from torch import Tensor
 
 from colossalai.cluster import DistCoordinator
 
 from ..policies.base_policy import Policy
 from .shard_config import ShardConfig
@@ -33,19 +32,15 @@
     shard_config = ShardConfig()
     shard_former = ShardFormer(shard_config=shard_config)
     model, shared_params = shard_former.optimize(org_model)
     ```
     """
 
     def __init__(self, shard_config: ShardConfig):
-        self.is_distributed = dist.is_initialized()
-        if self.is_distributed:
-            self.coordinator = DistCoordinator()
-        else:
-            self.coordinator = None
+        self.coordinator = DistCoordinator()
         self.shard_config = shard_config
 
     def optimize(self, model: nn.Module, policy: Policy = None) -> Tuple[nn.Module, List[Dict[int, Tensor]]]:
         r"""
         This method will optimize the model based on the given policy.
 
         Args:
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/shardformer/shard/utils.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/colo_parameter.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/colo_parameter.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/colo_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/colo_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/comm_spec.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/comm_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,15 +2,14 @@
     compute_global_numel,
     customized_distributed_tensor_to_param,
     distribute_tensor,
     distribute_tensor_with_customization,
     get_device_mesh,
     get_global_shape,
     get_layout,
-    get_shard_dim_1d,
     get_sharding_spec,
     init_as_dtensor,
     init_tensor_as_customization_distributed,
     is_customized_distributed_tensor,
     is_distributed_tensor,
     is_sharded,
     redistribute,
@@ -34,15 +33,14 @@
     "sharded_tensor_to_param",
     "compute_global_numel",
     "get_sharding_spec",
     "get_global_shape",
     "get_device_mesh",
     "redistribute",
     "get_layout",
-    "get_shard_dim_1d",
     "is_customized_distributed_tensor",
     "distribute_tensor_with_customization",
     "init_tensor_as_customization_distributed",
     "to_global_for_customized_distributed_tensor",
     "customized_distributed_tensor_to_param",
     "Layout",
     "ShardingSpec",
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/api.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/api.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,38 +4,21 @@
 from typing import Union
 
 import torch
 import torch.distributed as dist
 from torch.distributed import ProcessGroup
 
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.tensor.d_tensor.sharding_spec import DimSpec
 
 from .layout import Layout
 from .layout_converter import LayoutConverter
 from .sharding_spec import ShardingSpec
 
 layout_converter = LayoutConverter()
 
-_SHARD_DIM = DimSpec([0])
-
-
-def get_shard_dim_1d(p: torch.Tensor):
-    """
-    Get the dimension along which the tensor is sharded, for example in 1D Tensor Parallel.
-    Args:
-        p (torch.Tensor): the input tensor
-    Returns:
-        int: the dimension along which the tensor is sharded
-    """
-    if not is_distributed_tensor(p):
-        raise ValueError("p is not a distributed tensor")
-    sharding = p.dist_layout.sharding_spec.sharding_sequence
-    return sharding.index(_SHARD_DIM)
-
 
 def clear_layout_converter():
     global layout_converter
     layout_converter.cached_solution.clear()
 
 
 def is_distributed_tensor(tensor: torch.Tensor) -> bool:
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/comm_spec.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/comm_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/layout.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/layout_converter.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout_converter.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/sharding_spec.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/sharding_spec.py`

 * *Files 5% similar despite different names*

```diff
@@ -136,17 +136,16 @@
         """
         difference = self.difference_dict[(str(self), str(other))]
         return difference
 
 
 class ShardingSpec:
     """
-    Sharding spec describes how to shard a tensor with dim_size dimensions. For example for a 3D tensor, the sharding sequence
-    [R, S0, S1] means not sharding the first dim, sharding the 3rd along the 1st device mesh axis (Process group)
-    and sharding the 3th dim along the 2nd device mesh axis. Useful for say, 2D Tensor Parallel.
+    Sharding spec describes how to shard a tensor with dim_size dimensions. The sharding sequence looks like
+    [R, R, S0, S1], which means
 
     Argument:
         dim_partition_dict(Dict[int, List[int]], optional): The key is the dimension of tensor to be sharded,
             and the value of the key describe which logical axis will be sharded in that dimension.
         sharding_sequence(List[DimSpec], optional): A straight view of ShardingSpec looks like [R, R, S0, S1].
     """
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/d_tensor/utils.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/moe_tensor/api.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/api.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/moe_tensor/moe_info.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/moe_info.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/padded_tensor/api.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/api.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/param_op_hook.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/param_op_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/shape_consistency.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/shape_consistency.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/sharding_spec.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/sharding_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/tensor/utils.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/testing/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/testing/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/testing/comparison.py` & `colossalai-nightly-2024.5.4/colossalai/testing/comparison.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/testing/pytest_wrapper.py` & `colossalai-nightly-2024.5.4/colossalai/testing/pytest_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/testing/random.py` & `colossalai-nightly-2024.5.4/colossalai/testing/random.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/testing/utils.py` & `colossalai-nightly-2024.5.4/colossalai/testing/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/utils/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/utils/common.py` & `colossalai-nightly-2024.5.4/colossalai/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/utils/memory.py` & `colossalai-nightly-2024.5.4/colossalai/utils/memory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/utils/model/utils.py` & `colossalai-nightly-2024.5.4/colossalai/utils/model/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py` & `colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/utils/rank_recorder/rank_recorder.py` & `colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/rank_recorder.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/utils/tensor_detector/tensor_detector.py` & `colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/tensor_detector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/utils/timer.py` & `colossalai-nightly-2024.5.4/colossalai/utils/timer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/chunk.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/chunk.py`

 * *Files 4% similar despite different names*

```diff
@@ -160,16 +160,14 @@
         self.cpu_vis_flag = False
 
         # whether to record l2 norm for the gradient clipping calculation
         self.l2_norm_flag = False
         self.l2_norm = None
 
         self.grad_chunk = None
-        # the async all-reduce/reduce-scatter work of this grad chunk (None means sync)
-        self.grad_reduce_work = None
 
     @property
     def memory_usage(self) -> Dict[str, int]:
         cuda_memory = 0
         cpu_memory = 0
 
         if self.chunk_temp is not None:
@@ -242,15 +240,15 @@
         """Check if the chunk has inf or nan values on CUDA."""
         if self.is_gathered:
             valid_tensor = self.cuda_global_chunk[: self.utilized_size]
         else:
             assert self.cuda_shard is not None  # only check on CUDA
             valid_tensor = self.cuda_shard[: self.valid_end]
 
-        return torch.isinf(valid_tensor).any() | torch.isnan(valid_tensor).any()
+        return torch.isinf(valid_tensor).any().item() | torch.isnan(valid_tensor).any().item()
 
     def set_l2_norm(self) -> None:
         """Record l2 norm of this chunks on CUDA."""
         assert self.l2_norm is None, "you are calculating the l2 norm twice"
         if self.is_gathered:
             valid_tensor = self.cuda_global_chunk[: self.utilized_size]
         else:
@@ -372,57 +370,45 @@
         """Release the usable chunk. It's an operation done in CUDA."""
         # sanity check
         assert self.chunk_temp is None
 
         if self.is_gathered:
             self.__scatter()
 
-    def reduce(self, async_op: bool = False):
+    def reduce(self):
         """Reduce scatter all the gradients. It's an operation done in CUDA."""
         # sanity check
         assert self.is_gathered
-        assert self.grad_reduce_work is None
+
         if self.pg_size == 1:
             # tricky code here
             # just move cuda_global_chunk to cuda_shard
             # the communication is not necessary
             self.__scatter()
             if self.extra_dp_group is not None:
-                self.grad_reduce_work = dist.all_reduce(self.cuda_shard, group=self.extra_dp_group, async_op=async_op)
+                dist.all_reduce(self.cuda_shard, group=self.extra_dp_group)
         elif self.keep_gathered:
             # we use all-reduce here
-            self.grad_reduce_work = dist.all_reduce(self.cuda_global_chunk, group=self.torch_pg, async_op=async_op)
-            if self.extra_dp_group is not None:  # cannot guranatee the order of multiple all-reduce
-                self.wait_async_reduce()
-                self.grad_reduce_work = dist.all_reduce(
-                    self.cuda_global_chunk, group=self.extra_dp_group, async_op=async_op
-                )
+            dist.all_reduce(self.cuda_global_chunk, group=self.torch_pg)
+            if self.extra_dp_group is not None:
+                dist.all_reduce(self.cuda_global_chunk, group=self.extra_dp_group)
         else:
             self.cuda_shard = torch.empty(
                 self.shard_size, dtype=self.dtype, device=get_accelerator().get_current_device()
             )
 
             input_list = list(torch.chunk(self.cuda_global_chunk, chunks=self.pg_size, dim=0))
-            self.grad_reduce_work = dist.reduce_scatter(
-                self.cuda_shard, input_list, group=self.torch_pg, async_op=async_op
-            )
-
+            dist.reduce_scatter(self.cuda_shard, input_list, group=self.torch_pg)
             if self.extra_dp_group is not None:
-                self.wait_async_reduce()
-                self.grad_reduce_work = dist.all_reduce(self.cuda_shard, group=self.extra_dp_group, async_op=async_op)
+                dist.all_reduce(self.cuda_shard, group=self.extra_dp_group)
 
             free_storage(self.cuda_global_chunk)
             self.is_gathered = False
         self.__update_tensors_state(TensorState.HOLD)
 
-    def wait_async_reduce(self) -> None:
-        if self.grad_reduce_work is not None:
-            self.grad_reduce_work.wait()
-            self.grad_reduce_work = None
-
     def tensor_trans_state(self, tensor: torch.Tensor, tensor_state: TensorState) -> None:
         """
         Make a transition of the tensor into the next state.
 
         Args:
             tensor (torch.Tensor): a torch Tensor object.
             tensor_state (TensorState): the target state for transition.
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/manager.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/manager.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,36 +16,27 @@
     A manager class to manipulate the tensors in chunks.
 
     Args:
         chunk_configuration (Dict[int, Dict]): the configuration dictionary of this chunk manager.
         init_device (torch.device): optional, the device on which the chunk is initialized. The default is None.
     """
 
-    def __init__(
-        self,
-        chunk_configuration,
-        init_device: Optional[torch.device] = None,
-        reuse_fp16_chunk: bool = True,
-    ) -> None:
+    def __init__(self, chunk_configuration, init_device: Optional[torch.device] = None) -> None:
         self.device = init_device or get_accelerator().get_current_device()
         self.dp_degree_chunk_size_dict: Dict[int, int] = dict()
         self.kwargs_config = chunk_configuration
         for k, v in self.kwargs_config.items():
             self.dp_degree_chunk_size_dict[k] = v.pop("chunk_size")
             v["init_device"] = self.device
 
         self.chunk_groups: Dict[str, Deque[Chunk]] = dict()
         self.tensor_chunk_map: Dict[torch.Tensor, Chunk] = dict()
         self.accessed_chunks: Set[Chunk] = set()
         self.accessed_mem: int = 0
         self.total_mem: Dict[str, int] = {"cpu": 0, "cuda": 0}
-        self.reuse_fp16_chunk = reuse_fp16_chunk
-        # Whether model is accumulating gradients,
-        self.accumulating_grads = False
-        self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
 
     def register_tensor(
         self,
         tensor: torch.Tensor,
         group_type: str,
         config_key: int,
         zero_group: ProcessGroup,
@@ -139,20 +130,20 @@
         self.__add_memory_usage(chunk.memory_usage)
 
     def trans_tensor_state(self, tensor: torch.Tensor, state: TensorState) -> None:
         """Transit tensor state according to pre-defined state machine."""
         chunk = self.tensor_chunk_map[tensor]
         chunk.tensor_trans_state(tensor, state)
 
-    def reduce_chunk(self, chunk: Chunk, async_op: bool = False) -> bool:
+    def reduce_chunk(self, chunk: Chunk) -> bool:
         """Reduce or all reduce the chunk."""
         if not chunk.can_reduce:
             return False
         self.__sub_memory_usage(chunk.memory_usage)
-        chunk.reduce(async_op=async_op)
+        chunk.reduce()
         self.__sub_accessed_chunk(chunk)
         self.__add_memory_usage(chunk.memory_usage)
         return True
 
     def fake_release_chunk(self, chunk: Chunk) -> None:
         """Release gathered chunk in a fake mode.
         This function is used for keep-gathered chunk in the inference mode.
@@ -268,15 +259,15 @@
         self.__add_memory_usage(grad_chunk.memory_usage)
         if grad_chunk not in self.accessed_chunks:
             self.accessed_chunks.add(grad_chunk)
             self.accessed_mem += grad_chunk.chunk_mem
         return grad_chunk
 
     def rearrange_accumulated_grad_chunk(self, chunk: Chunk) -> Chunk:
-        """Rearrange gradients accumulated in chunk.grad_chunk, and get prepared for gradient reduction."""
+        """Rearrange gradients accumulated in chunk.grad_chunk, and getP prepared for gradient reduction."""
 
         assert chunk.grad_chunk is not None
 
         # Make a backup for gradient accumulated before.
         # Here backup gradients should be multiplied, since it will be divided after gradient reduction.
         if chunk.grad_chunk.is_gathered:
             accumulated_grad = chunk.grad_chunk.cuda_global_chunk.clone().detach().mul_(chunk.pg_size)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/search_utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/search_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/chunk/utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -15,15 +15,14 @@
     return a / b
 
 
 def init_chunk_manager(
     model: nn.Module,
     init_device: Optional[torch.device] = None,
     hidden_dim: Optional[int] = None,
-    reuse_fp16_chunk: bool = True,
     verbose: bool = False,
     **kwargs,
 ) -> ChunkManager:
     if hidden_dim:
         search_interval = hidden_dim
     else:
         search_interval = 1024  # defaults to 1024
@@ -47,13 +46,9 @@
             "used number: {:.2f} * 2^20, wasted number: {:.2f} * 2^20\n".format(total_size, wasted_size),
             "total wasted percentage is {:.2f}%".format(100 * safe_div(wasted_size, total_size + wasted_size)),
             sep="",
             flush=True,
         )
     dist.barrier()
 
-    chunk_manager = ChunkManager(
-        config_dict,
-        init_device,
-        reuse_fp16_chunk=reuse_fp16_chunk,
-    )
+    chunk_manager = ChunkManager(config_dict, init_device)
     return chunk_manager
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/gemini_ddp.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_ddp.py`

 * *Files 6% similar despite different names*

```diff
@@ -92,38 +92,30 @@
         scatter_after_inference: bool = True,
         mixed_precision: torch.dtype = torch.float16,
         zero_group: Optional[ProcessGroup] = None,
         memstats: Optional[MemStats] = None,  # genimi memory stats
         master_weights: bool = True,
         extra_dp_group: Optional[ProcessGroup] = None,
         verbose: bool = False,
-        enable_async_reduce: bool = True,
     ) -> None:
         assert mixed_precision in (torch.float16, torch.bfloat16)
-        reuse_fp16_chunk = master_weights if not enable_gradient_accumulation else False
-        self.enable_gradient_accumulation = enable_gradient_accumulation
         if chunk_config_dict is not None:
-            self.chunk_manager = ChunkManager(
-                chunk_config_dict,
-                chunk_init_device,
-                reuse_fp16_chunk=reuse_fp16_chunk,
-            )
+            self.chunk_manager = ChunkManager(chunk_config_dict, chunk_init_device)
         else:
             # some ugly hotfix for the compatibility with Lightning
             if search_range_m is None:
                 search_range_m = 32
             self.chunk_manager = init_chunk_manager(
                 model=module,
                 init_device=chunk_init_device,
                 hidden_dim=hidden_dim,
                 search_range_m=search_range_m,
                 min_chunk_size_m=min_chunk_size_m,
                 strict_ddp_flag=strict_ddp_mode,
                 process_group=zero_group,
-                reuse_fp16_chunk=reuse_fp16_chunk,
                 verbose=verbose,
             )
         self.gemini_manager = GeminiManager(
             placement_policy,
             self.chunk_manager,
             memstats,
             shard_param_frac=shard_param_frac,
@@ -132,24 +124,31 @@
             warmup_non_model_data_ratio=warmup_non_model_data_ratio,
             steady_cuda_cap_ratio=steady_cuda_cap_ratio,
         )
         self.force_outputs_fp32 = force_outputs_fp32
         self.param_op_hook = GeminiZeROHook(self.gemini_manager)
         self.fp32_params: List[torch.Tensor] = list()
         self.fp16_params: List[ColoParameter] = list()
+        self.overflow_counter = 0
         self.grads_device: Dict[torch.Tensor, torch.device] = dict()
         self.param2name: Dict[nn.Parameter, str] = dict()
         self.name2param: Dict[str, nn.Parameter] = dict()
         self.scatter_after_inference = scatter_after_inference
         self.mixed_precision = mixed_precision
         self.zero_group = zero_group or _get_default_group()
         self.extra_dp_group = extra_dp_group
 
+        self.reuse_fp16_chunk = master_weights
         self.master_weights = master_weights
 
+        self.enable_gradient_accumulation = enable_gradient_accumulation
+        if self.enable_gradient_accumulation:
+            self.reuse_fp16_chunk = False
+        self.accumulating_grads = False  # Whether model is accumulating gradients
+
         self._logger = get_dist_logger()
 
         if self.gemini_manager._premade_memstats_:
             # build chunk in param runtime visited order.
             param_order = self.gemini_manager.memstats()._param_runtime_order
         else:
             # build chunk in param initialized order.
@@ -175,39 +174,15 @@
         self._non_persistent_buffers_set = self._get_non_persistent_buffers_set(module)
         self._cast_buffers()
         # register grad hook
         for p in module.parameters():
             if is_ddp_ignored(p):
                 continue
             if p.requires_grad:
-                assert not hasattr(p, "_grad_handle")
-                p._grad_handle = p.register_hook(
-                    partial(
-                        GeminiDDP.grad_handle,
-                        chunk_manager=self.chunk_manager,
-                        param2name=self.param2name,
-                        grads_device=self.grads_device,
-                        master_weights=self.master_weights,
-                        enable_gradient_accumulation=self.enable_gradient_accumulation,
-                        p=p,
-                        async_reduce=enable_async_reduce,
-                    )
-                )
-
-    def remove_hooks(self):
-        for p in self.module.parameters():
-            if is_ddp_ignored(p):
-                continue
-            if p.requires_grad:
-                assert hasattr(p, "_grad_handle")
-                p._grad_handle.remove()
-                delattr(p, "_grad_handle")
-
-    def __del__(self):
-        self.remove_hooks()
+                p.register_hook(partial(self.grad_handle, p))
 
     def parameters(self, recurse: bool = True):
         return self.module.parameters(recurse)
 
     def named_parameters(self, prefix: str = "", recurse: bool = True):
         return self.module.named_parameters(prefix, recurse)
 
@@ -333,142 +308,101 @@
         # set a visit label for all parameters
         # the label is used to check whether the parameter is correctly reduced
         for param in self.param2name:
             if not is_ddp_ignored(param):
                 setattr(param, "_gemini_reduced", False)
 
     def _post_backward(self):
-        for param in self.param2name:
-            if hasattr(param, "_release_grad_chunk_cb"):
-                param._release_grad_chunk_cb()
-                delattr(param, "_release_grad_chunk_cb")
-
         if self.chunk_manager.accessed_mem != 0:
             error_params = ["Reduction failed at followed parameters:"]
             for param in self.param2name:
                 if not is_ddp_ignored(param) and not getattr(param, "_gemini_reduced"):
                     error_params.append(self.param2name[param])
             error_str = "\n\t".join(error_params)
             raise RuntimeError(
                 "ZERO DDP error: the synchronization of gradients doesn't exit properly.",
                 "The most possible reason is that the model is not compatible with GeminiDDP.\n",
                 f"{error_str}",
             )
         self._setup_grads_ptr()
-        if self.enable_gradient_accumulation and not self.chunk_manager.accumulating_grads:
-            self.chunk_manager.accumulating_grads = True  # Turn on the state of gradient accumulation.
+        if self.enable_gradient_accumulation and not self.accumulating_grads:
+            self.accumulating_grads = True  # Turn on the state of gradient accumulation.
         self._logger.debug(
             f"comp cuda demand time: {self.gemini_manager._comp_cuda_demand_time}, layout time: {self.gemini_manager._layout_time}, evict time: {self.gemini_manager._evict_time}, CPU->CUDA vol: {self.gemini_manager._h2d_volume}B, CUDA->CPU vol: {self.gemini_manager._d2h_volume}"
         )
         self.gemini_manager.post_iter()
 
     def backward(self, loss: torch.Tensor):
         self._pre_backward()
         with self.param_op_hook.switch_to_backward(), ColoParamOpHookManager.use_hooks(self.param_op_hook):
             loss.backward()
         self._post_backward()
 
     def backward_by_grad(self, tensor, grad):
         raise RuntimeError("Gemini is not compatible with pipeline. backward_by_grad shoudn't be called in Gemini.")
 
-    @staticmethod
-    def grad_handle(
-        grad,
-        chunk_manager: ChunkManager,
-        param2name: Dict,
-        grads_device: Dict,
-        master_weights: bool,
-        enable_gradient_accumulation: bool,
-        p: nn.Parameter,
-        async_reduce: bool,
-    ):
+    def grad_handle(self, p, grad):
         setattr(p, "_gemini_reduced", True)
         empty_grad = torch.empty_like(grad)
         free_storage(empty_grad)
         with torch._C.DisableTorchFunction():
-            chunk = chunk_manager.get_chunk(p)
+            chunk = self.chunk_manager.get_chunk(p)
             if chunk.tensors_info[p].state != TensorState.HOLD_AFTER_BWD:
                 raise RuntimeError(
-                    f"Parameter `{param2name[p]}` failed at the gradient reduction. "
+                    f"Parameter `{self.param2name[p]}` failed at the gradient reduction. "
                     "Some unsupported torch function is operated upon this parameter."
                 )
             grad_chunk = chunk
-            if not chunk_manager.reuse_fp16_chunk:
-                if not chunk_manager.accumulating_grads:
-                    grad_chunk = chunk_manager.init_grad_chunk(chunk)
+            if not self.reuse_fp16_chunk:
+                if not self.accumulating_grads:
+                    grad_chunk = self.chunk_manager.init_grad_chunk(chunk)
                 else:
                     assert chunk.grad_chunk is not None
-                    if chunk.grad_chunk not in chunk_manager.accessed_chunks:
-                        grad_chunk = chunk_manager.rearrange_accumulated_grad_chunk(chunk)
+                    if chunk.grad_chunk not in self.chunk_manager.accessed_chunks:
+                        grad_chunk = self.chunk_manager.rearrange_accumulated_grad_chunk(chunk)
                     else:
                         grad_chunk = chunk.grad_chunk
                         chunk.grad_chunk.l2_norm = None
 
                 # hold -> compute -> hold after bwd
                 grad_chunk.tensor_trans_state(p, TensorState.COMPUTE)
                 grad_chunk.tensor_trans_state(p, TensorState.HOLD_AFTER_BWD)
                 # fp16 param chunk: hold after bwd -> ready for reduce -> hold
                 chunk.tensor_trans_state(p, TensorState.READY_FOR_REDUCE)
                 chunk.tensor_trans_state(p, TensorState.HOLD)
 
             grad_chunk.tensor_trans_state(p, TensorState.READY_FOR_REDUCE)
-            if not chunk_manager.accumulating_grads:
-                grad_chunk.copy_tensor_to_chunk_slice(p, grad, update_ptr=chunk_manager.reuse_fp16_chunk)
+            if not self.accumulating_grads:
+                grad_chunk.copy_tensor_to_chunk_slice(p, grad, update_ptr=self.reuse_fp16_chunk)
             else:
                 grad_chunk.add_tensor_to_chunk_slice(p, grad)
-            reduced = chunk_manager.reduce_chunk(grad_chunk, async_op=async_reduce)
-            if reduced:  # if not async, can release immediately, else release in when work finished
-                if async_reduce:
-                    # dirty fix by installing callback
-                    assert not hasattr(p, "_release_grad_chunk_cb")
-
-                    def _release_grad_chunk_cb():
-                        grad_chunk.wait_async_reduce()
-                        GeminiDDP.release_grad_chunk_handle(
-                            chunk_manager,
-                            grads_device,
-                            master_weights,
-                            enable_gradient_accumulation,
-                            p,
-                            chunk,
-                            grad_chunk,
-                        )
-
-                    p._release_grad_chunk_cb = _release_grad_chunk_cb
+            reduced = self.chunk_manager.reduce_chunk(grad_chunk)
+            if reduced:
+                if not self.reuse_fp16_chunk:
+                    if chunk.keep_gathered:
+                        self.chunk_manager.fake_release_chunk(chunk)
+                    else:
+                        self.chunk_manager.release_chunk(chunk)
+                if grad_chunk.is_gathered:
+                    grad_chunk.cuda_global_chunk.div_(chunk.pg_size)
+                    if self.extra_dp_group is not None:
+                        grad_chunk.cuda_global_chunk.div_(chunk.extra_dp_size)
                 else:
-                    GeminiDDP.release_grad_chunk_handle(
-                        chunk_manager, grads_device, master_weights, enable_gradient_accumulation, p, chunk, grad_chunk
-                    )
-        return empty_grad
-
-    @staticmethod
-    def release_grad_chunk_handle(
-        chunk_manager, grads_device, master_weights, enable_gradient_accumulation, p, chunk, grad_chunk
-    ):
-        if not chunk_manager.reuse_fp16_chunk:
-            if chunk.keep_gathered:
-                chunk_manager.fake_release_chunk(chunk)
-            else:
-                chunk_manager.release_chunk(chunk)
-        if grad_chunk.is_gathered:
-            grad_chunk.cuda_global_chunk.div_(chunk.pg_size)
-            if chunk.extra_dp_group is not None:
-                grad_chunk.cuda_global_chunk.div_(chunk.extra_dp_size)
-        else:
-            grad_chunk.cuda_shard.div_(chunk.pg_size)
-            if chunk.extra_dp_group is not None:
-                grad_chunk.cuda_shard.div_(chunk.extra_dp_size)
+                    grad_chunk.cuda_shard.div_(chunk.pg_size)
+                    if self.extra_dp_group is not None:
+                        grad_chunk.cuda_shard.div_(chunk.extra_dp_size)
                 # check overflow elements
-        chunk_manager.overflow_counter += grad_chunk.has_inf_or_nan
-        # record l2 norm for gradient clipping. flag is bound to fp16 chunk
-        if chunk.l2_norm_flag:
-            grad_chunk.set_l2_norm()
-        chunk_manager.move_chunk(grad_chunk, grads_device[p], force_copy=True)
-        if not (master_weights) or (enable_gradient_accumulation):
-            chunk_manager.move_chunk(chunk, grads_device[p], force_copy=True)
+                self.overflow_counter += grad_chunk.has_inf_or_nan
+                # record l2 norm for gradient clipping. flag is bound to fp16 chunk
+                if chunk.l2_norm_flag:
+                    grad_chunk.set_l2_norm()
+                self.chunk_manager.move_chunk(grad_chunk, self.grads_device[p], force_copy=True)
+                if not (self.master_weights) or (self.enable_gradient_accumulation):
+                    self.chunk_manager.move_chunk(chunk, self.grads_device[p], force_copy=True)
+        return empty_grad
 
     def zero_grad(self, set_to_none: bool = False) -> None:
         self.module.zero_grad(set_to_none=True)
 
     def set_chunk_grad_device(self, chunk: Chunk, device: torch.device) -> None:
         for tensor in chunk.get_tensors():
             self.grads_device[tensor] = device
@@ -575,19 +509,19 @@
             prefix (str): the prefix for parameters and buffers used in this
                 module
         """
         assert keep_vars is False, "`state_dict` with parameter, `keep_vars=True`, is not supported now."
 
         # get copies of fp32 parameters in CPU
         # as memory of fp16_params may be reused by grad, it's not reliable, we should use fp32_params and convert to fp16
-        params = self.fp32_params if self.chunk_manager.reuse_fp16_chunk else self.fp16_params
+        params = self.fp32_params if self.reuse_fp16_chunk else self.fp16_params
         param_to_save_data = self._get_param_to_save_data(params, only_rank_0)
         # get the mapping between copies and fp16 parameters
         p_mapping = dict()
-        if self.chunk_manager.reuse_fp16_chunk:
+        if self.reuse_fp16_chunk:
             for p, fp32_p in zip(self.fp16_params, self.fp32_params):
                 name = self.param2name[p]
                 assert fp32_p in param_to_save_data, "Parameter '{}' is neglected in the chunk list".format(name)
                 record_parameter = param_to_save_data[fp32_p]
                 p_mapping[p] = record_parameter
         else:
             p_mapping = param_to_save_data
@@ -775,32 +709,30 @@
 
         fp32_to_name = dict()
         for p, fp32_p in zip(self.fp16_params, self.fp32_params):
             if p is not None:
                 name = self.param2name[p]
                 fp32_to_name[fp32_p] = name
 
-        params_to_load = self.fp32_params if self.chunk_manager.reuse_fp16_chunk else self.fp16_params
+        params_to_load = self.fp32_params if self.reuse_fp16_chunk else self.fp16_params
         chunk_list = self.chunk_manager.get_chunks(params_to_load)
         for chunk in chunk_list:
             temp_chunk = get_temp_total_chunk_on_cuda(chunk, self.mixed_precision)
 
             for tensor, tensor_info in chunk.tensors_info.items():
                 source_device_mesh, source_sharding_spec, shard_fn, gather_fn = None, None, None, None
                 if is_distributed_tensor(tensor):
                     # shard the input param
                     source_device_mesh = get_device_mesh(tensor)
                     source_sharding_spec = get_sharding_spec(tensor)
                 elif is_customized_distributed_tensor(tensor):
                     shard_fn = tensor.shard_fn
                     gather_fn = tensor.gather_fn
 
-                parameter_name = (
-                    fp32_to_name[tensor] if self.chunk_manager.reuse_fp16_chunk else self.param2name[tensor]
-                )
+                parameter_name = fp32_to_name[tensor] if self.reuse_fp16_chunk else self.param2name[tensor]
                 parameter_slice = temp_chunk[tensor_info.offset : tensor_info.end]
                 load(
                     parameter_name,
                     tensor,
                     partial(load_parameter, parameter_slice),
                     source_device_mesh,
                     source_sharding_spec,
@@ -964,15 +896,15 @@
         for name, param in self.name2param.items():
             if param is not None:
                 if is_ddp_ignored(param):
                     # deal with ddp ignored parameters
                     gathered_param = param if keep_vars else param.detach()
                 else:
                     # as memory of fp16 param may be reused, we should use fp32 param and then convert to fp16
-                    param_to_save = fp16_to_fp32[param] if self.chunk_manager.reuse_fp16_chunk else param
+                    param_to_save = fp16_to_fp32[param] if self.reuse_fp16_chunk else param
                     if param_to_save not in gathered_param_buffer:
                         chunk = self.chunk_manager.get_chunk(param_to_save)
                         gathered_param_buffer.update(self._get_chunk_to_save_data(chunk, only_rank_0))
                     gathered_param = gathered_param_buffer.pop(param_to_save)
 
                 block, block_size = sharder.append_param(prefix + name, gathered_param)
                 if block is not None:
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/gemini_hook.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/gemini_mgr.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/gemini_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_optimizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -58,18 +58,18 @@
     ) -> None:
         super().__init__(
             initial_scale, min_scale, growth_factor, backoff_factor, growth_interval, hysteresis, max_scale
         )
         self.module = module
 
     def check_local_overflow(self) -> bool:
-        return self.module.chunk_manager.overflow_counter.item() > 0
+        return self.module.overflow_counter > 0
 
     def pre_zero_grad(self) -> None:
-        self.module.chunk_manager.overflow_counter.zero_()
+        self.module.overflow_counter = 0
 
 
 class GeminiOptimizer(OptimizerWrapper):
     """A wrapper for optimizer. ``GeminiDDP`` and ``GeminiOptimizer`` implement Zero Redundancy Optimizer (ZeRO state-3).
 
     Note:
         You must use ``GeminiDDP`` with ``GeminiOptimizer``.
@@ -198,15 +198,15 @@
 
     def _set_grad_ptr(self):
         for group in self.param_groups:
             for fake_param in group["params"]:
                 chunk16 = self.param_to_chunk16[fake_param]
                 begin, end = self.param_to_range[fake_param]
 
-                grad_chunk16 = chunk16 if self.module.chunk_manager.reuse_fp16_chunk else chunk16.grad_chunk
+                grad_chunk16 = chunk16 if self.module.reuse_fp16_chunk else chunk16.grad_chunk
                 fake_param.data = grad_chunk16.payload[begin:end]
                 fake_param.grad = fake_param.data
 
                 to_update_chunk = chunk16.paired_chunk if self.module.master_weights else chunk16
                 fake_param.data = to_update_chunk.payload[begin:end]
 
     def _update_fp16_params(self):
@@ -217,22 +217,22 @@
                 fake_param.data = none_tensor.to(fake_param.device)
 
         for chunk16 in self.chunk16_set:
             chunk16.optim_update()
 
     def _clear_global_norm(self) -> None:
         for c16 in self.chunk16_set:
-            grad_chunk = c16 if self.module.chunk_manager.reuse_fp16_chunk else c16.grad_chunk
+            grad_chunk = c16 if self.module.reuse_fp16_chunk else c16.grad_chunk
             grad_chunk.l2_norm = None
 
     def _calc_global_norm(self) -> float:
         norm_sqr: float = 0.0
         group_to_norm = dict()
         for c16 in self.chunk16_set:
-            grad_chunk = c16 if self.module.chunk_manager.reuse_fp16_chunk else c16.grad_chunk
+            grad_chunk = c16 if self.module.reuse_fp16_chunk else c16.grad_chunk
             assert grad_chunk.l2_norm is not None
 
             if grad_chunk.is_gathered:
                 norm_sqr += grad_chunk.l2_norm
             else:
                 # this chunk is sharded, use communication to collect total norm
                 if grad_chunk.torch_pg not in group_to_norm:
@@ -271,28 +271,28 @@
         self._set_grad_ptr()
 
         if self.mix_precision_mixin.should_skip_step():
             if self.verbose:
                 self._logger.info(f"Found overflow. Skip step")
             self._clear_global_norm()  # clear recorded norm
             self.zero_grad()  # reset all gradients
-            if self.module.chunk_manager.reuse_fp16_chunk:
+            if self.module.reuse_fp16_chunk:
                 self._update_fp16_params()
             return
 
         # get combined scale. combined scale = loss scale * clipping norm
         # so that gradient = gradient / combined scale
         combined_scale = self._get_combined_scale()
 
         ret = self.optim.step(div_scale=combined_scale, *args, **kwargs)
         self._register_states()
         self.zero_grad()
         if self.module.master_weights:
             self._update_fp16_params()
-        self.module.chunk_manager.accumulating_grads = False
+        self.module.accumulating_grads = False
         return ret
 
     def clip_grad_norm(self, model: torch.nn.Module, max_norm: float, norm_type: float = 2.0):
         raise NotImplementedError
 
     def backward(self, loss: torch.Tensor):
         loss = self.mix_precision_mixin.pre_backward(loss)
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/memory_stats.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_stats.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/memstats_collector.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/param_runtime_order.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/param_runtime_order.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/memory_tracer/utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/placement_policy.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/placement_policy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/gemini/utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/low_level/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/gradient_store.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/gradient_store.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,32 +2,29 @@
 
 from torch import Tensor
 
 from .base_store import BaseStore
 
 
 class GradientStore(BaseStore):
-    def __init__(self, *args, partition_grad: bool = False, require_grad_sync: bool = True):
+    def __init__(self, *args, partition_grad: bool = False):
         super().__init__(*args)
         """
         self._grads_of_params mapping the parameter and its gradient slices
         data structure:
         {
          group_id:{
             param_id: [grad_rank0, grad_rank1, ...]
           }
         }
         """
         self._grads_of_params = dict()
-        # stage 2
-        self._partition_grads = partition_grad
-        # grad accumulation
-        self.require_grad_sync = require_grad_sync
-        self._working_index = 0 if partition_grad else self._local_rank
         # for zero2, it's `param_id: [grad_local_rank]`
+        self._working_index = 0 if partition_grad else self._local_rank
+
         self.grad_to_param_mapping = dict()
 
     def get_partitioned_gradients_by_param_id(self, group_id: int, param_id: int) -> List:
         """Return list of gradient slices of a specific parameter
 
         Args:
             group_id (int): The index of a parameter group
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/parameter_store.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/parameter_store.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,7 @@
-from typing import Dict
-
 from torch import Tensor
 from torch.distributed import ProcessGroup
 
 from .base_store import BaseStore
 
 
 class ParameterStore(BaseStore):
@@ -45,16 +43,7 @@
         Args:
             master_param (Tensor): The parameter copy in optimizer
             working_param (Tensor): The parameter of the model
         """
 
         self.master_to_working_param[id(master_param)] = working_param
         self.working_to_master_param[id(working_param)] = master_param
-
-    def get_padding_map(self) -> Dict[int, Tensor]:
-        """Return the padding map
-
-        Returns:
-            Dict[int, Tensor]: The padding map
-        """
-
-        return self._padding_map
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/low_level/bookkeeping/tensor_bucket.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/tensor_bucket.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/low_level/low_level_optim.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/low_level_optim.py`

 * *Files 9% similar despite different names*

```diff
@@ -86,20 +86,46 @@
     ):
         super(LowLevelZeroOptimizer, self).__init__(optim=optimizer)
 
         self._dtype = self.optim.param_groups[0]["params"][0].dtype
         self._logger = get_dist_logger()
         self._verbose = verbose
 
+        # stage 2
+        self._partition_grads = partition_grad
+
         self._cpu_offload = cpu_offload
 
+        # grad accumulation
+        self.require_grad_sync = True
+
+        # if process_group is none, will use the default one
+        self.dp_pg = dp_process_group
+        self._local_rank = dist.get_rank(group=self.dp_pg)
+        self._world_size = dist.get_world_size(group=self.dp_pg)
+
+        # extra dp
+        # This group is used to sync moe param, dp_world_size = moe_duplicates * extra_dp_size.
+        # Non moe param will be sync by global dp pg, moe param will be sync by extra dp pg.
+        # Moe param grad is be split as non moe param by global dp pg, and grad will be merged in step.
+        # And moe working and master param are split by extra dp pg.
+        self.moe_extra_dp_pg = moe_extra_dp_process_group
+        if self.moe_extra_dp_pg is not None:
+            self.moe_extra_dp_pg_size = dist.get_world_size(group=self.moe_extra_dp_pg)
+            self.moe_extra_dp_pg_rank = dist.get_rank(group=self.moe_extra_dp_pg)
+
         # working and master params for mixed precision training
         self._working_param_groups = dict()
         self._master_param_groups_of_current_rank = dict()
 
+        # communication params
+        self._overlap_communication = overlap_communication
+        self._reduce_bucket_size = reduce_bucket_size
+        self._communication_dtype = communication_dtype
+
         # gradient clipping
         self._clip_grad_norm = clip_grad_norm
 
         # master weights copy
         self._master_weights = master_weights
 
         if forced_dtype:
@@ -110,34 +136,32 @@
             self._dtype = forced_dtype
 
         # check argument conflict
         self._sanity_checks()
 
         # ParameterStore will manage the tensor buffers used for zero
         # it will not manage the tensors used by mixed precision training
-        self._param_store = ParameterStore(dp_process_group)
-        self._grad_store = GradientStore(dp_process_group, partition_grad=partition_grad, require_grad_sync=True)
-        self._bucket_store = BucketStore(
-            dp_process_group, reduce_bucket_size, overlap_communication, communication_dtype, moe_extra_dp_process_group
-        )
+        self._param_store = ParameterStore(self.dp_pg)
+        self._grad_store = GradientStore(self.dp_pg, partition_grad=partition_grad)
+        self._bucket_store = BucketStore(self.dp_pg)
 
         # moe param should not be stored in working_groups
         # because they have different parallel strategy
         # so we need to store them separately in param_groups
         # instead of working_groups
         self.working_moe_params = list()
 
         # iterate over the param group in the optimizer
         # partition these param groups for data parallel training
         # and add buffers to parameter store for future access
         for group_id, param_group in enumerate(self.optim.param_groups):
             group_params = list()
             for param in param_group["params"]:
                 if param.requires_grad:
-                    if self._bucket_store.moe_extra_dp_pg is None:
+                    if self.moe_extra_dp_pg is None:
                         # skip moe param
                         if is_moe_tensor(param):
                             self.working_moe_params.append(param)
                             continue
                     group_params.append(param)
 
             # add the working params to working_param_groups for bookkeeping
@@ -166,18 +190,23 @@
             self.moe_master_to_working_map = {}
             for master_moe_param, working_moe_param in zip(self.master_moe_params, self.working_moe_params):
                 self.moe_master_to_working_map[id(master_moe_param)] = working_moe_param
             # add to optim
             param_group["params"] = self.master_moe_params
             self.optim.param_groups.append(param_group)
 
+        # initialize communication stream for
+        # communication-computation overlapping
+        if self._overlap_communication:
+            self._comm_stream = get_accelerator().Stream()
+
         # reduction hook is only used if overlapping communication
         # or stage 2 is used
         # if it is stage 1 without overlapping, no hook will be attached
-        if self._bucket_store._overlap_communication or self._grad_store._partition_grads:
+        if self._overlap_communication or self._partition_grads:
             self._attach_reduction_hook()
 
         # initialize mixed precision mixin
         self.mixed_precision_mixin: Optional[MixedPrecisionMixin] = None
         if self._dtype is torch.float16:
             self.mixed_precision_mixin = LowLevelZeroFP16MixedPrecisionMixin(
                 self.num_param_groups,
@@ -189,17 +218,14 @@
                 growth_interval=growth_interval,
                 hysteresis=hysteresis,
                 max_scale=max_scale,
             )
         elif self._dtype is torch.bfloat16:
             self.mixed_precision_mixin = BF16MixedPrecisionMixin()
 
-    def __del__(self):
-        self.remove_hooks()
-
     @property
     def dtype(self):
         return self._dtype
 
     @property
     def num_param_groups(self):
         return len(self._working_param_groups)
@@ -216,102 +242,83 @@
 
     def _create_master_param_current_rank(self, param_list):
         # split each param evenly by world size
         params_current_rank = []
         device = "cpu" if self._cpu_offload else get_accelerator().get_current_device()
 
         for param in param_list:
-            padding_size = (
-                self._bucket_store.zero_world_size - param.numel() % self._bucket_store.zero_world_size
-            ) % self._bucket_store.zero_world_size
+            padding_size = (self._world_size - param.numel() % self._world_size) % self._world_size
             self._param_store.record_param_padding_size(param, padding_size)
 
             with torch.no_grad():
                 if padding_size > 0:
                     padding_param = torch.nn.functional.pad(param.data.view(-1), [0, padding_size])
                     # reset working params' ptr when no master weights
                     if self._master_weights == False:
                         param.data = padding_param[: param.numel()].view(param.shape)
                 else:
                     padding_param = param.data.view(-1)
 
-                if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(param):
-                    splited_params = padding_param.split(
-                        padding_param.numel() // self._bucket_store.moe_extra_dp_pg_size
-                    )
-                    splited_params = splited_params[self._bucket_store.moe_extra_dp_pg_rank]
+                if self.moe_extra_dp_pg is not None and is_moe_tensor(param):
+                    splited_params = padding_param.split(padding_param.numel() // self.moe_extra_dp_pg_size)
+                    splited_params = splited_params[self.moe_extra_dp_pg_rank]
                 else:
-                    splited_params = padding_param.split(padding_param.numel() // self._bucket_store.zero_world_size)
-                    splited_params = splited_params[self._bucket_store.zero_local_rank]
+                    splited_params = padding_param.split(padding_param.numel() // self._world_size)
+                    splited_params = splited_params[self._local_rank]
 
                 # use fp32 when master_weights is True
                 if self._master_weights is True:
                     splited_param_current_rank = splited_params.detach().float().to(device)
                 else:
                     splited_param_current_rank = splited_params
 
-                # Send the splited view to the optimizer to match ZeRO 2 grad shape
                 params_current_rank.append(splited_param_current_rank)
                 self._param_store.link_master_and_working_param(splited_param_current_rank, param)
 
         return params_current_rank
 
     ###########################
     # Backward Reduction Hook #
     ###########################
 
-    @staticmethod
-    def grad_handler(
-        param: nn.Parameter,
-        group_id: int,
-        bucket_store: BucketStore,
-        param_store: ParameterStore,
-        grad_store: GradientStore,
-    ):
+    def _grad_handler(self, group_id, param):
         # if run with no_sync context, would not sync grad when backward
-        if grad_store.require_grad_sync:
-            LowLevelZeroOptimizer.add_to_bucket(param, group_id, bucket_store, param_store, grad_store)
+        if self.require_grad_sync:
+            self._add_to_bucket(param, group_id)
 
     def _attach_reduction_hook(self):
         # we iterate over the working params
         # on each param, we register a hook to its AccumulateGrad object
         for group_id in range(self.num_param_groups):
             param_group = self._working_param_groups[group_id]
             for param in param_group:
                 if param.requires_grad:
-                    param._grad_handle = param.register_post_accumulate_grad_hook(
-                        partial(
-                            LowLevelZeroOptimizer.grad_handler,
-                            group_id=group_id,
-                            bucket_store=self._bucket_store,
-                            param_store=self._param_store,
-                            grad_store=self._grad_store,
-                        )
-                    )
+                    param.register_post_accumulate_grad_hook(partial(self._grad_handler, group_id))
 
     #######################
     # Reduction Functions #
     #######################
-    @staticmethod
-    def run_reduction(bucket_store: BucketStore, grad_store: GradientStore):
-        if bucket_store.num_elements_in_bucket() > 0:
-            bucket_store.build_grad_in_bucket()
-            if bucket_store.moe_extra_dp_pg is None:
-                flat_grads = bucket_store.get_flatten_grad()
-                flat_grads /= bucket_store.zero_world_size
+
+    def _run_reduction(self):
+        if self._bucket_store.num_elements_in_bucket() > 0:
+            self._bucket_store.build_grad_in_bucket()
+
+            if self.moe_extra_dp_pg is None:
+                flat_grads = self._bucket_store.get_flatten_grad()
+                flat_grads /= self._world_size
             else:
                 # record moe and non moe param
                 moe_list = []
-                for param in bucket_store._param_list:
+                for param in self._bucket_store._param_list:
                     moe_list.append(is_moe_tensor(param))
 
                 # divide them into different groups
                 moe_grad_list = []
                 non_moe_grad_list = []
-                for grad_list in bucket_store._grad_in_bucket.values():
+                for grad_list in self._bucket_store._grad_in_bucket.values():
                     non_moe_cur_grad = []
                     moe_cur_grad = []
                     for i in range(len(grad_list)):
                         if moe_list[i] == True:
                             moe_cur_grad.append(grad_list[i])
                         else:
                             non_moe_cur_grad.append(grad_list[i])
@@ -321,249 +328,218 @@
                         non_moe_grad_list.append(non_moe_cur_grad)
 
                 if len(non_moe_grad_list) > 0:
                     non_moe_flat_grads = []
                     for grad_list in non_moe_grad_list:
                         non_moe_flat_grads.append(_flatten_dense_tensors(grad_list))
                     non_moe_flat_grads = _flatten_dense_tensors(non_moe_flat_grads)
-                    non_moe_flat_grads /= bucket_store.zero_world_size
+                    non_moe_flat_grads /= self._world_size
 
                 if len(moe_grad_list) > 0:
                     moe_flat_grads = []
                     for grad_list in moe_grad_list:
                         moe_flat_grads.append(_flatten_dense_tensors(grad_list))
                     moe_flat_grads = _flatten_dense_tensors(moe_flat_grads)
 
             # ready to add other tensors to bucket
-            bucket_store.reset_num_elements_in_bucket()
+            self._bucket_store.reset_num_elements_in_bucket()
 
-            if bucket_store._overlap_communication:
-                stream = bucket_store.comm_stream
+            if self._overlap_communication:
+                stream = self._comm_stream
                 # in case of the memory being reused in the default stream
-                if bucket_store.moe_extra_dp_pg is None:
+                if self.moe_extra_dp_pg is None:
                     flat_grads.record_stream(stream)
                 else:
                     if len(non_moe_grad_list) > 0:
                         non_moe_flat_grads.record_stream(stream)
                     if len(moe_grad_list) > 0:
                         moe_flat_grads.record_stream(stream)
                 # waiting for ops in the default stream finishing
                 stream.wait_stream(get_accelerator().current_stream())
             else:
                 stream = get_accelerator().current_stream()
 
             with get_accelerator().stream(stream):
-                group_id = bucket_store.current_group_id
+                group_id = self._bucket_store.current_group_id
 
-                if bucket_store.moe_extra_dp_pg is None:
+                if self.moe_extra_dp_pg is None:
                     grad_dtype = flat_grads.dtype
-                    if bucket_store._communication_dtype is not None:
-                        flat_grads = flat_grads.to(bucket_store._communication_dtype)
+                    if self._communication_dtype is not None:
+                        flat_grads = flat_grads.to(self._communication_dtype)
 
-                if not grad_store._partition_grads:
-                    if bucket_store.moe_extra_dp_pg is None:
-                        dist.all_reduce(flat_grads, group=bucket_store.torch_pg)
+                if not self._partition_grads:
+                    if self.moe_extra_dp_pg is None:
+                        dist.all_reduce(flat_grads, group=self.dp_pg)
                         if flat_grads.dtype != grad_dtype:
                             flat_grads = flat_grads.to(grad_dtype)
 
-                        flat_grads_per_rank = flat_grads.split(flat_grads.numel() // bucket_store.zero_world_size)
-                        grad_in_bucket = bucket_store.get_grad()
-                        LowLevelZeroOptimizer.update_unpartitoned_grad(
-                            bucket_store, grad_store, grad_in_bucket.values(), flat_grads_per_rank, group_id
-                        )
+                        flat_grads_per_rank = flat_grads.split(flat_grads.numel() // self._world_size)
+                        grad_in_bucket = self._bucket_store.get_grad()
+                        self._update_unpartitoned_grad(grad_in_bucket.values(), flat_grads_per_rank, group_id)
 
                     # sync extra zero group
                     else:
                         # sync non moe param in global dp group
                         if len(non_moe_grad_list) > 0:
-                            dist.all_reduce(non_moe_flat_grads, group=bucket_store.torch_pg)
+                            dist.all_reduce(non_moe_flat_grads, group=self.dp_pg)
                             flat_grads_per_rank = non_moe_flat_grads.split(
-                                non_moe_flat_grads.numel() // bucket_store.zero_world_size
-                            )
-                            LowLevelZeroOptimizer.update_unpartitoned_grad(
-                                bucket_store, grad_store, non_moe_grad_list, flat_grads_per_rank, group_id
+                                non_moe_flat_grads.numel() // self._world_size
                             )
+                            self._update_unpartitoned_grad(non_moe_grad_list, flat_grads_per_rank, group_id)
 
                         # sync moe param only in zero group
                         if len(moe_grad_list) > 0:
-                            dist.all_reduce(moe_flat_grads, group=bucket_store.moe_extra_dp_pg)
-                            flat_grads_per_rank = moe_flat_grads.split(
-                                moe_flat_grads.numel() // bucket_store.zero_world_size
-                            )
-                            LowLevelZeroOptimizer.update_unpartitoned_grad(
-                                bucket_store, grad_store, moe_grad_list, flat_grads_per_rank, group_id
-                            )
+                            dist.all_reduce(moe_flat_grads, group=self.moe_extra_dp_pg)
+                            flat_grads_per_rank = moe_flat_grads.split(moe_flat_grads.numel() // self._world_size)
+                            self._update_unpartitoned_grad(moe_grad_list, flat_grads_per_rank, group_id)
 
                 else:
-                    if bucket_store.moe_extra_dp_pg is None:
-                        flat_grads_list = list(flat_grads.split(len(flat_grads) // bucket_store.zero_world_size))
-                        received_grad = torch.zeros_like(flat_grads_list[0])
-                        dist.reduce_scatter(received_grad, flat_grads_list, group=bucket_store.torch_pg)
-
-                        if received_grad.dtype != grad_dtype:
-                            received_grad = received_grad.to(grad_dtype)
-
-                        grad_in_bucket_current_rank = bucket_store.get_grad()[bucket_store.zero_local_rank]
-                        LowLevelZeroOptimizer.update_partitoned_grad(
-                            bucket_store, grad_store, grad_in_bucket_current_rank, received_grad, group_id, 1
-                        )
+                    if self.moe_extra_dp_pg is None:
+                        flat_grads_list = list(flat_grads.split(len(flat_grads) // self._world_size))
+                        recieved_grad = torch.zeros_like(flat_grads_list[0])
+                        dist.reduce_scatter(recieved_grad, flat_grads_list, group=self.dp_pg)
+
+                        if recieved_grad.dtype != grad_dtype:
+                            recieved_grad = recieved_grad.to(grad_dtype)
+
+                        grad_in_bucket_current_rank = self._bucket_store.get_grad()[self._local_rank]
+                        self._update_partitoned_grad(grad_in_bucket_current_rank, recieved_grad, group_id, 1)
                     else:
                         # categorize moe and non moe param
-                        grad_in_bucket_current_rank = bucket_store.get_grad()[bucket_store.zero_local_rank]
+                        grad_in_bucket_current_rank = self._bucket_store.get_grad()[self._local_rank]
                         moe_grad_in_bucket_current_rank = []
                         non_moe_grad_in_bucket_current_rank = []
                         for idx, grad in enumerate(grad_in_bucket_current_rank):
                             if moe_list[idx] == True:
                                 moe_grad_in_bucket_current_rank.append(grad)
                             else:
                                 non_moe_grad_in_bucket_current_rank.append(grad)
 
                         if len(non_moe_grad_list) > 0:
                             flat_grads_list = list(
-                                non_moe_flat_grads.split(len(non_moe_flat_grads) // bucket_store.zero_world_size)
+                                non_moe_flat_grads.split(len(non_moe_flat_grads) // self._world_size)
                             )
-                            received_grad = torch.zeros_like(flat_grads_list[0])
-                            dist.reduce_scatter(received_grad, flat_grads_list, group=bucket_store.torch_pg)
-                            LowLevelZeroOptimizer.update_partitoned_grad(
-                                bucket_store,
-                                grad_store,
+                            recieved_grad = torch.zeros_like(flat_grads_list[0])
+                            dist.reduce_scatter(recieved_grad, flat_grads_list, group=self.dp_pg)
+                            self._update_partitoned_grad(
                                 non_moe_grad_in_bucket_current_rank,
-                                received_grad,
+                                recieved_grad,
                                 group_id,
                                 1,
                             )
 
                         if len(moe_grad_list) > 0:
                             flat_grads_list = list(
-                                moe_flat_grads.split(len(moe_flat_grads) // bucket_store.moe_extra_dp_pg_size)
+                                moe_flat_grads.split(len(moe_flat_grads) // self.moe_extra_dp_pg_size)
                             )
-                            received_grad = torch.zeros_like(flat_grads_list[0])
+                            recieved_grad = torch.zeros_like(flat_grads_list[0])
                             dist.reduce_scatter(
-                                received_grad,
+                                recieved_grad,
                                 flat_grads_list,
-                                group=bucket_store.moe_extra_dp_pg,
+                                group=self.moe_extra_dp_pg,
                             )
-                            param_slice = bucket_store.zero_world_size // bucket_store.moe_extra_dp_pg_size
-                            received_grad = list(received_grad.split(len(received_grad) // param_slice))
-                            for split_recieved_grad in received_grad:
+                            param_slice = self._world_size // self.moe_extra_dp_pg_size
+                            recieved_grad = list(recieved_grad.split(len(recieved_grad) // param_slice))
+                            for split_recieved_grad in recieved_grad:
                                 split_recieved_grad = _unflatten_dense_tensors(
                                     split_recieved_grad, moe_grad_in_bucket_current_rank
                                 )
                                 for real_grad, grad in zip(split_recieved_grad, moe_grad_in_bucket_current_rank):
-                                    param_id = bucket_store.get_param_id_of_grad(grad)
-                                    LowLevelZeroOptimizer.add_grad(
-                                        grad_store, real_grad, param_slice, group_id, param_id
-                                    )
-
-                bucket_store.reset()
-
-    @staticmethod
-    def update_unpartitoned_grad(
-        bucket_store: BucketStore,
-        grad_store: GradientStore,
-        origin_grad_list: List,
-        flat_grad_list: List,
-        group_id: int,
-    ) -> None:
+                                    param_id = self._bucket_store.get_param_id_of_grad(grad)
+                                    self._add_grad(real_grad, param_slice, group_id, param_id)
+
+                self._bucket_store.reset()
+
+    def _update_unpartitoned_grad(self, origin_grad_list: List, flat_grad_list: List, group_id: int) -> None:
         for rank, grad_list in enumerate(origin_grad_list):
             sync_tensor(flat_grad_list[rank], grad_list)
             for grad in grad_list:
-                param_id = bucket_store.get_param_id_of_grad(grad)
-                LowLevelZeroOptimizer.add_grad(grad_store, grad, bucket_store.zero_world_size, group_id, param_id, rank)
+                param_id = self._bucket_store.get_param_id_of_grad(grad)
+                self._add_grad(grad, self._world_size, group_id, param_id, rank)
 
-    @staticmethod
-    def update_partitoned_grad(
-        bucket_store: BucketStore,
-        grad_store: GradientStore,
+    def _update_partitoned_grad(
+        self,
         origin_grad_list: List,
         flat_grad: torch.Tensor,
         group_id: int,
         partition_num: int,
     ) -> None:
         sync_tensor(flat_grad, origin_grad_list)
         for grad in origin_grad_list:
-            param_id = bucket_store.get_param_id_of_grad(grad)
-            LowLevelZeroOptimizer.add_grad(grad_store, grad, partition_num, group_id, param_id)
+            param_id = self._bucket_store.get_param_id_of_grad(grad)
+            self._add_grad(grad, partition_num, group_id, param_id)
 
-    @staticmethod
-    def add_grad(
-        grad_store: GradientStore,
+    def _add_grad(
+        self,
         grad: torch.Tensor,
         partition_num: int,
         group_id: int,
         param_id: int,
         rank: int = 0,
     ) -> None:
-        if len(grad_store.get_partitioned_gradients_by_param_id(group_id, param_id)) < partition_num:
-            grad_store.append_gradients_by_param_id(grad, group_id, param_id)
+        if len(self._grad_store.get_partitioned_gradients_by_param_id(group_id, param_id)) < partition_num:
+            self._grad_store.append_gradients_by_param_id(grad, group_id, param_id)
         else:
-            grad_store.add_gradients_by_param_id(grad, rank, group_id, param_id)
+            self._grad_store.add_gradients_by_param_id(grad, rank, group_id, param_id)
 
-    @staticmethod
-    def add_to_bucket(
-        param: nn.Parameter,
-        group_id: int,
-        bucket_store: BucketStore,
-        param_store: ParameterStore,
-        grad_store: GradientStore,
-    ):
+    def _add_to_bucket(self, param, group_id):
         param_size = param.numel()
 
         # check if the bucket is full
         # if full, will reduce the grads already in the bucket
         # or got a grad of param from another group
         # after reduction, the bucket will be empty
         if (
-            bucket_store.num_elements_in_bucket() + param_size > bucket_store.reduce_bucket_size
-            or group_id != bucket_store.current_group_id
+            self._bucket_store.num_elements_in_bucket() + param_size > self._reduce_bucket_size
+            or group_id != self._bucket_store.current_group_id
         ):
-            LowLevelZeroOptimizer.run_reduction(bucket_store, grad_store)
+            self._run_reduction()
 
-        padding_size = param_store.get_param_padding_size(param)
-        bucket_store.add_param_grad(group_id, param, padding_size)
+        padding_size = self._param_store.get_param_padding_size(param)
+        self._bucket_store.add_param_grad(group_id, param, padding_size)
 
     ################################
     # torch.optim.Optimizer methods
     ################################
 
     def backward(self, loss, retain_graph=False):
         assert not (
-            self._grad_store._partition_grads and not self._grad_store.require_grad_sync
+            self._partition_grads and not self.require_grad_sync
         ), "ZeRO2(partition_grads) and no_sync are not compatible"
 
         if self.mixed_precision_mixin is not None:
             loss = self.mixed_precision_mixin.pre_backward(loss)
 
         loss.backward(retain_graph=retain_graph)
 
-        if not self._grad_store.require_grad_sync:
+        if not self.require_grad_sync:
             return
 
-        self._reduce_grad(self._grad_store._partition_grads)
+        self._reduce_grad(self._partition_grads)
 
         # clear reduced grads
-        if self._bucket_store._overlap_communication:
+        if self._overlap_communication:
             get_accelerator().synchronize()
         self.zero_grad()
 
     def backward_by_grad(self, tensor, grad):
         assert not (
-            self._grad_store._partition_grads and not self._grad_store.require_grad_sync
+            self._partition_grads and not self.require_grad_sync
         ), "ZeRO2(partition_grads) and gradient accumulation(no_sync) are not compatible"
 
         if self.mixed_precision_mixin is not None:
             grad = self.mixed_precision_mixin.pre_backward_by_grad(tensor, grad)
         torch.autograd.backward(tensor, grad)
 
-        if not self._grad_store.require_grad_sync:
+        if not self.require_grad_sync:
             return
-        self._reduce_grad(self._grad_store._partition_grads)
+        self._reduce_grad(self._partition_grads)
 
         # clear reduced grads
-        if self._bucket_store._overlap_communication:
+        if self._overlap_communication:
             get_accelerator().synchronize()
 
         self.zero_grad()
 
     def zero_grad(self, set_to_none=True):
         """
         Set parameter gradients to zero. If set_to_none = True, gradient
@@ -586,15 +562,15 @@
 
     ####################
     # Update Parameter #
     ####################
 
     def step(self, closure=None):
         assert closure is None, "closure is not supported by step()"
-        if not self._grad_store.require_grad_sync:
+        if not self.require_grad_sync:
             return
 
         if self.mixed_precision_mixin is not None and self.mixed_precision_mixin.should_skip_step():
             self._grad_store.reset_all_gradients()
             if self._verbose:
                 self._logger.info(f"Found overflow. Skip step")
             self.zero_grad()
@@ -605,37 +581,35 @@
         norm_groups = []
 
         # sometimes not all params are 'really' working
         # for instance, when layer drop, the dropped layer has no grad
         # and should not be updated
         real_working_params = dict()
         real_master_params = dict()
-        grad_index = 0 if self._grad_store._partition_grads else self._bucket_store.zero_local_rank
+        grad_index = 0 if self._partition_grads else self._local_rank
         for group_id in range(self.num_param_groups):
             master_params = self._master_param_groups_of_current_rank[group_id]
             real_working_params[group_id] = []
             real_master_params[group_id] = []
             for splited_param in master_params:
                 working_param = self._param_store.master_to_working_param[id(splited_param)]
                 # if a working param requires grad and has no grad
                 # it is not 'really' working, e.g. the droped layer
                 # else the splited grad should be attached to the splited param
                 grads = self._grad_store.get_partitioned_gradients_by_param_id(group_id, id(working_param))
                 if len(grads) > 0:
                     # moe hybrid zero
-                    if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
+                    if self.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
                         real_working_params[group_id].append(working_param)
-                        if self._grad_store._partition_grads:
+                        if self._partition_grads:
                             grad = grads
                         else:
-                            param_slice = self._bucket_store.zero_world_size // self._bucket_store.moe_extra_dp_pg_size
+                            param_slice = self._world_size // self.moe_extra_dp_pg_size
                             grad = grads[
-                                self._bucket_store.moe_extra_dp_pg_rank
-                                * param_slice : (self._bucket_store.moe_extra_dp_pg_rank + 1)
-                                * param_slice
+                                self.moe_extra_dp_pg_rank * param_slice : (self.moe_extra_dp_pg_rank + 1) * param_slice
                             ]
                         grad = flatten(grad)
                     else:
                         real_working_params[group_id].append(working_param)
                         grad = grads[grad_index]
                     # no need to copy fp32 grad if master_weights is False
                     if self._master_weights:
@@ -696,33 +670,33 @@
 
         # update working partition updated by the current rank
         device = get_accelerator().get_current_device()
         for group_id in range(self.num_param_groups):
             master_working_param = self.optim.param_groups[group_id]["params"]
             for idx, splited_param in enumerate(master_working_param):
                 working_param = real_working_params[group_id][idx]
-                if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
+                if self.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
                     all_splited_param = [
                         torch.zeros(splited_param.shape, device=device, dtype=self._dtype)
-                        for _ in range(self._bucket_store.moe_extra_dp_pg_size)
+                        for _ in range(self.moe_extra_dp_pg_size)
                     ]
                     dist.all_gather(
                         all_splited_param,
                         splited_param.to(device).to(self._dtype),
-                        group=self._bucket_store.moe_extra_dp_pg,
+                        group=self.moe_extra_dp_pg,
                     )
                 else:
                     all_splited_param = [
                         torch.zeros(splited_param.shape, device=device, dtype=self._dtype)
-                        for _ in range(self._bucket_store.zero_world_size)
+                        for _ in range(self._world_size)
                     ]
                     dist.all_gather(
                         all_splited_param,
                         splited_param.to(device).to(self._dtype),
-                        group=self._bucket_store.torch_pg,
+                        group=self.dp_pg,
                     )
                 working_param.data.copy_(flatten(all_splited_param)[: working_param.numel()].reshape_as(working_param))
             self.optim.param_groups[group_id]["params"] = self._master_param_groups_of_current_rank[group_id]
 
     def _compute_grad_norm(self, gradients: List[Tensor], norm_type: int = 2) -> float:
         r"""
         Compute and return the gradient norm for gradient clipping.
@@ -742,15 +716,15 @@
         if norm_type == inf:
             total_norm = max(grad.data.abs().max() for grad in gradients)
             total_norm_cuda = torch.tensor(
                 [float(total_norm)],
                 device=get_accelerator().get_current_device(),
                 dtype=torch.float,
             )
-            dist.all_reduce(total_norm_cuda, op=torch.distributed.ReduceOp.MAX, group=self._bucket_store.torch_pg)
+            dist.all_reduce(total_norm_cuda, op=torch.distributed.ReduceOp.MAX, group=self.dp_pg)
             total_norm = total_norm_cuda.item()
 
         else:
             total_norm_exponentiated = 0.0
             for grad in gradients:
                 grad_norm_exponentiated = grad.data.double().norm(norm_type) ** norm_type
                 total_norm_exponentiated += grad_norm_exponentiated
@@ -760,15 +734,15 @@
                 [float(total_norm_exponentiated)],
                 device=get_accelerator().get_current_device(),
                 dtype=torch.float,
             )
             torch.distributed.all_reduce(
                 total_norm_exponentiated_cuda,
                 op=torch.distributed.ReduceOp.SUM,
-                group=self._bucket_store.torch_pg,
+                group=self.dp_pg,
             )
             total_norm = total_norm_exponentiated_cuda.item() ** (1.0 / norm_type)
 
         return total_norm
 
     #############################
     # Mixed Precision Utilities #
@@ -795,41 +769,35 @@
 
     # this method is used to sync gradient manually
     def _sync_grad(self):
         for group_id in range(self.num_param_groups):
             param_group = self._working_param_groups[group_id]
             for param in param_group:
                 if param.requires_grad and param.grad is not None:
-                    LowLevelZeroOptimizer.add_to_bucket(
-                        param,
-                        group_id,
-                        self._bucket_store,
-                        self._param_store,
-                        self._grad_store,
-                    )
+                    self._add_to_bucket(param, group_id)
 
-        LowLevelZeroOptimizer.run_reduction(self._bucket_store, self._grad_store)
+        self._run_reduction()
 
     def _reduce_grad(self, partition_grad):
         # if not overlapping communication (no reduction hook is attached) when zero1
         # we need to manually reduce these gradients
-        if not partition_grad and not self._bucket_store._overlap_communication:
+        if not partition_grad and not self._overlap_communication:
             self._sync_grad()
         else:
-            LowLevelZeroOptimizer.run_reduction(self._bucket_store, self._grad_store)
+            self._run_reduction()
 
     # this context comes from pytorch DDP
     @contextmanager
     def no_sync(self):
-        old_require_grad_sync = self._grad_store.require_grad_sync
-        self._grad_store.require_grad_sync = False
+        old_require_grad_sync = self.require_grad_sync
+        self.require_grad_sync = False
         try:
             yield
         finally:
-            self._grad_store.require_grad_sync = old_require_grad_sync
+            self.require_grad_sync = old_require_grad_sync
 
     ##############
     # State Dict #
     ##############
 
     def _pack_state(self, state: Dict) -> Dict:
         # comes from pytorch optimizer.state_dict()
@@ -861,26 +829,24 @@
         zero_state = dict()
         device = get_accelerator().get_current_device()
         for param, state in self.optim.state.items():
             zero_state[param] = copy.deepcopy(state)
             for k, v in state.items():
                 if isinstance(v, torch.Tensor) and k != "step":
                     working_param = self._param_store.master_to_working_param[id(param)]
-                    if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(v):
+                    if self.moe_extra_dp_pg is not None and is_moe_tensor(v):
                         gather_tensor = [
-                            torch.zeros(v.shape, device=device, dtype=v.dtype)
-                            for _ in range(self._bucket_store.moe_extra_dp_pg_size)
+                            torch.zeros(v.shape, device=device, dtype=v.dtype) for _ in range(self.moe_extra_dp_pg_size)
                         ]
-                        dist.all_gather(gather_tensor, v.to(device), group=self._bucket_store.moe_extra_dp_pg)
+                        dist.all_gather(gather_tensor, v.to(device), group=self.moe_extra_dp_pg)
                     else:
                         gather_tensor = [
-                            torch.zeros(v.shape, device=device, dtype=v.dtype)
-                            for _ in range(self._bucket_store.zero_world_size)
+                            torch.zeros(v.shape, device=device, dtype=v.dtype) for _ in range(self._world_size)
                         ]
-                        dist.all_gather(gather_tensor, v.to(device), group=self._bucket_store.torch_pg)
+                        dist.all_gather(gather_tensor, v.to(device), group=self.dp_pg)
                     param_state = (
                         torch.stack(gather_tensor).view(-1)[: working_param.numel()].reshape_as(working_param).cpu()
                     )
                     zero_state[param][k] = param_state
 
         states_dict = self._pack_state(zero_state)
 
@@ -892,31 +858,25 @@
         Args:
             state_dict (dict): A pytorch form state_dict
         """
         zero_state_dict = copy.deepcopy(state_dict)
         for param_idx, state in zero_state_dict["state"].items():
             for k, v in state.items():
                 if isinstance(v, torch.Tensor) and k != "step":
-                    padding_size = (
-                        self._bucket_store.zero_world_size - v.numel() % self._bucket_store.zero_world_size
-                    ) % self._bucket_store.zero_world_size
+                    padding_size = (self._world_size - v.numel() % self._world_size) % self._world_size
                     with torch.no_grad():
                         v = v.flatten()
                         if padding_size > 0:
                             v = torch.nn.functional.pad(v, [0, padding_size])
-                        if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(v):
-                            v_list = v.split(v.numel() // self._bucket_store.moe_extra_dp_pg_size)
-                            zero_state_dict["state"][param_idx][k] = (
-                                v_list[self._bucket_store.moe_extra_dp_pg_rank].detach().clone()
-                            )
+                        if self.moe_extra_dp_pg is not None and is_moe_tensor(v):
+                            v_list = v.split(v.numel() // self.moe_extra_dp_pg_size)
+                            zero_state_dict["state"][param_idx][k] = v_list[self.moe_extra_dp_pg_rank].detach().clone()
                         else:
-                            v_list = v.split(v.numel() // self._bucket_store.zero_world_size)
-                            zero_state_dict["state"][param_idx][k] = (
-                                v_list[self._bucket_store.zero_local_rank].detach().clone()
-                            )
+                            v_list = v.split(v.numel() // self._world_size)
+                            zero_state_dict["state"][param_idx][k] = v_list[self._local_rank].detach().clone()
 
         self.optim.load_state_dict(zero_state_dict)
 
     def state_dict_shard(self, max_shard_size: int = 1024) -> Iterator[Tuple[Dict, int]]:
         """Returns dictionaries containing a whole state of the module one by one. The max size of dictionary shard is specified by ``max_shard_size``.
            Only include the 'state' in state_dict.
 
@@ -940,26 +900,24 @@
                 if (group_id + 1) * len(pg) < param_idx:
                     continue
                 master_param = pg[param_idx - (group_id) * len(pg)]
                 working_param = self._param_store.master_to_working_param[id(master_param)]
 
             for k, v in states.items():
                 if isinstance(v, torch.Tensor) and k != "step":
-                    if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(v):
+                    if self.moe_extra_dp_pg is not None and is_moe_tensor(v):
                         state_tensor = [
-                            torch.zeros(v.shape, device=device, dtype=v.dtype)
-                            for _ in range(self._bucket_store.moe_extra_dp_pg_size)
+                            torch.zeros(v.shape, device=device, dtype=v.dtype) for _ in range(self.moe_extra_dp_pg_size)
                         ]
-                        dist.all_gather(state_tensor, v.to(device), group=self._bucket_store.moe_extra_dp_pg)
+                        dist.all_gather(state_tensor, v.to(device), group=self.moe_extra_dp_pg)
                     else:
                         state_tensor = [
-                            torch.zeros(v.shape, device=device, dtype=v.dtype)
-                            for _ in range(self._bucket_store.zero_world_size)
+                            torch.zeros(v.shape, device=device, dtype=v.dtype) for _ in range(self._world_size)
                         ]
-                        dist.all_gather(state_tensor, v.to(device), group=self._bucket_store.torch_pg)
+                        dist.all_gather(state_tensor, v.to(device), group=self.dp_pg)
                     state_tensor = (
                         torch.stack(state_tensor).view(-1)[: working_param.numel()].reshape_as(working_param).cpu()
                     )
                     current_block_size += state_tensor.numel()
                     current_block[k] = state_tensor
 
             if ret_block_size + current_block_size > max_shard_size and len(ret_block) > 0:
@@ -982,44 +940,25 @@
             p_id = id(p)
             if p_id in self._param_store.working_to_master_param:
                 master_param = self._param_store.working_to_master_param[p_id]
                 padding_size = self._param_store.get_param_padding_size(p)
                 working_param = p.data.view(-1)
                 if padding_size > 0:
                     working_param = torch.nn.functional.pad(working_param, [0, padding_size])
-                if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(p):
+                if self.moe_extra_dp_pg is not None and is_moe_tensor(p):
                     master_param.copy_(working_param.chunk(self.extra_dp_pg_size)[self.extra_dp_pg_rank])
                 else:
-                    master_param.copy_(
-                        working_param.chunk(self._bucket_store.zero_world_size)[self._bucket_store.zero_local_rank]
-                    )
+                    master_param.copy_(working_param.chunk(self._world_size)[self._local_rank])
         if hasattr(self, "master_moe_params"):
             for master_moe_param, working_moe_param in zip(self.master_moe_params, self.working_moe_params):
                 master_moe_param.copy_(working_moe_param)
 
-    def remove_hooks(self) -> None:
-        """remove the registered hooks
-
-        Args:
-            plugin (LowLevelZeroPlugin): the plugin to bound this method.
-        """
-        for group_id in range(self.num_param_groups):
-            param_group = self._working_param_groups[group_id]
-            for param in param_group:
-                if param.requires_grad:
-                    assert hasattr(param, "_grad_handle")
-                    param._grad_handle.remove()
-                    delattr(param, "_grad_handle")
-
     def get_working_to_master_map(self) -> Dict[int, torch.Tensor]:
         return self._param_store.working_to_master_param
 
     def get_master_to_working_map(self) -> Dict[int, torch.Tensor]:
         if hasattr(self, "moe_master_to_working_map"):
             return {
                 **self._param_store.master_to_working_param,
                 **self.moe_master_to_working_map,
             }
         return self._param_store.master_to_working_param
-
-    def get_param_padding_map(self) -> Dict[int, torch.Tensor]:
-        return self._param_store.get_padding_map()
```

### Comparing `colossalai-nightly-2024.5.25/colossalai/zero/wrapper.py` & `colossalai-nightly-2024.5.4/colossalai/zero/wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/PKG-INFO` & `colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: colossalai-nightly
-Version: 2024.5.25
+Version: 2024.5.4
 Summary: An integrated large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org
 License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io
@@ -32,15 +32,14 @@
         
         
            | [English](README.md) | [中文](docs/README-zh-Hans.md) |
         
         </div>
         
         ## Latest News
-        * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)
         * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
         * [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
         * [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
         * [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
         * [2024/03] [Open-Sora：Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
         * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
         * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
@@ -83,17 +82,19 @@
              <li><a href="#GPT-2-Single">GPT-2</a></li>
              <li><a href="#PaLM-Single">PaLM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Inference">Inference</a>
            <ul>
-             <li><a href="#Colossal-Inference">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>
              <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
              <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
+             <li><a href="#GPT-3-Inference">GPT-3</a></li>
+             <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
+             <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Installation">Installation</a>
            <ul>
              <li><a href="#PyPI">PyPI</a></li>
              <li><a href="#Install-From-Source">Install From Source</a></li>
@@ -383,52 +384,56 @@
         
         - 34x larger model size on the same hardware
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         
         ## Inference
-        ### Colossal-Inference
-        <p align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png" width=1000/>
-        </p>
-        
-        <p align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png" width=1000/>
-        </p>
-        
-         - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.
-        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)
-        [[blog]](https://hpc-ai.com/blog/colossal-inference)
-        
         ### Grok-1
         <p id="Grok-1" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
         </p>
         
          - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
         
         [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
         [[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
         [[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
         [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
         
-        ### SwiftInfer
         <p id="SwiftInfer" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
         </p>
         
         - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
         
+        <p id="GPT-3-Inference" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference_GPT-3.jpg" width=800/>
+        </p>
+        
+        - [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference acceleration on the same hardware
+        
+        <p id="OPT-Serving" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20serving.png" width=600/>
+        </p>
+        
+        - [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service): Try 175-billion-parameter OPT online services
+        
+        <p id="BLOOM-Inference" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
+        </p>
+        
+        - [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
+        
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Installation
         
         Requirements:
-        - PyTorch >= 2.1
+        - PyTorch >= 1.11 and PyTorch <= 2.1
         - Python >= 3.7
         - CUDA >= 11.0
         - [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
         - Linux OS
         
         If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.5.25 Summary: An
+Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.5.4 Summary: An
 integrated large-scale model training system with efficient parallelization
 techniques Home-page: https://www.colossalai.org License: Apache Software
 License 2.0 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/
 discussions Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/
 issues Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io Project-URL:
 Github, https://github.com/hpcaitech/ColossalAI Description: # Colossal-AI
@@ -22,47 +22,46 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 å¾®ä¿¡-å å¥-green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | [ä¸­æ]
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-
-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference) *
-[2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-
-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/
-open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-
-and-720p-resolution-in-open-source) * [2024/04] [Most cost-effective solutions
-for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://
-hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-
-pretraining-tailored-to-llama3-series) * [2024/03] [314 Billion Parameter Grok-
-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace
-version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-
-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-
-version-is-here) * [2024/03] [Open-Sora: Revealing Complete Model Parameters,
-Training Details, and Everything for Sora-like Video Generation Models](https:/
-/hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-Soraï¼Sora Replication
-Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million]
-(https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference Performance Improved
-by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round
-Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01]
-[Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI
-Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11]
-[Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More
-Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-
-moe-model-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of
-Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large
-Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://
-www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-
-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-
-free-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2
-Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-
-training) * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding]
-(https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-
-funding-to-fuel-team-expansion-and-business-growth) ## Table of Contents
+## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
+Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
+/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
+video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
+effective solutions for inference, fine-tuning and pretraining, tailored to
+LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
+inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
+[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
+Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
+billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
+use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
+Complete Model Parameters, Training Details, and Everything for Sora-like Video
+Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
+Soraï¼Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to
+Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
+Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
+LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
+SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
+USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
+colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
+Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
+enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
+efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
+Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
+Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
+of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
+large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
+[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
+(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
+Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
+tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
+business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
           o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
             _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
@@ -81,18 +80,20 @@
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
-          o _C_o_l_o_s_s_a_l_-_I_n_f_e_r_e_n_c_e_:_ _L_a_r_g_e_ _A_I_ _M_o_d_e_l_s_ _I_n_f_e_r_e_n_c_e_ _S_p_e_e_d_ _D_o_u_b_l_e_d
           o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
+          o _G_P_T_-_3
+          o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
+          o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
           o _I_n_s_t_a_l_l_ _F_r_o_m_ _S_o_u_r_c_e
     * _U_s_e_ _D_o_c_k_e_r
     * _C_o_m_m_u_n_i_t_y
     * _C_o_n_t_r_i_b_u_t_i_n_g
     * _C_i_t_e_ _U_s
@@ -282,66 +283,72 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference ### Colossal-Inference
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                    inference/colossal-inference-v1-1.png]
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                    inference/colossal-inference-v1-2.png]
-- Large AI models inference speed doubled, compared to the offline inference
-performance of vLLM in some cases. [[code]](https://github.com/hpcaitech/
-ColossalAI/tree/main/colossalai/inference) [[blog]](https://hpc-ai.com/blog/
-colossal-inference) ### Grok-1
+## Inference ### Grok-1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/grok-1-inference.jpg]
 - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
 Python + PyTorch + HuggingFace version for Inference. [[code]](https://
 github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
 (https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
 3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
 [HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
 grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
-models/colossalai/grok-1-pytorch/summary) ### SwiftInfer
+models/colossalai/grok-1-pytorch/summary)
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             inference_GPT-3.jpg]
+- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
+acceleration on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             BLOOM%20serving.png]
+- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
+Try 175-billion-parameter OPT online services
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                            BLOOM%20Inference.PNG]
+- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
+Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
+times.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Installation Requirements: - PyTorch >= 2.1 - Python >= 3.7 - CUDA >= 11.0 -
-[NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0
-(V100/RTX20 and higher) - Linux OS If you encounter any problem with
-installation, you may want to raise an [issue](https://github.com/hpcaitech/
-ColossalAI/issues/new/choose) in this repository. ### Install from PyPI You can
-easily install Colossal-AI with the following command. **By default, we do not
-build PyTorch extensions during installation.** ```bash pip install colossalai
-``` **Note: only Linux is supported for now.** However, if you want to build
-the PyTorch extensions during installation, you can set `BUILD_EXT=1`. ```bash
-BUILD_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
-during runtime when you actually need them.** We also keep releasing the
-nightly version to PyPI every week. This allows you to access the unreleased
-features and bug fixes in the main branch. Installation can be made via ```bash
-pip install colossalai-nightly ``` ### Download From Source > The version of
-Colossal-AI will be in line with the main branch of the repository. Feel free
-to raise an issue if you encounter any problems. :) ```shell git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
-install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
-build them during runtime. If you want to install and enable CUDA kernel fusion
-(compulsory installation when using fused optimizer): ```shell BUILD_EXT=1 pip
-install . ``` For Users with CUDA 10.2, you can still build ColossalAI from
-source. However, you need to manually download the cub library and copy it to
-the corresponding directory. ```bash # clone the repository git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # download the cub library
-wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip
-cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ #
-install BUILD_EXT=1 pip install . ```
+## Installation Requirements: - PyTorch >= 1.11 and PyTorch <= 2.1 - Python >=
+3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
+developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
+you encounter any problem with installation, you may want to raise an [issue]
+(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+### Install from PyPI You can easily install Colossal-AI with the following
+command. **By default, we do not build PyTorch extensions during
+installation.** ```bash pip install colossalai ``` **Note: only Linux is
+supported for now.** However, if you want to build the PyTorch extensions
+during installation, you can set `BUILD_EXT=1`. ```bash BUILD_EXT=1 pip install
+colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
+actually need them.** We also keep releasing the nightly version to PyPI every
+week. This allows you to access the unreleased features and bug fixes in the
+main branch. Installation can be made via ```bash pip install colossalai-
+nightly ``` ### Download From Source > The version of Colossal-AI will be in
+line with the main branch of the repository. Feel free to raise an issue if you
+encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
+ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
+we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
+If you want to install and enable CUDA kernel fusion (compulsory installation
+when using fused optimizer): ```shell BUILD_EXT=1 pip install . ``` For Users
+with CUDA 10.2, you can still build ColossalAI from source. However, you need
+to manually download the cub library and copy it to the corresponding
+directory. ```bash # clone the repository git clone https://github.com/
+hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
+github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
+1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
+BUILD_EXT=1 pip install . ```
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
```

### Comparing `colossalai-nightly-2024.5.25/colossalai_nightly.egg-info/SOURCES.txt` & `colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/SOURCES.txt`

 * *Files 5% similar despite different names*

```diff
@@ -271,141 +271,113 @@
 colossalai/fx/tracer/meta_patch/patched_module/convolution.py
 colossalai/fx/tracer/meta_patch/patched_module/embedding.py
 colossalai/fx/tracer/meta_patch/patched_module/linear.py
 colossalai/fx/tracer/meta_patch/patched_module/normalization.py
 colossalai/fx/tracer/meta_patch/patched_module/pooling.py
 colossalai/fx/tracer/meta_patch/patched_module/rnn.py
 colossalai/inference/__init__.py
-colossalai/inference/batch_bucket.py
-colossalai/inference/config.py
-colossalai/inference/flash_decoding_utils.py
-colossalai/inference/graph_runner.py
-colossalai/inference/logit_processors.py
-colossalai/inference/sampler.py
-colossalai/inference/struct.py
-colossalai/inference/utils.py
-colossalai/inference/core/__init__.py
-colossalai/inference/core/async_engine.py
-colossalai/inference/core/engine.py
-colossalai/inference/core/plugin.py
-colossalai/inference/core/request_handler.py
-colossalai/inference/core/rpc_engine.py
+colossalai/inference/engine/__init__.py
+colossalai/inference/engine/engine.py
+colossalai/inference/engine/microbatch_manager.py
+colossalai/inference/engine/modeling/__init__.py
+colossalai/inference/engine/modeling/_utils.py
+colossalai/inference/engine/modeling/bloom.py
+colossalai/inference/engine/modeling/chatglm2.py
+colossalai/inference/engine/modeling/llama.py
+colossalai/inference/engine/policies/__init__.py
+colossalai/inference/engine/policies/bloom.py
+colossalai/inference/engine/policies/chatglm2.py
+colossalai/inference/engine/policies/llama.py
 colossalai/inference/kv_cache/__init__.py
-colossalai/inference/kv_cache/block_cache.py
+colossalai/inference/kv_cache/batch_infer_state.py
 colossalai/inference/kv_cache/kvcache_manager.py
-colossalai/inference/modeling/__init__.py
-colossalai/inference/modeling/layers/__init__.py
-colossalai/inference/modeling/layers/attention.py
-colossalai/inference/modeling/layers/baichuan_tp_linear.py
-colossalai/inference/modeling/models/__init__.py
-colossalai/inference/modeling/models/glide_llama.py
-colossalai/inference/modeling/models/nopadding_baichuan.py
-colossalai/inference/modeling/models/nopadding_llama.py
-colossalai/inference/modeling/policy/__init__.py
-colossalai/inference/modeling/policy/glide_llama.py
-colossalai/inference/modeling/policy/nopadding_baichuan.py
-colossalai/inference/modeling/policy/nopadding_llama.py
-colossalai/inference/server/__init__.py
-colossalai/inference/server/api_server.py
-colossalai/inference/server/chat_service.py
-colossalai/inference/server/completion_service.py
-colossalai/inference/server/utils.py
-colossalai/inference/spec/__init__.py
-colossalai/inference/spec/drafter.py
-colossalai/inference/spec/struct.py
+colossalai/inference/quant/__init__.py
+colossalai/inference/quant/gptq/__init__.py
+colossalai/inference/quant/gptq/gptq_manager.py
+colossalai/inference/quant/gptq/cai_gptq/__init__.py
+colossalai/inference/quant/gptq/cai_gptq/cai_quant_linear.py
+colossalai/inference/quant/gptq/cai_gptq/gptq_op.py
+colossalai/inference/quant/smoothquant/__init__.py
+colossalai/inference/quant/smoothquant/models/__init__.py
+colossalai/inference/quant/smoothquant/models/base_model.py
+colossalai/inference/quant/smoothquant/models/linear.py
+colossalai/inference/quant/smoothquant/models/llama.py
+colossalai/inference/quant/smoothquant/models/parallel_linear.py
 colossalai/interface/__init__.py
 colossalai/interface/model.py
 colossalai/interface/optimizer.py
 colossalai/interface/pretrained.py
 colossalai/kernel/__init__.py
 colossalai/kernel/kernel_loader.py
 colossalai/kernel/extensions/__init__.py
 colossalai/kernel/extensions/base_extension.py
 colossalai/kernel/extensions/cpp_extension.py
 colossalai/kernel/extensions/cuda_extension.py
 colossalai/kernel/extensions/triton_extension.py
 colossalai/kernel/extensions/utils.py
+colossalai/kernel/extensions/cpu_adam/__init__.py
+colossalai/kernel/extensions/cpu_adam/cpu_adam_arm.py
+colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py
 colossalai/kernel/extensions/csrc/__init__.py
-colossalai/kernel/extensions/csrc/common/data_type.h
-colossalai/kernel/extensions/csrc/common/micros.h
-colossalai/kernel/extensions/csrc/common/mp_type_traits.h
-colossalai/kernel/extensions/csrc/common/target.h
-colossalai/kernel/extensions/csrc/common/vec_type_traits.h
-colossalai/kernel/extensions/csrc/funcs/binary_functor.h
-colossalai/kernel/extensions/csrc/funcs/cast_functor.h
-colossalai/kernel/extensions/csrc/funcs/reduce_function.h
-colossalai/kernel/extensions/csrc/funcs/ternary_functor.h
-colossalai/kernel/extensions/csrc/funcs/unary_functor.h
-colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.cpp
-colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.h
-colossalai/kernel/extensions/csrc/kernel/cuda/activation_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/layer_norm_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/moe_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh
-colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu
-colossalai/kernel/extensions/csrc/kernel/cuda/attention/attention_utils.h
-colossalai/kernel/extensions/csrc/kernel/cuda/utils/gpu_launch_config.h
-colossalai/kernel/extensions/csrc/kernel/cuda/utils/micros.h
-colossalai/kernel/extensions/csrc/kernel/cuda/utils/nvgpu_dev_info.h
-colossalai/kernel/extensions/csrc/kernel/cuda/utils/vec_copy.h
-colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.cpp
-colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.h
-colossalai/kernel/extensions/pybind/__init__.py
-colossalai/kernel/extensions/pybind/cpu_adam/__init__.py
-colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_arm.py
-colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_x86.py
-colossalai/kernel/extensions/pybind/flash_attention/__init__.py
-colossalai/kernel/extensions/pybind/flash_attention/flash_attention_dao_cuda.py
-colossalai/kernel/extensions/pybind/flash_attention/flash_attention_npu.py
-colossalai/kernel/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py
-colossalai/kernel/extensions/pybind/inference/__init__.py
-colossalai/kernel/extensions/pybind/inference/inference.cpp
-colossalai/kernel/extensions/pybind/inference/inference_ops_cuda.py
-colossalai/kernel/extensions/pybind/layernorm/__init__.py
-colossalai/kernel/extensions/pybind/layernorm/layer_norm.cpp
-colossalai/kernel/extensions/pybind/layernorm/layernorm_cuda.py
-colossalai/kernel/extensions/pybind/moe/__init__.py
-colossalai/kernel/extensions/pybind/moe/moe.cpp
-colossalai/kernel/extensions/pybind/moe/moe_cuda.py
-colossalai/kernel/extensions/pybind/optimizer/__init__.py
-colossalai/kernel/extensions/pybind/optimizer/fused_optimizer_cuda.py
-colossalai/kernel/extensions/pybind/optimizer/optimizer.cpp
-colossalai/kernel/extensions/pybind/softmax/__init__.py
-colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax.cpp
-colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax_cuda.py
-colossalai/kernel/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp
-colossalai/kernel/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+colossalai/kernel/extensions/csrc/scaled_softmax.py
+colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp
+colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h
+colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp
+colossalai/kernel/extensions/csrc/cuda/compat.h
+colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp
+colossalai/kernel/extensions/csrc/cuda/cpu_adam.h
+colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp
+colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
+colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp
+colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu
+colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu
+colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh
+colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
+colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu
+colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
+colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
+colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp
+colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h
+colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
+colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
+colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
+colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
+colossalai/kernel/extensions/csrc/cuda/type_shim.h
+colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h
+colossalai/kernel/extensions/flash_attention/__init__.py
+colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py
+colossalai/kernel/extensions/flash_attention/flash_attention_npu.py
+colossalai/kernel/extensions/flash_attention/flash_attention_sdpa_cuda.py
+colossalai/kernel/extensions/layernorm/__init__.py
+colossalai/kernel/extensions/layernorm/layernorm_cuda.py
+colossalai/kernel/extensions/moe/__init__.py
+colossalai/kernel/extensions/moe/moe_cuda.py
+colossalai/kernel/extensions/optimizer/__init__.py
+colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py
+colossalai/kernel/extensions/softmax/__init__.py
+colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py
+colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
 colossalai/kernel/jit/__init__.py
 colossalai/kernel/jit/bias_dropout_add.py
 colossalai/kernel/jit/bias_gelu.py
 colossalai/kernel/jit/option.py
 colossalai/kernel/triton/__init__.py
-colossalai/kernel/triton/context_attn_unpad.py
+colossalai/kernel/triton/context_attention.py
+colossalai/kernel/triton/copy_kv_cache_dest.py
+colossalai/kernel/triton/custom_autotune.py
 colossalai/kernel/triton/flash_decoding.py
-colossalai/kernel/triton/fused_rotary_embedding.py
-colossalai/kernel/triton/kvcache_copy.py
+colossalai/kernel/triton/fused_layernorm.py
+colossalai/kernel/triton/gptq_triton.py
+colossalai/kernel/triton/int8_rotary_embedding_kernel.py
 colossalai/kernel/triton/llama_act_combine_kernel.py
-colossalai/kernel/triton/no_pad_rotary_embedding.py
 colossalai/kernel/triton/qkv_matmul_kernel.py
-colossalai/kernel/triton/rms_layernorm.py
-colossalai/kernel/triton/rotary_cache_copy.py
+colossalai/kernel/triton/self_attention_nofusion.py
+colossalai/kernel/triton/smooth_attention.py
 colossalai/kernel/triton/softmax.py
+colossalai/kernel/triton/token_attention_kernel.py
 colossalai/lazy/__init__.py
 colossalai/lazy/construction.py
 colossalai/lazy/lazy_init.py
 colossalai/lazy/pretrained.py
 colossalai/legacy/__init__.py
 colossalai/legacy/constants.py
 colossalai/legacy/core.py
@@ -671,25 +643,18 @@
 colossalai/nn/lr_scheduler/delayed.py
 colossalai/nn/lr_scheduler/linear.py
 colossalai/nn/lr_scheduler/multistep.py
 colossalai/nn/lr_scheduler/onecycle.py
 colossalai/nn/lr_scheduler/poly.py
 colossalai/nn/lr_scheduler/torch.py
 colossalai/nn/optimizer/__init__.py
-colossalai/nn/optimizer/adafactor.py
-colossalai/nn/optimizer/came.py
 colossalai/nn/optimizer/cpu_adam.py
-colossalai/nn/optimizer/distributed_adafactor.py
-colossalai/nn/optimizer/distributed_came.py
-colossalai/nn/optimizer/distributed_galore.py
-colossalai/nn/optimizer/distributed_lamb.py
 colossalai/nn/optimizer/fused_adam.py
 colossalai/nn/optimizer/fused_lamb.py
 colossalai/nn/optimizer/fused_sgd.py
-colossalai/nn/optimizer/galore.py
 colossalai/nn/optimizer/hybrid_adam.py
 colossalai/nn/optimizer/lamb.py
 colossalai/nn/optimizer/lars.py
 colossalai/nn/optimizer/nvme_optimizer.py
 colossalai/pipeline/__init__.py
 colossalai/pipeline/p2p.py
 colossalai/pipeline/stage_manager.py
@@ -723,15 +688,14 @@
 colossalai/shardformer/modeling/falcon.py
 colossalai/shardformer/modeling/gpt2.py
 colossalai/shardformer/modeling/gptj.py
 colossalai/shardformer/modeling/jit.py
 colossalai/shardformer/modeling/llama.py
 colossalai/shardformer/modeling/mistral.py
 colossalai/shardformer/modeling/opt.py
-colossalai/shardformer/modeling/qwen2.py
 colossalai/shardformer/modeling/sam.py
 colossalai/shardformer/modeling/t5.py
 colossalai/shardformer/modeling/vit.py
 colossalai/shardformer/modeling/whisper.py
 colossalai/shardformer/modeling/chatglm2_6b/__init__.py
 colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py
 colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py
@@ -744,15 +708,14 @@
 colossalai/shardformer/policies/chatglm2.py
 colossalai/shardformer/policies/falcon.py
 colossalai/shardformer/policies/gpt2.py
 colossalai/shardformer/policies/gptj.py
 colossalai/shardformer/policies/llama.py
 colossalai/shardformer/policies/mistral.py
 colossalai/shardformer/policies/opt.py
-colossalai/shardformer/policies/qwen2.py
 colossalai/shardformer/policies/sam.py
 colossalai/shardformer/policies/t5.py
 colossalai/shardformer/policies/vit.py
 colossalai/shardformer/policies/whisper.py
 colossalai/shardformer/shard/__init__.py
 colossalai/shardformer/shard/grad_ckpt_config.py
 colossalai/shardformer/shard/shard_config.py
@@ -841,89 +804,68 @@
 examples/language/performance_evaluator.py
 extensions/__init__.py
 extensions/base_extension.py
 extensions/cpp_extension.py
 extensions/cuda_extension.py
 extensions/triton_extension.py
 extensions/utils.py
+extensions/cpu_adam/__init__.py
+extensions/cpu_adam/cpu_adam_arm.py
+extensions/cpu_adam/cpu_adam_x86.py
 extensions/csrc/__init__.py
-extensions/csrc/common/data_type.h
-extensions/csrc/common/micros.h
-extensions/csrc/common/mp_type_traits.h
-extensions/csrc/common/target.h
-extensions/csrc/common/vec_type_traits.h
-extensions/csrc/funcs/binary_functor.h
-extensions/csrc/funcs/cast_functor.h
-extensions/csrc/funcs/reduce_function.h
-extensions/csrc/funcs/ternary_functor.h
-extensions/csrc/funcs/unary_functor.h
-extensions/csrc/kernel/arm/cpu_adam_arm.cpp
-extensions/csrc/kernel/arm/cpu_adam_arm.h
-extensions/csrc/kernel/cuda/activation_kernel.cu
-extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu
-extensions/csrc/kernel/cuda/convert_fp8_kernel.cu
-extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu
-extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu
-extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu
-extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu
-extensions/csrc/kernel/cuda/layer_norm_kernel.cu
-extensions/csrc/kernel/cuda/moe_kernel.cu
-extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu
-extensions/csrc/kernel/cuda/multi_tensor_apply.cuh
-extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu
-extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu
-extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu
-extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu
-extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu
-extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu
-extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu
-extensions/csrc/kernel/cuda/attention/attention_utils.h
-extensions/csrc/kernel/cuda/utils/gpu_launch_config.h
-extensions/csrc/kernel/cuda/utils/micros.h
-extensions/csrc/kernel/cuda/utils/nvgpu_dev_info.h
-extensions/csrc/kernel/cuda/utils/vec_copy.h
-extensions/csrc/kernel/x86/cpu_adam.cpp
-extensions/csrc/kernel/x86/cpu_adam.h
-extensions/pybind/__init__.py
-extensions/pybind/cpu_adam/__init__.py
-extensions/pybind/cpu_adam/cpu_adam_arm.py
-extensions/pybind/cpu_adam/cpu_adam_x86.py
-extensions/pybind/flash_attention/__init__.py
-extensions/pybind/flash_attention/flash_attention_dao_cuda.py
-extensions/pybind/flash_attention/flash_attention_npu.py
-extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py
-extensions/pybind/inference/__init__.py
-extensions/pybind/inference/inference.cpp
-extensions/pybind/inference/inference_ops_cuda.py
-extensions/pybind/layernorm/__init__.py
-extensions/pybind/layernorm/layer_norm.cpp
-extensions/pybind/layernorm/layernorm_cuda.py
-extensions/pybind/moe/__init__.py
-extensions/pybind/moe/moe.cpp
-extensions/pybind/moe/moe_cuda.py
-extensions/pybind/optimizer/__init__.py
-extensions/pybind/optimizer/fused_optimizer_cuda.py
-extensions/pybind/optimizer/optimizer.cpp
-extensions/pybind/softmax/__init__.py
-extensions/pybind/softmax/scaled_masked_softmax.cpp
-extensions/pybind/softmax/scaled_masked_softmax_cuda.py
-extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp
-extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+extensions/csrc/scaled_softmax.py
+extensions/csrc/arm/cpu_adam_arm.cpp
+extensions/csrc/arm/cpu_adam_arm.h
+extensions/csrc/cuda/colossal_C_frontend.cpp
+extensions/csrc/cuda/compat.h
+extensions/csrc/cuda/cpu_adam.cpp
+extensions/csrc/cuda/cpu_adam.h
+extensions/csrc/cuda/layer_norm_cuda.cpp
+extensions/csrc/cuda/layer_norm_cuda_kernel.cu
+extensions/csrc/cuda/moe_cuda.cpp
+extensions/csrc/cuda/moe_cuda_kernel.cu
+extensions/csrc/cuda/multi_tensor_adam.cu
+extensions/csrc/cuda/multi_tensor_apply.cuh
+extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
+extensions/csrc/cuda/multi_tensor_lamb.cu
+extensions/csrc/cuda/multi_tensor_scale_kernel.cu
+extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
+extensions/csrc/cuda/scaled_masked_softmax.cpp
+extensions/csrc/cuda/scaled_masked_softmax.h
+extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
+extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
+extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
+extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
+extensions/csrc/cuda/type_shim.h
+extensions/csrc/cuda/include/block_reduce.h
+extensions/flash_attention/__init__.py
+extensions/flash_attention/flash_attention_dao_cuda.py
+extensions/flash_attention/flash_attention_npu.py
+extensions/flash_attention/flash_attention_sdpa_cuda.py
+extensions/layernorm/__init__.py
+extensions/layernorm/layernorm_cuda.py
+extensions/moe/__init__.py
+extensions/moe/moe_cuda.py
+extensions/optimizer/__init__.py
+extensions/optimizer/fused_optimizer_cuda.py
+extensions/softmax/__init__.py
+extensions/softmax/scaled_masked_softmax_cuda.py
+extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+requirements/requirements-infer.txt
 requirements/requirements-test.txt
 requirements/requirements.txt
 tests/kit/__init__.py
 tests/kit/model_zoo/__init__.py
 tests/kit/model_zoo/executor.py
 tests/kit/model_zoo/registry.py
 tests/kit/model_zoo/custom/__init__.py
 tests/kit/model_zoo/custom/base.py
 tests/kit/model_zoo/custom/hanging_param_model.py
 tests/kit/model_zoo/custom/nested_model.py
 tests/kit/model_zoo/custom/repeated_computed_layers.py
-tests/kit/model_zoo/custom/simple_mlp.py
 tests/kit/model_zoo/custom/simple_net.py
 tests/kit/model_zoo/diffusers/__init__.py
 tests/kit/model_zoo/diffusers/diffusers.py
 tests/kit/model_zoo/timm/__init__.py
 tests/kit/model_zoo/timm/timm.py
 tests/kit/model_zoo/torchaudio/__init__.py
 tests/kit/model_zoo/torchaudio/torchaudio.py
@@ -939,15 +881,14 @@
 tests/kit/model_zoo/transformers/chatglm2.py
 tests/kit/model_zoo/transformers/falcon.py
 tests/kit/model_zoo/transformers/gpt.py
 tests/kit/model_zoo/transformers/gptj.py
 tests/kit/model_zoo/transformers/llama.py
 tests/kit/model_zoo/transformers/mistral.py
 tests/kit/model_zoo/transformers/opt.py
-tests/kit/model_zoo/transformers/qwen2.py
 tests/kit/model_zoo/transformers/sam.py
 tests/kit/model_zoo/transformers/t5.py
 tests/kit/model_zoo/transformers/vit.py
 tests/kit/model_zoo/transformers/whisper.py
 tests/test_analyzer/__init__.py
 tests/test_analyzer/test_fx/__init__.py
 tests/test_analyzer/test_fx/test_bias_addition.py
@@ -1002,43 +943,14 @@
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
-tests/test_infer/__init__.py
-tests/test_infer/_utils.py
-tests/test_infer/test_batch_bucket.py
-tests/test_infer/test_config_and_struct.py
-tests/test_infer/test_continuous_batching.py
-tests/test_infer/test_cuda_graph.py
-tests/test_infer/test_drafter.py
-tests/test_infer/test_inference_engine.py
-tests/test_infer/test_kvcache_manager.py
-tests/test_infer/test_request_handler.py
-tests/test_infer/test_rpc_engine.py
-tests/test_infer/test_kernels/__init__.py
-tests/test_infer/test_kernels/cuda/__init__.py
-tests/test_infer/test_kernels/cuda/test_convert_fp8.py
-tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py
-tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py
-tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py
-tests/test_infer/test_kernels/cuda/test_rms_layernorm.py
-tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py
-tests/test_infer/test_kernels/cuda/test_silu_and_mul.py
-tests/test_infer/test_kernels/triton/__init__.py
-tests/test_infer/test_kernels/triton/kernel_utils.py
-tests/test_infer/test_kernels/triton/test_context_attn_unpad.py
-tests/test_infer/test_kernels/triton/test_decoding_attn.py
-tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py
-tests/test_infer/test_kernels/triton/test_kvcache_copy.py
-tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py
-tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py
-tests/test_infer/test_kernels/triton/test_xine_copy.py
 tests/test_shardformer/__init__.py
 tests/test_shardformer/test_flash_attention.py
 tests/test_shardformer/test_shard_utils.py
 tests/test_shardformer/test_with_torch_ddp.py
 tests/test_shardformer/test_model/__init__.py
 tests/test_shardformer/test_model/_utils.py
 tests/test_shardformer/test_model/test_shard_bert.py
@@ -1047,12 +959,11 @@
 tests/test_shardformer/test_model/test_shard_chatglm2.py
 tests/test_shardformer/test_model/test_shard_falcon.py
 tests/test_shardformer/test_model/test_shard_gpt2.py
 tests/test_shardformer/test_model/test_shard_gptj.py
 tests/test_shardformer/test_model/test_shard_llama.py
 tests/test_shardformer/test_model/test_shard_mistral.py
 tests/test_shardformer/test_model/test_shard_opt.py
-tests/test_shardformer/test_model/test_shard_qwen2.py
 tests/test_shardformer/test_model/test_shard_sam.py
 tests/test_shardformer/test_model/test_shard_t5.py
 tests/test_shardformer/test_model/test_shard_vit.py
 tests/test_shardformer/test_model/test_shard_whisper.py
```

### Comparing `colossalai-nightly-2024.5.25/examples/language/data_utils.py` & `colossalai-nightly-2024.5.4/examples/language/data_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/examples/language/model_utils.py` & `colossalai-nightly-2024.5.4/examples/language/model_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/examples/language/performance_evaluator.py` & `colossalai-nightly-2024.5.4/examples/language/performance_evaluator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/__init__.py` & `colossalai-nightly-2024.5.4/extensions/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,39 +1,32 @@
-from .pybind.cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
-from .pybind.flash_attention import (
-    FlashAttentionDaoCudaExtension,
-    FlashAttentionNpuExtension,
-    FlashAttentionSdpaCudaExtension,
-)
-from .pybind.inference import InferenceOpsCudaExtension
-from .pybind.layernorm import LayerNormCudaExtension
-from .pybind.moe import MoeCudaExtension
-from .pybind.optimizer import FusedOptimizerCudaExtension
-from .pybind.softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
+from .cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
+from .flash_attention import FlashAttentionDaoCudaExtension, FlashAttentionNpuExtension, FlashAttentionSdpaCudaExtension
+from .layernorm import LayerNormCudaExtension
+from .moe import MoeCudaExtension
+from .optimizer import FusedOptimizerCudaExtension
+from .softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
 
 ALL_EXTENSIONS = [
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     FusedOptimizerCudaExtension,
-    InferenceOpsCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
     FlashAttentionDaoCudaExtension,
     FlashAttentionSdpaCudaExtension,
     FlashAttentionNpuExtension,
 ]
 
 __all__ = [
     "CpuAdamArmExtension",
     "CpuAdamX86Extension",
     "LayerNormCudaExtension",
     "MoeCudaExtension",
     "FusedOptimizerCudaExtension",
-    "InferenceOpsCudaExtension",
     "ScaledMaskedSoftmaxCudaExtension",
     "ScaledUpperTriangleMaskedSoftmaxCudaExtension",
     "FlashAttentionDaoCudaExtension",
     "FlashAttentionSdpaCudaExtension",
     "FlashAttentionNpuExtension",
 ]
```

### Comparing `colossalai-nightly-2024.5.25/extensions/base_extension.py` & `colossalai-nightly-2024.5.4/extensions/base_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/cpp_extension.py` & `colossalai-nightly-2024.5.4/extensions/cpp_extension.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,17 +21,14 @@
         self.prebuilt_module_path = "colossalai._C"
         self.prebuilt_import_path = f"{self.prebuilt_module_path}.{self.name}"
         self.version_dependent_macros = ["-DVERSION_GE_1_1", "-DVERSION_GE_1_3", "-DVERSION_GE_1_5"]
 
     def csrc_abs_path(self, path):
         return os.path.join(self.relative_to_abs_path("csrc"), path)
 
-    def pybind_abs_path(self, path):
-        return os.path.join(self.relative_to_abs_path("pybind"), path)
-
     def relative_to_abs_path(self, code_path: str) -> str:
         """
         This function takes in a path relative to the colossalai root directory and return the absolute path.
         """
 
         # get the current file path
         # iteratively check the parent directory
@@ -115,15 +112,14 @@
         """
 
     @abstractmethod
     def include_dirs(self) -> List[str]:
         """
         This function should return a list of include files for extensions.
         """
-        return [self.csrc_abs_path("")]
 
     @abstractmethod
     def cxx_flags(self) -> List[str]:
         """
         This function should return a list of cxx compilation flags for extensions.
         """
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/arm/cpu_adam_arm.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/arm/cpu_adam_arm.h` & `colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/layer_norm_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda_kernel.cu`

 * *Files 6% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 
 #include "ATen/ATen.h"
 #include "ATen/AccumulateType.h"
 #include "ATen/cuda/CUDAContext.h"
 #include "ATen/cuda/DeviceUtils.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 template <typename U>
 __device__ void cuWelfordOnlineSum(const U curr, U& mu, U& sigma2, U& count) {
   count = count + U(1);
   U delta = curr - mu;
   U lmean = mu + delta / count;
   mu = lmean;
@@ -602,19 +602,19 @@
 #else
                      at::IntList normalized_shape,
 #endif
                      at::Tensor* gamma, at::Tensor* beta, double epsilon) {
   using namespace at;
   DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
       input->scalar_type(), output->scalar_type(), "cuda_layer_norm_kernel",
-      HostApplyLayerNorm(output->data_ptr<scalar_t_out>(),
-                         mean->data_ptr<float>(), invvar->data_ptr<float>(),
-                         input->data_ptr<scalar_t_in>(), n1, n2, epsilon,
-                         gamma != NULL ? gamma->data_ptr<scalar_t_out>() : NULL,
-                         beta != NULL ? beta->data_ptr<scalar_t_out>() : NULL);)
+      HostApplyLayerNorm(output->DATA_PTR<scalar_t_out>(),
+                         mean->DATA_PTR<float>(), invvar->DATA_PTR<float>(),
+                         input->DATA_PTR<scalar_t_in>(), n1, n2, epsilon,
+                         gamma != NULL ? gamma->DATA_PTR<scalar_t_out>() : NULL,
+                         beta != NULL ? beta->DATA_PTR<scalar_t_out>() : NULL);)
 }
 
 template <typename T, typename U, typename V>
 void HostLayerNormGradient(const V* dout, const U* mean, const U* invvar,
                            at::Tensor* input, int n1, int n2, const V* gamma,
                            const V* beta, double epsilon, T* grad_input,
                            V* grad_gamma, V* grad_beta) {
@@ -629,33 +629,33 @@
         2 * sizeof(U) * threads2.y * threads2.y * (threads2.x + 1);
     const int nshared2_b = threads2.x * threads2.y * sizeof(U);
     const int nshared2 = nshared2_a > nshared2_b ? nshared2_a : nshared2_b;
     at::Tensor part_grad_gamma = at::empty(
         {part_size, n2}, input->options().dtype(at::ScalarType::Float));
     at::Tensor part_grad_beta = at::empty_like(part_grad_gamma);
     cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
-        dout, input->data_ptr<T>(), n1, n2, mean, invvar, U(epsilon),
-        part_grad_gamma.data_ptr<U>(), part_grad_beta.data_ptr<U>());
+        dout, input->DATA_PTR<T>(), n1, n2, mean, invvar, U(epsilon),
+        part_grad_gamma.DATA_PTR<U>(), part_grad_beta.DATA_PTR<U>());
 
     const dim3 threads3(32, 8, 1);
     const dim3 blocks3((n2 + threads2.x - 1) / threads2.x, 1, 1);
     const int nshared3 = threads3.x * threads3.y * sizeof(U);
     cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
-        part_grad_gamma.data_ptr<U>(), part_grad_beta.data_ptr<U>(), part_size,
+        part_grad_gamma.DATA_PTR<U>(), part_grad_beta.DATA_PTR<U>(), part_size,
         n1, n2, grad_gamma, grad_beta);
   }
 
   // compute grad_input
   const uint64_t maxGridY =
       at::cuda::getCurrentDeviceProperties()->maxGridSize[1];
   const dim3 blocks1(1, std::min((uint64_t)n1, maxGridY), 1);
   const dim3 threads1(32, 4, 1);
   int nshared = threads1.y > 1 ? threads1.y * threads1.x * sizeof(U) : 0;
   cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
-      dout, input->data_ptr<T>(), n1, n2, mean, invvar, U(epsilon), gamma,
+      dout, input->DATA_PTR<T>(), n1, n2, mean, invvar, U(epsilon), gamma,
       grad_input);
 }
 
 void cuda_layer_norm_gradient(at::Tensor* dout, at::Tensor* mean,
                               at::Tensor* invvar, at::Tensor* input, int n1,
                               int n2,
 #ifdef VERSION_GE_1_1
@@ -667,17 +667,17 @@
                               double epsilon, at::Tensor* grad_input,
                               at::Tensor* grad_gamma, at::Tensor* grad_beta) {
   using namespace at;
   DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
       input->scalar_type(), gamma->scalar_type(),
       "cuda_layer_norm_gradient_kernel",
       HostLayerNormGradient(
-          dout->data_ptr<scalar_t_out>(), mean->data_ptr<float>(),
-          invvar->data_ptr<float>(), input, n1, n2,
+          dout->DATA_PTR<scalar_t_out>(), mean->DATA_PTR<float>(),
+          invvar->DATA_PTR<float>(), input, n1, n2,
           // TMJ pass NULL argument for gamma, beta, grad_gamma and grad_beta
           // if gamma Tensor is NULL on input.
-          gamma != NULL ? gamma->data_ptr<scalar_t_out>() : NULL,
-          gamma != NULL ? beta->data_ptr<scalar_t_out>() : NULL, epsilon,
-          grad_input->data_ptr<scalar_t_in>(),
-          gamma != NULL ? grad_gamma->data_ptr<scalar_t_out>() : NULL,
-          gamma != NULL ? grad_beta->data_ptr<scalar_t_out>() : NULL);)
+          gamma != NULL ? gamma->DATA_PTR<scalar_t_out>() : NULL,
+          gamma != NULL ? beta->DATA_PTR<scalar_t_out>() : NULL, epsilon,
+          grad_input->DATA_PTR<scalar_t_in>(),
+          gamma != NULL ? grad_gamma->DATA_PTR<scalar_t_out>() : NULL,
+          gamma != NULL ? grad_beta->DATA_PTR<scalar_t_out>() : NULL);)
 }
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/moe_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda_kernel.cu`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,14 @@
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <torch/extension.h>
 
 #include <cub/cub.cuh>
 
-#include "funcs/reduce_function.h"
-
-using colossalAI::funcs::block_reduce;
-using colossalAI::funcs::ReduceType;
+#include "block_reduce.h"
 
 template <typename T, int block_size, int pack_size>
 __device__ void moe_dpch_one_fwd(T *src_row, T *dst_row, const int cols) {
   assert(cols % pack_size == 0);
   const int bpack_size = block_size * pack_size;
 
   typedef cub::BlockLoad<T, block_size, pack_size, cub::BLOCK_LOAD_VECTORIZE>
@@ -156,15 +153,16 @@
     for (int i = 0; i < pack_size; ++i) {
       thread_sum += grad[i] * tokens[i];
       grad[i] *= weight;
     }
 
     BlockStore(ts_store).Store(src_row + idx, grad);
   }
-  block_reduce<float, ReduceType::kSum, 1>(&thread_sum);
+
+  blockReduce<ReduceType::kSum, 1>(&thread_sum);
 
   if (threadIdx.x == 0) *weight_grad = static_cast<T>(thread_sum);
 }
 
 template <typename T, int block_size, int pack_size>
 __device__ void moe_cb_two_fwd(T *src_row1, T *src_row2, T *dst_row,
                                const T weight1, const T weight2,
@@ -228,15 +226,15 @@
       sgrad2[i] = weight2 * grad[i];
     }
 
     BlockStore(ts_store).Store(src_row1 + idx, sgrad1);
     BlockStore(ts_store).Store(src_row2 + idx, sgrad2);
   }
 
-  block_reduce<float, ReduceType::kSum, 2>(thread_sum);
+  blockReduce<ReduceType::kSum, 2>(thread_sum);
 
   if (threadIdx.x == 0)
     *weight_grad1 = static_cast<T>(thread_sum[0]);
   else if (threadIdx.x == 1)
     *weight_grad2 = static_cast<T>(thread_sum[1]);
 }
 
@@ -535,15 +533,15 @@
     cumsum_kernel<1024, 2><<<e, 1024>>>(inputs, outputs, s, e);
   else
     cumsum_kernel<1024, 4><<<e, 1024>>>(inputs, outputs, s, e);
 }
 
 // API FUNCTIONS --------------------------------
 
-#define DISPATCH_FLOAT_AND_HALF_MOE(TYPE, NAME, ...)                   \
+#define DISPATCH_FLOAT_AND_HALF(TYPE, NAME, ...)                       \
   switch (TYPE) {                                                      \
     case at::ScalarType::Float: {                                      \
       using scalar_t = float;                                          \
       __VA_ARGS__;                                                     \
       break;                                                           \
     }                                                                  \
     case at::ScalarType::Half: {                                       \
@@ -561,41 +559,41 @@
                                         torch::Tensor dest_idx) {
   assert(h % 16 == 0);
   auto res = torch::zeros(
       {ec, h},
       torch::dtype(batch_tokens.dtype()).device(batch_tokens.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF_MOE(
+  DISPATCH_FLOAT_AND_HALF(
       batch_tokens.scalar_type(), "moe dispatch forward",
       moe_dpch_fwd_launch<scalar_t>(
-          batch_tokens.data_ptr<scalar_t>(), res.data_ptr<scalar_t>(),
-          mask[0].data_ptr<int>(), k == 1 ? nullptr : mask[1].data_ptr<int>(),
-          dest_idx[0].data_ptr<int>(),
-          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, h));
+          batch_tokens.data<scalar_t>(), res.data<scalar_t>(),
+          mask[0].data<int>(), k == 1 ? nullptr : mask[1].data<int>(),
+          dest_idx[0].data<int>(),
+          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, h));
 
   return res;
 }
 
 torch::Tensor moe_dispatch_cuda_backward(int s, int ec, int h,
                                          torch::Tensor expert_grad,
                                          torch::Tensor mask,
                                          torch::Tensor dest_idx) {
   assert(h % 16 == 0);
   auto res = torch::zeros(
       {s, h}, torch::dtype(expert_grad.dtype()).device(expert_grad.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF_MOE(
+  DISPATCH_FLOAT_AND_HALF(
       expert_grad.scalar_type(), "moe dispatch backward",
       moe_dpch_bwd_launch<scalar_t>(
-          res.data_ptr<scalar_t>(), expert_grad.data_ptr<scalar_t>(),
-          mask[0].data_ptr<int>(), k == 1 ? nullptr : mask[1].data_ptr<int>(),
-          dest_idx[0].data_ptr<int>(),
-          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, h));
+          res.data<scalar_t>(), expert_grad.data<scalar_t>(),
+          mask[0].data<int>(), k == 1 ? nullptr : mask[1].data<int>(),
+          dest_idx[0].data<int>(),
+          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, h));
 
   return res;
 }
 
 torch::Tensor moe_combine_cuda_forward(int s, int e, int c, int h,
                                        torch::Tensor expert_tokens,
                                        torch::Tensor logits, torch::Tensor mask,
@@ -604,21 +602,21 @@
   assert(expert_tokens.dtype() == logits.dtype());
 
   auto res = torch::zeros(
       {s, h},
       torch::dtype(expert_tokens.dtype()).device(expert_tokens.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF_MOE(
+  DISPATCH_FLOAT_AND_HALF(
       expert_tokens.scalar_type(), "moe combine forward",
       moe_cb_fwd_launch<scalar_t>(
-          expert_tokens.data_ptr<scalar_t>(), res.data_ptr<scalar_t>(),
-          logits.data_ptr<scalar_t>(), mask[0].data_ptr<int>(),
-          k == 1 ? nullptr : mask[1].data_ptr<int>(), dest_idx[0].data_ptr<int>(),
-          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, e, c,
+          expert_tokens.data<scalar_t>(), res.data<scalar_t>(),
+          logits.data<scalar_t>(), mask[0].data<int>(),
+          k == 1 ? nullptr : mask[1].data<int>(), dest_idx[0].data<int>(),
+          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, e, c,
           h));
 
   return res;
 }
 
 std::vector<torch::Tensor> moe_combine_cuda_backward(
     int s, int e, int c, int h, torch::Tensor tokens_grad,
@@ -631,31 +629,31 @@
   auto egrad = torch::zeros(
            {e * c, h},
            torch::dtype(tokens_grad.dtype()).device(tokens_grad.device())),
        wgrad = torch::zeros(
            {s, e}, torch::dtype(logits.dtype()).device(logits.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF_MOE(
+  DISPATCH_FLOAT_AND_HALF(
       tokens_grad.scalar_type(), "moe combine backward",
       moe_cb_bwd_launch<scalar_t>(
-          tokens_grad.data_ptr<scalar_t>(), egrad.data_ptr<scalar_t>(),
-          expert_tokens.data_ptr<scalar_t>(), logits.data_ptr<scalar_t>(),
-          wgrad.data_ptr<scalar_t>(), mask[0].data_ptr<int>(),
-          k == 1 ? nullptr : mask[1].data_ptr<int>(), dest_idx[0].data_ptr<int>(),
-          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, e, c,
+          tokens_grad.data<scalar_t>(), egrad.data<scalar_t>(),
+          expert_tokens.data<scalar_t>(), logits.data<scalar_t>(),
+          wgrad.data<scalar_t>(), mask[0].data<int>(),
+          k == 1 ? nullptr : mask[1].data<int>(), dest_idx[0].data<int>(),
+          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, e, c,
           h));
 
   return {egrad, wgrad};
 }
 
 torch::Tensor cumsum_sub_one_in_dim0(torch::Tensor mask) {
   assert(mask.dim() == 2);
   assert(mask.dtype() == torch::kInt32);
 
   const int s = mask.size(0), e = mask.size(1);
   auto res =
       torch::empty({s, e}, torch::dtype(torch::kInt32).device(mask.device()));
-  cumsum_launch(mask.data_ptr<int>(), res.data_ptr<int>(), s, e);
+  cumsum_launch(mask.data<int>(), res.data<int>(), s, e);
 
   return res;
 }
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_adam.cu`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 typedef enum {
   ADAM_MODE_0 = 0,  // L2 regularization mode
   ADAM_MODE_1 = 1   // Decoupled weight decay mode(AdamW)
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_apply.cuh`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <assert.h>
 #include <c10/cuda/CUDAGuard.h>
 
-#include "common/micros.h"
+#include "compat.h"
 
 // #include <iostream>
 
 // This header is the one-stop shop for all your multi-tensor apply needs.
 
 // TODO:  Kernel arg size limit may be <4KB for some other cards (ie Jetson)
 constexpr int depth_to_max_tensors[5] = {110, 64, 48, 36, 30};
@@ -100,15 +100,15 @@
       bool tensors_full = (loc_tensor_info == depth_to_max_tensors[depth - 1] &&
                            chunk == chunks_this_tensor - 1);
       bool blocks_full = (loc_block_info == depth_to_max_blocks[depth - 1]);
       bool last_chunk = (t == ntensors - 1 && chunk == chunks_this_tensor - 1);
       if (tensors_full || blocks_full || last_chunk) {
         // using accscalar_t = acc_type<scalar_t, true>;
         multi_tensor_apply_kernel<<<loc_block_info, block_size, 0, stream>>>(
-            chunk_size, noop_flag.data_ptr<int>(), tl, callable, args...);
+            chunk_size, noop_flag.DATA_PTR<int>(), tl, callable, args...);
 
         AT_CUDA_CHECK(cudaGetLastError());
 
         // Reset.  The control flow possibilities here make my brain hurt.
         loc_block_info = 0;
         if (chunk == chunks_this_tensor - 1) {
           // std::cout << "Hit case 1 " << cond1 << " " << cond2 << " " << cond3
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu`

 * *Files 15% similar despite different names*

```diff
@@ -7,107 +7,19 @@
 #include <c10/cuda/CUDAGuard.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
-
-template <typename T>
-__device__ __forceinline__ T reduce_block_into_lanes(
-    T* x, T val, int lanes = 1,
-    bool share_result = false)  // lanes is intended to be <= 32.
-{
-  int tid = threadIdx.x + threadIdx.y * blockDim.x;
-  int blockSize =
-      blockDim.x * blockDim.y;  // blockSize is intended to be a multiple of 32.
-
-  if (blockSize >= 64) {
-    x[tid] = val;
-    __syncthreads();
-  }
-
-#pragma unroll
-  for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
-    if (tid < i) x[tid] = x[tid] + x[tid + i];
-    __syncthreads();
-  }
-
-  T final;
-
-  if (tid < 32) {
-    if (blockSize >= 64)
-      final = x[tid] + x[tid + 32];
-    else
-      final = val;
-      // __SYNCWARP();
-
-#pragma unroll
-    for (int i = 16; i >= lanes; i >>= 1)
-      final = final + __shfl_down_sync(0xffffffff, final, i);
-  }
-
-  if (share_result) {
-    if (tid < lanes) x[tid] = final;  // EpilogueOp
-    // Make sure the smem result is visible to all warps.
-    __syncthreads();
-  }
-
-  return final;
-}
-
-template <typename T>
-__device__ __forceinline__ T reduce_block_into_lanes_max_op(
-    T* x, T val, int lanes = 1,
-    bool share_result = false)  // lanes is intended to be <= 32.
-{
-  int tid = threadIdx.x + threadIdx.y * blockDim.x;
-  int blockSize =
-      blockDim.x * blockDim.y;  // blockSize is intended to be a multiple of 32.
-
-  if (blockSize >= 64) {
-    x[tid] = val;
-    __syncthreads();
-  }
-
-#pragma unroll
-  for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
-    if (tid < i) x[tid] = fmaxf(fabsf(x[tid]), fabsf(x[tid + i]));
-    __syncthreads();
-  }
-
-  T final;
-
-  if (tid < 32) {
-    if (blockSize >= 64)
-      final = fmaxf(fabsf(x[tid]), fabsf(x[tid + 32]));
-    else
-      final = val;
-      // __SYNCWARP();
-
-#pragma unroll
-    for (int i = 16; i >= lanes; i >>= 1)
-      final =
-          fmaxf(fabsf(final), fabsf(__shfl_down_sync(0xffffffff, final, i)));
-  }
-
-  if (share_result) {
-    if (tid < lanes) x[tid] = final;  // EpilogueOp
-    // Make sure the smem result is visible to all warps.
-    __syncthreads();
-  }
-
-  return final;
-}
-
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
 }
 
 template <typename T>
 __device__ __forceinline__ void load_store(T *dst, T *src, int dst_offset,
@@ -373,32 +285,32 @@
     ret_per_tensor = at::empty({0}, float_options);
   }
 
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "multi_tensor_l2norm_cuda",
       multi_tensor_apply<1>(
           BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-          L2NormFunctor<scalar_t_0>(), output.data_ptr<float>(),
-          per_tensor ? output_per_tensor.data_ptr<float>() : nullptr,
+          L2NormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
+          per_tensor ? output_per_tensor.DATA_PTR<float>() : nullptr,
           per_tensor, max_chunks_per_tensor);)
 
   AT_CUDA_CHECK(cudaGetLastError());
   // AT_CUDA_CHECK(cudaDeviceSynchronize());
 
   // This involves one more small kernel launches, but will be negligible end to
   // end. I could get rid of these by hacking the functor + multi tensor harness
   // with persistence logic, but keeping it simple for now
   auto ret = at::empty({1}, output.options());
   const at::cuda::OptionalCUDAGuard device_guard(device_of(output));
   auto stream = at::cuda::getCurrentCUDAStream();
   cleanup<<<per_tensor ? ntensors : 1, 512, 0, stream>>>(
-      output.data_ptr<float>(),
-      per_tensor ? output_per_tensor.data_ptr<float>() : nullptr,
-      ret.data_ptr<float>(),
-      per_tensor ? ret_per_tensor.data_ptr<float>() : nullptr, per_tensor,
+      output.DATA_PTR<float>(),
+      per_tensor ? output_per_tensor.DATA_PTR<float>() : nullptr,
+      ret.DATA_PTR<float>(),
+      per_tensor ? ret_per_tensor.DATA_PTR<float>() : nullptr, per_tensor,
       max_chunks_per_tensor);
 
   return std::tuple<at::Tensor, at::Tensor>(ret, ret_per_tensor);
 }
 
 // Compute and update grad norm
 // Here use a per tensor norm, and blend new norm(n) and old norm(gn) by
@@ -433,23 +345,23 @@
       at::zeros({ntensors * max_chunks_per_tensor}, float_options);
 
   if (norm_type == 0) {
     DISPATCH_FLOAT_AND_HALF(
         tensor_lists[0][0].scalar_type(), 0, "multi_tensor_maxnorm_cuda",
         multi_tensor_apply<1>(
             BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-            MaxNormFunctor<scalar_t_0>(), output.data_ptr<float>(),
-            output_per_tensor.data_ptr<float>(), true, max_chunks_per_tensor);)
+            MaxNormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
+            output_per_tensor.DATA_PTR<float>(), true, max_chunks_per_tensor);)
   } else {
     DISPATCH_FLOAT_AND_HALF(
         tensor_lists[0][0].scalar_type(), 0, "multi_tensor_l2norm_cuda",
         multi_tensor_apply<1>(
             BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-            L2NormFunctor<scalar_t_0>(), output.data_ptr<float>(),
-            output_per_tensor.data_ptr<float>(), true, max_chunks_per_tensor);)
+            L2NormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
+            output_per_tensor.DATA_PTR<float>(), true, max_chunks_per_tensor);)
   }
   AT_CUDA_CHECK(cudaGetLastError());
 
   // AT_CUDA_CHECK(cudaDeviceSynchronize());
 
   // This involves one more small kernel launches, but will be negligible end to
   // end. I could get rid of these by hacking the functor + multi tensor harness
@@ -458,13 +370,13 @@
 
   // Adding the following device guard since it happens sometimes that the
   // tensors are on one device and the cuda stream is on another device which
   // results in ILLEGAL MEM ACCESS error.
   const at::cuda::OptionalCUDAGuard device_guard(device_of(output));
   auto stream = at::cuda::getCurrentCUDAStream();
   cleanup_v2<<<ntensors, 512, 0, stream>>>(
-      output.data_ptr<float>(), output_per_tensor.data_ptr<float>(),
-      ret.data_ptr<float>(), out.data_ptr<float>(), true, max_chunks_per_tensor,
+      output.DATA_PTR<float>(), output_per_tensor.DATA_PTR<float>(),
+      ret.DATA_PTR<float>(), out.DATA_PTR<float>(), true, max_chunks_per_tensor,
       norm_type, alpha, beta);
 
   return;
 }
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_lamb.cu`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
@@ -329,26 +329,26 @@
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "lamb_stage_1",
       multi_tensor_apply<4>(BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
                             LAMBStage1Functor<scalar_t_0>(), beta1, beta2,
                             beta3,  // 1-beta1 or 1 depends on averaging mode
                             bias_correction1, bias_correction2, epsilon,
                             (adamMode_t)mode, weight_decay,
-                            global_grad_norm.data_ptr<float>(), max_grad_norm);)
+                            global_grad_norm.DATA_PTR<float>(), max_grad_norm);)
 
   // Compute update norms
   auto update_norm_tuple =
       multi_tensor_l2norm_cuda(chunk_size, noop_flag, grad_list, true);
 
   std::vector<std::vector<at::Tensor>> grad_param_list(
       tensor_lists.begin(), tensor_lists.begin() + 2);
 
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "lamb_stage_2",
       multi_tensor_apply<2>(BLOCK_SIZE, chunk_size, noop_flag, grad_param_list,
                             LAMBStage2Functor<scalar_t_0>(),
-                            std::get<1>(param_norm_tuple).data_ptr<float>(),
-                            std::get<1>(update_norm_tuple).data_ptr<float>(),
+                            std::get<1>(param_norm_tuple).DATA_PTR<float>(),
+                            std::get<1>(update_norm_tuple).DATA_PTR<float>(),
                             lr, weight_decay, use_nvlamb);)
 
   AT_CUDA_CHECK(cudaGetLastError());
 }
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_scale_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 // #include <torch/all.h>
 
 #include <assert.h>
 // Stringstream is a big hammer, but I want to rely on operator<< for dtype.
 #include <sstream>
 
 #include "multi_tensor_apply.cuh"
-#include "common/micros.h"
+#include "type_shim.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <assert.h>
 #include <cuda_runtime.h>
 
-#include "common/micros.h"
+#include "compat.h"
 #include "multi_tensor_apply.cuh"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 /**
  * Perform fused SGD on multiple buffers
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,34 +1,103 @@
 /*This code from NVIDIA Megatron:
  *     with minor changes. */
 
-#include <ATen/ATen.h>
-#include <ATen/cuda/CUDAContext.h>
-#include <cuda.h>
-#include <cuda_fp16.h>
-#include <cuda_profiler_api.h>
-#include <cuda_runtime.h>
-#include <torch/extension.h>
+#pragma once
 
 #include <assert.h>
 #include <c10/macros/Macros.h>
+#include <cuda_fp16.h>
+#include <stdint.h>
+
 #include <cfloat>
 #include <limits>
 
-#include "common/micros.h"
-#include "utils/vec_copy.h"
-#include "funcs/reduce_function.h"
-#include "funcs/unary_functor.h"
-
-using colossalAI::funcs::UnaryOpFunctor;
-using colossalAI::funcs::UnaryOpType;
-using colossalAI::funcs::warp_reduce;
-using colossalAI::funcs::ReduceType;
-using colossalAI::cuda::utils::copy;
+namespace {
+
+template <typename Datatype, int ELEMENTS_PER_LDG>
+__device__ __inline__ void copy_vector(Datatype *dst, const Datatype *src);
+
+template <>
+__device__ __inline__ void copy_vector<c10::BFloat16, 1>(
+    c10::BFloat16 *dst, const c10::BFloat16 *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::BFloat16, 4>(
+    c10::BFloat16 *dst, const c10::BFloat16 *src) {
+  *((float2 *)dst) = *((float2 *)src);
+}
 
+template <>
+__device__ __inline__ void copy_vector<c10::Half, 1>(c10::Half *dst,
+                                                     const c10::Half *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::Half, 4>(c10::Half *dst,
+                                                     const c10::Half *src) {
+  *((float2 *)dst) = *((float2 *)src);
+}
+
+template <>
+__device__ __inline__ void copy_vector<uint8_t, 1>(uint8_t *dst,
+                                                   const uint8_t *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,
+                                                   const uint8_t *src) {
+  *((half2 *)dst) = *((half2 *)src);
+}
+
+int log2_ceil(int value) {
+  int log2_value = 0;
+  while ((1 << log2_value) < value) ++log2_value;
+  return log2_value;
+}
+
+template <typename T>
+struct Add {
+  __device__ __forceinline__ T operator()(T a, T b) const { return a + b; }
+};
+
+template <typename T>
+struct Max {
+  __device__ __forceinline__ T operator()(T a, T b) const {
+    return a < b ? b : a;
+  }
+};
+
+template <typename T>
+__device__ __forceinline__ T
+WARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,
+                     unsigned int mask = 0xffffffff) {
+#if CUDA_VERSION >= 9000
+  return __shfl_xor_sync(mask, value, laneMask, width);
+#else
+  return __shfl_xor(value, laneMask, width);
+#endif
+}
+
+template <typename acc_t, int WARP_BATCH, int WARP_SIZE,
+          template <typename> class ReduceOp>
+__device__ __forceinline__ void warp_reduce(acc_t *sum) {
+  ReduceOp<acc_t> r;
+#pragma unroll
+  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
+#pragma unroll
+    for (int i = 0; i < WARP_BATCH; ++i) {
+      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);
+      sum[i] = r(sum[i], b);
+    }
+  }
+}
 
 /*
  * Extended softmax (from native aten pytorch) with following additional
  * features 1) input scaling 2) Explicit masking
  */
 template <typename input_t, typename output_t, typename acc_t,
           int log2_elements>
@@ -83,16 +152,16 @@
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
 
       if (element_index < batch_element_count) {
         int itr_idx = i * element_count + it * WARP_SIZE;
-        copy<input_t, ELEMENTS_PER_LDG_STG>(src + itr_idx, temp_data);
-        copy<uint8_t, ELEMENTS_PER_LDG_STG>(mask + itr_idx, temp_mask);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(temp_data, src + itr_idx);
+        copy_vector<uint8_t, ELEMENTS_PER_LDG_STG>(temp_mask, mask + itr_idx);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (temp_mask[element] != 1) {
             elements[i][it + element] = (acc_t)temp_data[element] * scale;
           } else {
             elements[i][it + element] = -10000.0;
@@ -114,42 +183,42 @@
     max_value[i] = elements[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       max_value[i] =
           (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kMax,WARP_BATCH,WARP_SIZE>(max_value);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);
 
   acc_t sum[WARP_BATCH]{0.0f};
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; ++it) {
       elements[i][it] = std::exp((elements[i][it] - max_value[i]));
       sum[i] += elements[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
 
   // store result
   output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < element_count) {
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] = elements[i][it + element] / sum[i];
         }
-        copy<output_t, ELEMENTS_PER_LDG_STG>(
-          out,  dst + i * element_count + it * WARP_SIZE);
+        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
+            dst + i * element_count + it * WARP_SIZE, out);
       } else {
         break;
       }
     }
   }
 }
 
@@ -196,18 +265,18 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     int batch_element_count = (i >= local_batches) ? 0 : element_count;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < batch_element_count) {
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            grad + i * element_count + it * WARP_SIZE, temp_grad);
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            output + i * element_count + it * WARP_SIZE, temp_output);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_grad, grad + i * element_count + it * WARP_SIZE);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_output, output + i * element_count + it * WARP_SIZE);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           output_reg[i][it + element] = (acc_t)temp_output[element];
         }
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
@@ -223,15 +292,15 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     sum[i] = grad_reg[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       sum[i] += grad_reg[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
 
 // store result
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
@@ -241,25 +310,25 @@
         output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] =
               (output_t)(scale * (grad_reg[i][it + element] -
                                   output_reg[i][it + element] * sum[i]));
         }
-        copy<output_t, ELEMENTS_PER_LDG_STG>(
-          out, gradInput + i * element_count + it * WARP_SIZE);
+        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
+            gradInput + i * element_count + it * WARP_SIZE, out);
       }
     }
   }
 }
-
+}  // end of anonymous namespace
 
 int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
                         int attn_heads) {
-  int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
+  int log2_elements = log2_ceil(key_seq_len);
   const int next_power_of_two = 1 << log2_elements;
 
   int warp_size =
       (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
   int batches_per_warp = (next_power_of_two <= 128) ? 2 : 1;
 
   constexpr int threads_per_block = 128;
@@ -276,15 +345,15 @@
                                             int query_seq_len, int key_seq_len,
                                             int batches, int attn_heads,
                                             int pad_batches) {
   TORCH_INTERNAL_ASSERT(key_seq_len >= 0 && key_seq_len <= 2048);
   if (key_seq_len == 0) {
     return;
   } else {
-    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
+    int log2_elements = log2_ceil(key_seq_len);
     const int next_power_of_two = 1 << log2_elements;
     int batch_count = batches * attn_heads * query_seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_forward.
     int warp_size =
         (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
@@ -376,15 +445,15 @@
                                              const acc_t scale,
                                              int query_seq_len, int key_seq_len,
                                              int batches, int attn_heads) {
   TORCH_INTERNAL_ASSERT(key_seq_len >= 0 && key_seq_len <= 2048);
   if (key_seq_len == 0) {
     return;
   } else {
-    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
+    int log2_elements = log2_ceil(key_seq_len);
     const int next_power_of_two = 1 << log2_elements;
     int batch_count = batches * attn_heads * query_seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_backward.
     int warp_size =
         (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
@@ -463,71 +532,7 @@
                 grad_input, grad, output, scale, batch_count, key_seq_len);
         break;
       default:
         break;
     }
   }
 }
-
-torch::Tensor fwd_cuda(torch::Tensor const& input, torch::Tensor const& mask,
-                       float scale_factor) {
-  // input is a 4d tensor with dimensions [batches, attn_heads, seq_len,
-  // seq_len]
-  const int batches = input.size(0);
-  const int pad_batches = mask.size(0);
-  const int attn_heads = input.size(1);
-  const int query_seq_len = input.size(2);
-  const int key_seq_len = input.size(3);
-  TORCH_INTERNAL_ASSERT(key_seq_len <= 2048);
-  TORCH_INTERNAL_ASSERT(query_seq_len > 1);
-  TORCH_INTERNAL_ASSERT(pad_batches == 1 || pad_batches == batches);
-  TORCH_INTERNAL_ASSERT(mask.size(1) == 1);
-  TORCH_INTERNAL_ASSERT(mask.size(2) == query_seq_len);
-  TORCH_INTERNAL_ASSERT(mask.size(3) == key_seq_len);
-
-  // Output
-  auto act_options = input.options().requires_grad(false);
-  torch::Tensor softmax_results = torch::empty(
-      {batches, attn_heads, query_seq_len, key_seq_len}, act_options);
-
-  // Softmax Intermediate Result Ptr
-  void* input_ptr = static_cast<void*>(input.data_ptr());
-  void* mask_ptr = static_cast<void*>(mask.data_ptr());
-  void* softmax_results_ptr = static_cast<void*>(softmax_results.data_ptr());
-
-  DISPATCH_HALF_AND_BFLOAT(
-      input.scalar_type(), "dispatch_scaled_masked_softmax_forward",
-      dispatch_scaled_masked_softmax_forward<scalar_t, scalar_t, float>(
-          reinterpret_cast<scalar_t*>(softmax_results_ptr),
-          reinterpret_cast<const scalar_t*>(input_ptr),
-          reinterpret_cast<const uint8_t*>(mask_ptr), scale_factor,
-          query_seq_len, key_seq_len, batches, attn_heads, pad_batches););
-  return softmax_results;
-}
-
-torch::Tensor bwd_cuda(torch::Tensor const& output_grads_,
-                       torch::Tensor const& softmax_results_,
-                       float scale_factor) {
-  auto output_grads = output_grads_.contiguous();
-  auto softmax_results = softmax_results_.contiguous();
-
-  // output grads is a 4d tensor with dimensions [batches, attn_heads, seq_len,
-  // seq_len]
-  const int batches = output_grads.size(0);
-  const int attn_heads = output_grads.size(1);
-  const int query_seq_len = output_grads.size(2);
-  const int key_seq_len = output_grads.size(3);
-
-  void* output_grads_ptr = static_cast<void*>(output_grads.data_ptr());
-
-  // Softmax Grad
-  DISPATCH_HALF_AND_BFLOAT(
-      output_grads_.scalar_type(), "dispatch_scaled_masked_softmax_backward",
-      dispatch_scaled_masked_softmax_backward<scalar_t, scalar_t, float>(
-          reinterpret_cast<scalar_t*>(output_grads_ptr),
-          reinterpret_cast<scalar_t*>(output_grads_ptr),
-          reinterpret_cast<scalar_t const*>(softmax_results.data_ptr()),
-          scale_factor, query_seq_len, key_seq_len, batches, attn_heads););
-
-  // backward pass is completely in-place
-  return output_grads;
-}
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,128 @@
 /*This code from NVIDIA Megatron:
  *     with minor changes. */
 
-#include <ATen/ATen.h>
-#include <ATen/cuda/CUDAContext.h>
-#include <cuda.h>
-#include <cuda_fp16.h>
-#include <cuda_profiler_api.h>
-#include <cuda_runtime.h>
-#include <torch/extension.h>
+#pragma once
+
 #include <assert.h>
 #include <c10/macros/Macros.h>
+#include <cuda_fp16.h>
 #include <stdint.h>
+
 #include <cfloat>
 #include <limits>
 
-#include "common/micros.h"
-#include "utils/vec_copy.h"
-#include "funcs/reduce_function.h"
-#include "funcs/unary_functor.h"
-
-using colossalAI::funcs::UnaryOpFunctor;
-using colossalAI::funcs::UnaryOpType;
-using colossalAI::funcs::warp_reduce;
-using colossalAI::funcs::ReduceType;
-using colossalAI::cuda::utils::copy;
-using colossalAI::cuda::utils::copy_zero;
+namespace {
+
+template <typename Datatype, int ELEMENTS_PER_LDG>
+__device__ __inline__ void copy_vector(Datatype *dst, const Datatype *src);
+
+template <>
+__device__ __inline__ void copy_vector<c10::BFloat16, 1>(
+    c10::BFloat16 *dst, const c10::BFloat16 *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::BFloat16, 4>(
+    c10::BFloat16 *dst, const c10::BFloat16 *src) {
+  *((float2 *)dst) = *((float2 *)src);
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::Half, 1>(c10::Half *dst,
+                                                     const c10::Half *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<c10::Half, 4>(c10::Half *dst,
+                                                     const c10::Half *src) {
+  *((float2 *)dst) = *((float2 *)src);
+}
+
+template <>
+__device__ __inline__ void copy_vector<uint8_t, 1>(uint8_t *dst,
+                                                   const uint8_t *src) {
+  *dst = *src;
+}
+
+template <>
+__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,
+                                                   const uint8_t *src) {
+  *((half2 *)dst) = *((half2 *)src);
+}
+
+template <typename Datatype, int ELEMENTS_PER_LDG>
+__device__ __inline__ void copy_zero_vector(Datatype *dst);
+
+template <>
+__device__ __inline__ void copy_zero_vector<c10::BFloat16, 1>(
+    c10::BFloat16 *dst) {
+  *dst = 0.0;
+}
+
+template <>
+__device__ __inline__ void copy_zero_vector<c10::BFloat16, 4>(
+    c10::BFloat16 *dst) {
+  *((float2 *)dst) = make_float2(0.0f, 0.0f);
+}
+
+template <>
+__device__ __inline__ void copy_zero_vector<c10::Half, 1>(c10::Half *dst) {
+  *dst = 0.0;
+}
+
+template <>
+__device__ __inline__ void copy_zero_vector<c10::Half, 4>(c10::Half *dst) {
+  *((float2 *)dst) = make_float2(0.0f, 0.0f);
+}
+
+int log2_ceil(int value) {
+  int log2_value = 0;
+  while ((1 << log2_value) < value) ++log2_value;
+  return log2_value;
+}
+
+template <typename T>
+struct Add {
+  __device__ __forceinline__ T operator()(T a, T b) const { return a + b; }
+};
+
+template <typename T>
+struct Max {
+  __device__ __forceinline__ T operator()(T a, T b) const {
+    return a < b ? b : a;
+  }
+};
+
+template <typename T>
+__device__ __forceinline__ T
+WARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,
+                     unsigned int mask = 0xffffffff) {
+#if CUDA_VERSION >= 9000
+  return __shfl_xor_sync(mask, value, laneMask, width);
+#else
+  return __shfl_xor(value, laneMask, width);
+#endif
+}
+
+template <typename acc_t, int WARP_BATCH, int WARP_SIZE,
+          template <typename> class ReduceOp>
+__device__ __forceinline__ void warp_reduce(acc_t *sum) {
+  ReduceOp<acc_t> r;
+#pragma unroll
+  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
+#pragma unroll
+    for (int i = 0; i < WARP_BATCH; ++i) {
+      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);
+      sum[i] = r(sum[i], b);
+    }
+  }
+}
 
 /*
  * Extended softmax (from native aten pytorch) with following additional
  * features 1) input scaling 2) Implicit time (diagonal masking)
  */
 template <typename input_t, typename output_t, typename acc_t,
           int log2_elements>
@@ -71,16 +165,16 @@
     int batch_element_count = (i >= local_batches) ? 0 : local_seq;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
 
       if (element_index < batch_element_count) {
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            src + i * element_count * stride + it * WARP_SIZE, temp_data);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_data, src + i * element_count * stride + it * WARP_SIZE);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if ((element_index + element) < batch_element_count) {
             elements[i][it + element] = (acc_t)temp_data[element] * scale;
           } else {
             elements[i][it + element] = -std::numeric_limits<acc_t>::infinity();
@@ -102,29 +196,28 @@
     max_value[i] = elements[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       max_value[i] =
           (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kMax,WARP_BATCH,WARP_SIZE>(max_value);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);
 
   acc_t sum[WARP_BATCH]{0.0f};
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; ++it) {
       if (it < warp_iteration_limit) {
         elements[i][it] = std::exp((elements[i][it] - max_value[i]));
         sum[i] += elements[i][it];
       }
     }
   }
-  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
-
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
 
   // store result
   output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
@@ -136,18 +229,18 @@
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (element_index + element < local_seq) {
             out[element] = elements[i][it + element] / sum[i];
           } else {
             out[element] = 0;
           }
         }
-        copy<output_t, ELEMENTS_PER_LDG_STG>(
-            out, dst + i * element_count * stride + it * WARP_SIZE);
+        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
+            dst + i * element_count * stride + it * WARP_SIZE, out);
       } else if (element_index < element_count) {
-        copy_zero<output_t, ELEMENTS_PER_LDG_STG>(
+        copy_zero_vector<output_t, ELEMENTS_PER_LDG_STG>(
             dst + i * element_count * stride + it * WARP_SIZE);
       } else {
         break;
       }
     }
   }
 }
@@ -195,18 +288,18 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     int batch_element_count = (i >= local_batches) ? 0 : local_seq;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < batch_element_count) {
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            grad + i * element_count * stride + it * WARP_SIZE, temp_grad);
-        copy<input_t, ELEMENTS_PER_LDG_STG>(
-            output + i * element_count * stride + it * WARP_SIZE, temp_output);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_grad, grad + i * element_count * stride + it * WARP_SIZE);
+        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
+            temp_output, output + i * element_count * stride + it * WARP_SIZE);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (element_index + element < batch_element_count) {
             output_reg[i][it + element] = (acc_t)temp_output[element];
           }
         }
@@ -226,15 +319,15 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     sum[i] = grad_reg[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       sum[i] += grad_reg[i][it];
     }
   }
-  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
+  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
 
 // store result
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
@@ -244,30 +337,32 @@
         output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] =
               (output_t)(scale * (grad_reg[i][it + element] -
                                   output_reg[i][it + element] * sum[i]));
         }
-        copy<output_t, ELEMENTS_PER_LDG_STG>(
-            out, gradInput + i * element_count * stride + it * WARP_SIZE);
+        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
+            gradInput + i * element_count * stride + it * WARP_SIZE, out);
       }
     }
   }
 }
 
+}  // end of anonymous namespace
+
 template <typename input_t, typename output_t, typename acc_t>
 void dispatch_scaled_upper_triang_masked_softmax_forward(
     output_t *dst, const input_t *src, const input_t scale,
     int softmax_elements, int softmax_elements_stride, int attn_batches) {
   TORCH_INTERNAL_ASSERT(softmax_elements >= 0 && softmax_elements <= 2048);
   if (softmax_elements == 0) {
     return;
   } else {
-    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(softmax_elements);
+    int log2_elements = log2_ceil(softmax_elements);
     const int next_power_of_two = 1 << log2_elements;
     int seq_len = softmax_elements;
     int batch_count = attn_batches * seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_forward.
     int warp_size =
@@ -384,15 +479,15 @@
     output_t *grad_input, input_t *grad, const input_t *output,
     const acc_t scale, int softmax_elements, int softmax_elements_stride,
     int attn_batches) {
   TORCH_INTERNAL_ASSERT(softmax_elements >= 0 && softmax_elements <= 2048);
   if (softmax_elements == 0) {
     return;
   } else {
-    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(softmax_elements);
+    int log2_elements = log2_ceil(softmax_elements);
     const int next_power_of_two = 1 << log2_elements;
     int seq_len = softmax_elements;
     int batch_count = attn_batches * seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_backward.
     int warp_size =
@@ -499,65 +594,7 @@
                 softmax_elements_stride, softmax_elements);
         break;
       default:
         break;
     }
   }
 }
-
-
-
-
-torch::Tensor fwd_cuda(torch::Tensor const& input, float scale_factor) {
-  // input is a 3d tensor with dimensions [attn_batches, seq_len, seq_len]
-  const int attn_batches = input.size(0);
-  const int seq_len = input.size(1);
-  TORCH_INTERNAL_ASSERT(seq_len <= 2048);
-
-  // Output
-  auto act_options = input.options().requires_grad(false);
-  torch::Tensor softmax_results =
-      torch::empty({attn_batches, seq_len, seq_len}, act_options);
-
-  // Softmax Intermediate Result Ptr
-  void* input_ptr = static_cast<void*>(input.data_ptr());
-  void* softmax_results_ptr = static_cast<void*>(softmax_results.data_ptr());
-
-  DISPATCH_HALF_AND_BFLOAT(
-      input.scalar_type(),
-      "dispatch_scaled_upper_triang_masked_softmax_forward",
-      dispatch_scaled_upper_triang_masked_softmax_forward<scalar_t, scalar_t,
-                                                          float>(
-          reinterpret_cast<scalar_t*>(softmax_results_ptr),
-          reinterpret_cast<const scalar_t*>(input_ptr), scale_factor, seq_len,
-          seq_len, attn_batches););
-  return softmax_results;
-}
-
-torch::Tensor bwd_cuda(torch::Tensor const& output_grads_,
-                       torch::Tensor const& softmax_results_,
-                       float scale_factor) {
-  auto output_grads = output_grads_.contiguous();
-  auto softmax_results = softmax_results_.contiguous();
-
-  // output grads is a 3d tensor with dimensions [attn_batches, seq_len,
-  // seq_len]
-  const int attn_batches = output_grads.size(0);
-  const int seq_len = output_grads.size(1);
-  TORCH_INTERNAL_ASSERT(output_grads.size(1) == output_grads.size(2));
-
-  void* output_grads_ptr = static_cast<void*>(output_grads.data_ptr());
-
-  // Softmax Grad
-  DISPATCH_HALF_AND_BFLOAT(
-      output_grads_.scalar_type(),
-      "dispatch_scaled_upper_triang_masked_softmax_backward",
-      dispatch_scaled_upper_triang_masked_softmax_backward<scalar_t, scalar_t,
-                                                           float>(
-          reinterpret_cast<scalar_t*>(output_grads_ptr),
-          reinterpret_cast<scalar_t*>(output_grads_ptr),
-          reinterpret_cast<scalar_t const*>(softmax_results.data_ptr()),
-          scale_factor, seq_len, seq_len, attn_batches););
-
-  // backward pass is completely in-place
-  return output_grads;
-}
```

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/x86/cpu_adam.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/csrc/kernel/x86/cpu_adam.h` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/cuda_extension.py` & `colossalai-nightly-2024.5.4/extensions/cuda_extension.py`

 * *Files 13% similar despite different names*

```diff
@@ -17,15 +17,14 @@
 
 class _CudaExtension(_CppExtension):
     @abstractmethod
     def nvcc_flags(self) -> List[str]:
         """
         This function should return a list of nvcc compilation flags for extensions.
         """
-        return ["-DCOLOSSAL_WITH_CUDA"]
 
     def is_available(self) -> bool:
         # cuda extension can only be built if cuda is available
         try:
             import torch
 
             cuda_available = torch.cuda.is_available()
@@ -50,20 +49,14 @@
         from torch.utils.cpp_extension import CUDA_HOME
 
         if CUDA_HOME is None:
             raise RuntimeError("CUDA_HOME is None, please set CUDA_HOME to compile C++/CUDA kernels in ColossalAI.")
         cuda_include = os.path.join(CUDA_HOME, "include")
         return cuda_include
 
-    def include_dirs(self) -> List[str]:
-        """
-        This function should return a list of include files for extensions.
-        """
-        return super().include_dirs() + [self.get_cuda_home_include()]
-
     def build_jit(self) -> None:
         from torch.utils.cpp_extension import CUDA_HOME, load
 
         set_cuda_arch_list(CUDA_HOME)
 
         # get build dir
         build_directory = _Extension.get_jit_extension_folder_path()
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/cpu_adam/cpu_adam_x86.py` & `colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_x86.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import platform
 
-from ...cuda_extension import _CudaExtension
-from ...utils import append_nvcc_threads
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads
 
 
 class CpuAdamX86Extension(_CudaExtension):
     def __init__(self):
         super().__init__(name="cpu_adam_x86")
 
     def is_available(self) -> bool:
@@ -17,18 +17,21 @@
             arch == "x86_64"
         ), f"[extension] The {self.name} kernel requires the CPU architecture to be x86_64 but got {arch}"
         super().assert_compatible()
 
     # necessary 4 functions
     def sources_files(self):
         ret = [
-            self.csrc_abs_path("kernel/x86/cpu_adam.cpp"),
+            self.csrc_abs_path("cuda/cpu_adam.cpp"),
         ]
         return ret
 
+    def include_dirs(self):
+        return [self.csrc_abs_path("includes"), self.get_cuda_home_include()]
+
     def cxx_flags(self):
         extra_cxx_flags = [
             "-std=c++14",
             "-std=c++17",
             "-lcudart",
             "-lcublas",
             "-g",
@@ -43,9 +46,9 @@
             "-std=c++14",
             "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
             "-U__CUDA_NO_HALF2_OPERATORS__",
             "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
         ]
-        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags + super().nvcc_flags()
+        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/flash_attention/flash_attention_dao_cuda.py` & `colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_dao_cuda.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ...base_extension import _Extension
+from ..base_extension import _Extension
 
 
 class FlashAttentionDaoCudaExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_dao_cuda", support_aot=False, support_jit=False, priority=10)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/flash_attention/flash_attention_npu.py` & `colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_npu.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ...base_extension import _Extension
+from ..base_extension import _Extension
 
 
 class FlashAttentionNpuExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_npu", support_aot=False, support_jit=False)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py` & `colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_sdpa_cuda.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ...base_extension import _Extension
+from ..base_extension import _Extension
 
 
 class FlashAttentionSdpaCudaExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_sdpa_cuda", support_aot=False, support_jit=False)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/layernorm/layer_norm.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
  *     with minor changes. */
 
 #include <torch/extension.h>
 
 #include <cassert>
 #include <vector>
 
-#include "common/micros.h"
+#include "compat.h"
 
 namespace {
 
 void compute_n1_n2(at::Tensor input, at::IntArrayRef normalized_shape, int &n1,
                    int &n2) {
   int idiff = input.ndimension() - normalized_shape.size();
   n2 = 1;
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/layernorm/layernorm_cuda.py` & `colossalai-nightly-2024.5.4/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,26 +1,34 @@
-from ...cuda_extension import _CudaExtension
-from ...utils import append_nvcc_threads, get_cuda_cc_flag
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads, get_cuda_cc_flag
 
 
-class LayerNormCudaExtension(_CudaExtension):
+class ScaledUpperTriangleMaskedSoftmaxCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="layernorm_cuda")
+        super().__init__(name="scaled_upper_triangle_masked_softmax_cuda")
+
+    def include_dirs(self):
+        return [self.get_cuda_home_include()]
 
     def sources_files(self):
-        ret = [self.csrc_abs_path(fname) for fname in ["kernel/cuda/layer_norm_kernel.cu"]] + [
-            self.pybind_abs_path("layernorm/layer_norm.cpp")
+        ret = [
+            self.csrc_abs_path(fname)
+            for fname in [
+                "cuda/scaled_upper_triang_masked_softmax.cpp",
+                "cuda/scaled_upper_triang_masked_softmax_cuda.cu",
+            ]
         ]
         return ret
 
-    def include_dirs(self):
-        ret = [self.get_cuda_home_include()] + [self.csrc_abs_path("")]
-        return ret
-
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
-        extra_cuda_flags = ["-maxrregcount=50"]
+        extra_cuda_flags = [
+            "-U__CUDA_NO_HALF_OPERATORS__",
+            "-U__CUDA_NO_HALF_CONVERSIONS__",
+            "--expt-relaxed-constexpr",
+            "--expt-extended-lambda",
+        ]
         extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + self.version_dependent_macros + super().nvcc_flags()
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/moe/moe.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/optimizer/optimizer.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/colossal_C_frontend.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/softmax/scaled_masked_softmax.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.cpp`

 * *Files 18% similar despite different names*

```diff
@@ -2,23 +2,27 @@
  *     with minor changes. */
 
 #include <cuda_fp16.h>
 #include <torch/extension.h>
 
 #include <vector>
 
+namespace multihead_attn {
+namespace fused_softmax {
+namespace scaled_masked_softmax {
+
 torch::Tensor fwd_cuda(torch::Tensor const& input, torch::Tensor const& mask,
                        float scale_factor);
 
 torch::Tensor bwd_cuda(torch::Tensor const& output_grads,
                        torch::Tensor const& softmax_results,
                        float scale_factor);
 
-int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
-                        int attn_heads);
+int get_batch_per_block_cuda(int query_seq_len, int key_seq_len, int batches,
+                             int attn_heads);
 
 torch::Tensor fwd(torch::Tensor const& input, torch::Tensor const& mask,
                   float scale_factor) {
   AT_ASSERTM(input.dim() == 4, "expected 4D tensor");
   AT_ASSERTM((input.scalar_type() == at::ScalarType::Half) ||
                  (input.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
@@ -38,17 +42,29 @@
   AT_ASSERTM((softmax_results.scalar_type() == at::ScalarType::Half) ||
                  (softmax_results.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
 
   return bwd_cuda(output_grads, softmax_results, scale_factor);
 }
 
+int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
+                        int attn_heads) {
+  return get_batch_per_block_cuda(query_seq_len, key_seq_len, batches,
+                                  attn_heads);
+}
+
+}  // end namespace scaled_masked_softmax
+}  // end namespace fused_softmax
+}  // end namespace multihead_attn
+
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("forward", &fwd,
+  m.def("forward", &multihead_attn::fused_softmax::scaled_masked_softmax::fwd,
         "Self Multihead Attention scaled, time masked softmax -- Forward.");
 
-  m.def("backward", &bwd,
+  m.def("backward", &multihead_attn::fused_softmax::scaled_masked_softmax::bwd,
         "Self Multihead Attention scaled, time masked softmax -- Backward.");
 
-  m.def("get_batch_per_block", &get_batch_per_block,
+  m.def("get_batch_per_block",
+        &multihead_attn::fused_softmax::scaled_masked_softmax::
+            get_batch_per_block,
         "Return Batch per block size.");
 }
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/softmax/scaled_masked_softmax_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/layernorm_cuda.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,28 +1,24 @@
-from ...cuda_extension import _CudaExtension
-from ...utils import append_nvcc_threads
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads, get_cuda_cc_flag
 
 
-class ScaledMaskedSoftmaxCudaExtension(_CudaExtension):
+class LayerNormCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="scaled_masked_softmax_cuda")
+        super().__init__(name="layernorm_cuda")
 
     def sources_files(self):
-        ret = [self.csrc_abs_path(fname) for fname in ["kernel/cuda/scaled_masked_softmax_kernel.cu"]] + [
-            self.pybind_abs_path("softmax/scaled_masked_softmax.cpp")
-        ]
+        ret = [self.csrc_abs_path(fname) for fname in ["cuda/layer_norm_cuda.cpp", "cuda/layer_norm_cuda_kernel.cu"]]
+        return ret
+
+    def include_dirs(self):
+        ret = [self.get_cuda_home_include()]
         return ret
 
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
-        extra_cuda_flags = [
-            "-std=c++14",
-            "-std=c++17",
-            "-U__CUDA_NO_HALF_OPERATORS__",
-            "-U__CUDA_NO_HALF_CONVERSIONS__",
-            "-U__CUDA_NO_HALF2_OPERATORS__",
-            "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
-        ]
-        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags + super().nvcc_flags()
+        extra_cuda_flags = ["-maxrregcount=50"]
+        extra_cuda_flags.extend(get_cuda_cc_flag())
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + self.version_dependent_macros
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py` & `colossalai-nightly-2024.5.4/extensions/softmax/scaled_masked_softmax_cuda.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,30 +1,32 @@
-from ...cuda_extension import _CudaExtension
-from ...utils import append_nvcc_threads, get_cuda_cc_flag
+from ..cuda_extension import _CudaExtension
+from ..utils import append_nvcc_threads
 
 
-class ScaledUpperTriangleMaskedSoftmaxCudaExtension(_CudaExtension):
+class ScaledMaskedSoftmaxCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="scaled_upper_triangle_masked_softmax_cuda")
+        super().__init__(name="scaled_masked_softmax_cuda")
 
     def sources_files(self):
         ret = [
             self.csrc_abs_path(fname)
-            for fname in [
-                "kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu",
-            ]
-        ] + [self.pybind_abs_path("softmax/scaled_upper_triang_masked_softmax.cpp")]
+            for fname in ["cuda/scaled_masked_softmax.cpp", "cuda/scaled_masked_softmax_cuda.cu"]
+        ]
         return ret
 
+    def include_dirs(self):
+        return [self.get_cuda_home_include()]
+
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
         extra_cuda_flags = [
+            "-std=c++14",
+            "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
-            "--expt-relaxed-constexpr",
-            "--expt-extended-lambda",
+            "-U__CUDA_NO_HALF2_OPERATORS__",
+            "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
         ]
-        extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + super().nvcc_flags()
+        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.25/extensions/triton_extension.py` & `colossalai-nightly-2024.5.4/extensions/triton_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/extensions/utils.py` & `colossalai-nightly-2024.5.4/extensions/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/setup.py` & `colossalai-nightly-2024.5.4/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -90,15 +90,15 @@
         raise RuntimeError("[extension] Could not find any kernel compatible with the current environment.")
     else:
         op_name_list = ", ".join(op_names)
         print(f"[extension] Building extensions{op_name_list}")
 else:
     ext_modules = []
 
-version = "2024.05.25"
+version = "2024.05.04"
 package_name = "colossalai-nightly"
 
 setup(
     name=package_name,
     version=version,
     packages=find_packages(
         exclude=(
@@ -107,14 +107,15 @@
             "docker",
             "tests",
             "docs",
             "examples",
             "tests",
             "scripts",
             "requirements",
+            "extensions",
             "*.egg-info",
         ),
     ),
     description="An integrated large-scale model training system with efficient parallelization techniques",
     long_description=fetch_readme(),
     long_description_content_type="text/markdown",
     license="Apache Software License 2.0",
```

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/__init__.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/base.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/hanging_param_model.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/hanging_param_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/nested_model.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/nested_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/repeated_computed_layers.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/repeated_computed_layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/custom/simple_net.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/simple_net.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/diffusers/diffusers.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/diffusers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/executor.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/executor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/registry.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/timm/timm.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/timm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchaudio/torchaudio.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/torchaudio.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchrec/torchrec.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/torchrec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/torchvision/torchvision.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/torchvision.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/albert.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/albert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/bert.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/blip2.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/bloom.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/chatglm2.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/falcon.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/falcon.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/gpt.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gpt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/gptj.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gptj.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/llama.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/mistral.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/mistral.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/opt.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/opt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/qwen2.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/vit.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,89 +1,70 @@
 import torch
 import transformers
 
 from ..registry import ModelAttribute, model_zoo
 
-try:
-    from transformers import Qwen2Config
-
-    HAS_QWEN2 = True
-except ImportError:
-    HAS_QWEN2 = False
-
-if HAS_QWEN2:
-    # ===============================
-    # Register Qwen2
-    # ===============================
-
-    def data_gen():
-        # the input ids are corresponding to the sentence
-        # 'Hello, my dog is cute'
-        #
-        # the code is give below:
-        # -----------------------------------
-        # from transformers import Qwen2TokenizerFast
-        # tokenizer = Qwen2TokenizerFast.from_pretrained("Qwen/Qwen1.5-7B-Chat")
-        # input = 'Hello, my dog is cute'
-        # tokenized_input = tokenizer(input, return_tensors='pt').to('cuda')
-        # -----------------------------------
-
-        input_ids = torch.Tensor(
-            [[9707, 11, 847, 5562, 374, 13, 123, 18838], [9707, 11, 847, 5562, 374, 17, 89, 18838]]
-        ).long()
-        attention_mask = torch.Tensor([[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]).long()
-        return dict(input_ids=input_ids, attention_mask=attention_mask)
-
-    # label is needed for casual lm
-    def data_gen_for_casual_lm():
-        data = data_gen()
-        labels = data["input_ids"].clone()
-        data["labels"] = labels
-        return data
-
-    # transform the output to a dict
-    output_transform_fn = lambda x: x
-
-    # function to get the loss
-    loss_fn = lambda output: output["last_hidden_state"].mean()
-    loss_fn_for_casual_lm = lambda output: output["loss"]
-    loss_fn_for_seq_classification = lambda output: output["logits"].mean()
-
-    config = Qwen2Config(
-        hidden_size=128,
-        intermediate_size=256,
-        max_window_layers=4,
-        num_attention_heads=16,
-        num_hidden_layers=4,
-        num_key_value_heads=16,
-    )
-
-    config.pad_token_id = 0
-
-    # register the following models
-    # transformers.Qwen2Model,
-    # transformers.Qwen2ForCausalLM,
-    # transformers.Qwen2ForSequenceClassification,
-    model_zoo.register(
-        name="transformers_qwen2",
-        model_fn=lambda: transformers.Qwen2Model(config),
-        data_gen_fn=data_gen,
-        output_transform_fn=output_transform_fn,
-        loss_fn=loss_fn,
-        model_attribute=ModelAttribute(has_control_flow=True),
-    )
-    model_zoo.register(
-        name="transformers_qwen2_for_casual_lm",
-        model_fn=lambda: transformers.Qwen2ForCausalLM(config),
-        data_gen_fn=data_gen_for_casual_lm,
-        output_transform_fn=output_transform_fn,
-        loss_fn=loss_fn_for_casual_lm,
-        model_attribute=ModelAttribute(has_control_flow=True),
-    )
-    model_zoo.register(
-        name="transformers_qwen2_for_sequence_classification",
-        model_fn=lambda: transformers.Qwen2ForSequenceClassification(config),
-        data_gen_fn=data_gen,
-        output_transform_fn=output_transform_fn,
-        loss_fn=loss_fn_for_seq_classification,
-        model_attribute=ModelAttribute(has_control_flow=True),
-    )
+# ===============================
+# Register single-sentence VIT
+# ===============================
+
+config = transformers.ViTConfig(num_hidden_layers=4, hidden_size=128, intermediate_size=256, num_attention_heads=4)
+
+
+# define data gen function
+def data_gen():
+    pixel_values = torch.randn(1, 3, 224, 224)
+    return dict(pixel_values=pixel_values)
+
+
+def data_gen_for_image_classification():
+    data = data_gen()
+    data["labels"] = torch.tensor([0])
+    return data
+
+
+def data_gen_for_masked_image_modeling():
+    data = data_gen()
+    num_patches = (config.image_size // config.patch_size) ** 2
+    bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()
+    data["bool_masked_pos"] = bool_masked_pos
+    return data
+
+
+# define output transform function
+output_transform_fn = lambda x: x
+
+# function to get the loss
+loss_fn_for_vit_model = lambda x: x["pooler_output"].mean()
+loss_fn_for_image_classification = lambda x: x["logits"].mean()
+loss_fn_for_masked_image_modeling = lambda x: x["loss"]
+
+# register the following models
+# transformers.ViTModel,
+# transformers.ViTForMaskedImageModeling,
+# transformers.ViTForImageClassification,
+model_zoo.register(
+    name="transformers_vit",
+    model_fn=lambda: transformers.ViTModel(config),
+    data_gen_fn=data_gen,
+    output_transform_fn=output_transform_fn,
+    loss_fn=loss_fn_for_vit_model,
+    model_attribute=ModelAttribute(has_control_flow=True),
+)
+
+model_zoo.register(
+    name="transformers_vit_for_masked_image_modeling",
+    model_fn=lambda: transformers.ViTForMaskedImageModeling(config),
+    data_gen_fn=data_gen_for_masked_image_modeling,
+    output_transform_fn=output_transform_fn,
+    loss_fn=loss_fn_for_masked_image_modeling,
+    model_attribute=ModelAttribute(has_control_flow=True),
+)
+
+model_zoo.register(
+    name="transformers_vit_for_image_classification",
+    model_fn=lambda: transformers.ViTForImageClassification(config),
+    data_gen_fn=data_gen_for_image_classification,
+    output_transform_fn=output_transform_fn,
+    loss_fn=loss_fn_for_image_classification,
+    model_attribute=ModelAttribute(has_control_flow=True),
+)
```

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/sam.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/t5.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/t5.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/kit/model_zoo/transformers/whisper.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/whisper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_bias_addition.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_bias_addition.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_mod_dir.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_mod_dir.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_nested_ckpt.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_nested_ckpt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_shape_prop.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_shape_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/test_symbolic_profile.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_symbolic_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_fx/zoo.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/zoo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_subclasses/test_aten.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_aten.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_subclasses/test_flop_tensor.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_flop_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_analyzer/test_subclasses/test_meta_mode.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_meta_mode.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_pass/test_node_converting_pass.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_node_converting_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_flash_attention.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_flash_attention.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/_utils.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/_utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,22 +7,19 @@
 from torch import Tensor
 from torch import distributed as dist
 from torch.distributed import ProcessGroup
 from torch.nn import Module
 from torch.optim import Adam, Optimizer
 from torch.testing import assert_close
 
-from colossalai.accelerator import get_accelerator
 from colossalai.booster import Booster
-from colossalai.booster.plugin import HybridParallelPlugin, LowLevelZeroPlugin
+from colossalai.booster.plugin import HybridParallelPlugin
 from colossalai.booster.plugin.hybrid_parallel_plugin import HybridParallelModule
 from colossalai.checkpoint_io.utils import gather_distributed_param
 from colossalai.lazy import LazyInitContext
-from colossalai.nn.optimizer import GaLoreAdamW8bit
-from colossalai.nn.optimizer.galore import get_galore_param_groups
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer import ShardConfig, ShardFormer
 from colossalai.shardformer._utils import getattr_
 from colossalai.shardformer.policies.auto_policy import Policy
 from colossalai.tensor.d_tensor.api import is_customized_distributed_tensor, is_distributed_tensor
 from colossalai.tensor.padded_tensor.api import is_padded_tensor, to_unpadded_tensor
 
@@ -112,47 +109,28 @@
         assert k in shard_sd, f"{name} {k} not in sharded model"
         shard_v = shard_sd[k]
         assert v.shape == shard_v.shape, f"{name} {k} shape mismatch, {v.shape} vs {shard_v.shape}"
         assert v.dtype == shard_v.dtype, f"{name} {k} dtype mismatch, {v.dtype} vs {shard_v.dtype}"
         assert torch.equal(v, shard_v), f"{name} {k} value mismatch"
 
 
-def build_model_from_hybrid_plugin(
-    model_fn: Callable, loss_fn: Callable, test_config: Dict[str, Any], optim_class=Adam, sharded_optim_class=Adam
-):
+def build_model_from_hybrid_plugin(model_fn: Callable, loss_fn: Callable, test_config: Dict[str, Any]):
     use_lazy_init = False
     if "use_lazy_init" in test_config:
         use_lazy_init = test_config.pop("use_lazy_init")
 
     ctx = LazyInitContext() if use_lazy_init else nullcontext()
     with ctx:
         org_model = model_fn()
         sharded_model = copy.deepcopy(org_model)
     if use_lazy_init:
         ctx.materialize(org_model)
     org_model = org_model.cuda()
-    if optim_class == GaLoreAdamW8bit:
-        # Disable clipping and block-wise quantization
-        org_optimizer = optim_class(
-            get_galore_param_groups(org_model, weight_decay=0, rank=4),
-            lr=1e-3,
-            percentile_clipping=101,
-            block_wise=False,
-            min_8bit_size=1e10,
-        )
-        sharded_optimizer = sharded_optim_class(
-            get_galore_param_groups(sharded_model, weight_decay=0, rank=4),
-            lr=1e-3,
-            percentile_clipping=101,
-            block_wise=False,
-            min_8bit_size=1e10,
-        )
-    else:
-        org_optimizer = optim_class(org_model.parameters(), lr=1e-3)
-        sharded_optimizer = sharded_optim_class(sharded_model.parameters(), lr=1e-3)
+    org_optimizer = Adam(org_model.parameters(), lr=1e-3)
+    sharded_optimizer = Adam(sharded_model.parameters(), lr=1e-3)
     criterion = loss_fn
 
     plugin = HybridParallelPlugin(**test_config)
     booster = Booster(plugin=plugin)
 
     sharded_model, sharded_optimizer, criterion, _, _ = booster.boost(sharded_model, sharded_optimizer, criterion)
     return (
@@ -161,40 +139,14 @@
         sharded_model,
         sharded_optimizer,
         criterion,
         booster,
     )
 
 
-def build_model_from_low_level_zero_plugin(
-    model_fn: Callable, loss_fn: Callable, test_config: Dict[str, Any], optim_class=Adam, sharded_optim_class=Adam
-):
-    use_lazy_init = False
-    if "use_lazy_init" in test_config:
-        use_lazy_init = test_config.pop("use_lazy_init")
-
-    ctx = LazyInitContext() if use_lazy_init else nullcontext()
-    with ctx:
-        org_model = model_fn()
-        sharded_model = copy.deepcopy(org_model)
-    if use_lazy_init:
-        ctx.materialize(org_model)
-
-    org_model = org_model.cuda()
-    org_optimizer = optim_class(org_model.parameters(), lr=1e-3)
-    sharded_optimizer = sharded_optim_class(sharded_model.parameters(), lr=1e-3)
-    criterion = loss_fn
-
-    plugin = LowLevelZeroPlugin(**test_config)
-    booster = Booster(plugin=plugin)
-
-    sharded_model, sharded_optimizer, criterion, _, _ = booster.boost(sharded_model, sharded_optimizer, criterion)
-    return org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster
-
-
 def run_forward_backward_with_hybrid_plugin(
     org_model: Module,
     sharded_model: Module,
     sharded_optimizer: Optimizer,
     data_gen_fn: Callable,
     output_transform_fn: Callable,
     criterion: Callable,
@@ -253,52 +205,14 @@
     org_output = org_model(**unshard_test_data)
     org_loss = criterion(org_output)
     org_loss.backward()
 
     return org_loss, org_output, sharded_loss, sharded_output
 
 
-def run_forward_backward_with_low_level_zero_plugin(
-    org_model: Module,
-    sharded_model: Module,
-    sharded_optimizer: Optimizer,
-    data_gen_fn: Callable,
-    output_transform_fn: Callable,
-    criterion: Callable,
-    booster: Booster,
-):
-    get_accelerator().get_current_device()
-    org_model.cuda()
-    sharded_model.cuda()
-
-    def _criterion(outputs, inputs):
-        outputs = output_transform_fn(outputs)
-        loss = criterion(outputs)
-        return loss
-
-    data = data_gen_fn()
-
-    # data = {
-    #     k: v.to(device) if torch.is_tensor(v) or "Tensor" in v.__class__.__name__ else v for k, v in data.items()
-    # }
-    data = {k: v.cuda() for k, v in data.items()}
-
-    sharded_model.train()
-    sharded_output = sharded_model(**data)
-    sharded_loss = criterion(sharded_output)
-    sharded_optimizer.backward(sharded_loss)
-
-    org_model.train()
-    org_output = org_model(**data)
-    org_loss = criterion(org_output)
-    org_loss.backward()
-
-    return org_loss, org_output, sharded_loss, sharded_output
-
-
 def check_output_hidden_state(
     org_output: Tensor,
     sharded_output: Tensor,
     stage_manager: Optional[PipelineStageManager] = None,
     atol: float = 1e-5,
     rtol: float = 1e-3,
 ):
@@ -394,17 +308,14 @@
     rtol: float = 1e-3,
     verbose: bool = False,
 ):
     for suffix in layer_suffix:
         org_grad = getattr_(org_model, suffix).weight.grad
         shard_grad = getattr_(sharded_model, suffix).weight.grad
         shard_weight = getattr_(sharded_model, suffix).weight
-        # if verbose and dist.get_rank() == 0:
-        #     print("shard_weight", shard_weight)
-        #     print("org_grad", org_grad)
         if is_distributed_tensor(shard_weight) or is_customized_distributed_tensor(shard_weight):
             shard_grad_list = [torch.zeros_like(shard_grad).to("cuda") for _ in range(dist.get_world_size(tp_group))]
             dist.all_gather(shard_grad_list, shard_grad, tp_group)
             shard_grad = torch.cat(shard_grad_list, dim=dim)
 
         # embedding may be resized when using tensor parallel
         if shard_grad.shape[0] > org_grad.shape[0]:
```

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_bert.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_blip2.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_bloom.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_chatglm2.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_falcon.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_falcon.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_gpt2.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gpt2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_gptj.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gptj.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_llama.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_llama.py`

 * *Files 0% similar despite different names*

```diff
@@ -60,17 +60,15 @@
         booster.plugin.zero_stage in [1, 2]
         and booster.plugin.shard_config.enable_sequence_parallelism
         and booster.plugin.shard_config.sequence_parallelism_mode == "all_to_all"
     ):
         for p1, p2 in zip(llama_model.parameters(), sharded_optimizer._master_param_groups_of_current_rank[0]):
             working_p = sharded_optimizer._param_store.master_to_working_param[id(p2)]
             grads = sharded_optimizer._grad_store.get_partitioned_gradients_by_param_id(0, id(working_p))
-            grad_index = (
-                0 if sharded_optimizer._grad_store._partition_grads else sharded_optimizer._bucket_store.zero_local_rank
-            )
+            grad_index = 0 if sharded_optimizer._partition_grads else sharded_optimizer._local_rank
             grad = grads[grad_index]
             sharded_grad = p1.grad.view(-1).chunk(dist.get_world_size())[dist.get_rank()]
             assert_close(sharded_grad, grad[: sharded_grad.shape[0]], atol=5e-3, rtol=5e-3, check_dtype=False)
 
     # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
     if (stage_manager is None or stage_manager.is_first_stage(ignore_chunk=True)) and booster.plugin.zero_stage == 0:
```

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_mistral.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_mistral.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_opt.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_opt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_qwen2.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_t5.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,12 +1,9 @@
-import os
-
 import pytest
 import torch
-import transformers
 
 import colossalai
 from colossalai.logging import disable_existing_loggers
 from colossalai.shardformer.layer.utils import Randomizer
 from colossalai.tensor.d_tensor.api import clear_layout_converter
 from colossalai.testing import clear_cache_before_run, parameterize, rerun_if_address_is_in_use, spawn
 from tests.kit.model_zoo import model_zoo
@@ -17,76 +14,83 @@
     check_output_hidden_state,
     check_weight,
     get_grad_tensors_for_check,
     run_forward_backward_with_hybrid_plugin,
     unwrap_model,
 )
 
-os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "true"
-
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
-        org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
+        org_model,
+        sharded_model,
+        sharded_optimizer,
+        data_gen_fn,
+        output_transform_fn,
+        criterion,
+        booster,
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
     # unwrap model
-    qwen2_model = unwrap_model(org_model, "Qwen2Model", "model")
-    shard_qwen2_model = unwrap_model(sharded_model, "Qwen2Model", "model")
+    t5 = unwrap_model(org_model)
+    sharded_t5 = unwrap_model(sharded_model)
 
-    row_layer_for_check = ["layers[0].self_attn.q_proj", "embed_tokens"]
-    col_layer_for_check = ["layers[0].self_attn.o_proj"]
+    row_layer_for_check = ["shared", "encoder.block[0].layer[0].SelfAttention.q"]
 
     # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
-    if (stage_manager is None or stage_manager.is_first_stage(ignore_chunk=True)) and booster.plugin.zero_stage == 0:
-        if test_config["precision"] == "fp32":
-            atol, rtol = 1e-6, 1e-4
-        else:
-            atol, rtol = 5e-3, 5e-3
+    if test_config["precision"] == "fp32":
+        atol, rtol = 1e-5, 1e-3
+    else:
+        atol, rtol = 5e-3, 5e-3
+    if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
         row_layer_grads = get_grad_tensors_for_check(
-            qwen2_model, shard_qwen2_model, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
-        )
-        col_layer_grads = get_grad_tensors_for_check(
-            qwen2_model, shard_qwen2_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            t5, sharded_t5, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0
         )
-        grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
-    if stage_manager is None or stage_manager.is_last_stage(ignore_chunk=True):
+    if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
             atol, rtol = 1e-5, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
 
-        if org_model.__class__.__name__ == "Qwen2Model":
+        if org_model.__class__.__name__ != "T5ForConditionalGeneration":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
-    if stage_manager is None or stage_manager.is_first_stage(ignore_chunk=True):
-        if test_config["precision"] == "fp32":
-            atol, rtol = 1e-4, 1e-3
-        else:
-            atol, rtol = 5e-3, 5e-3
+    if test_config["precision"] == "fp32":
+        # TODO he precision in weight checking is too significant.
+        atol, rtol = 1e-3, 1e-3
+    else:
+        atol, rtol = 5e-3, 5e-3
+    if stage_manager is None or stage_manager.is_first_stage():
         check_weight(
-            qwen2_model, shard_qwen2_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            t5,
+            sharded_t5,
+            row_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=0,
+            verbose=False,
         )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
     torch.cuda.empty_cache()
 
@@ -94,142 +98,165 @@
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 2,
+            "enable_metadata_cache": False,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 4,
+            "enable_metadata_cache": False,
             "use_lazy_init": False,
-            "precision": "fp32",
+            "precision": "fp16",
+            "initial_scale": 1,
         },
         {
             "tp_size": 4,
             "pp_size": 1,
-            "enable_all_optimization": True,
+            "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
         {
             "tp_size": 1,
             "pp_size": 4,
             "num_microbatches": 4,
+            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
-        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
         {
             "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 2,
+            "enable_metadata_cache": False,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "zero_stage": 1,
             "precision": "fp16",
             "initial_scale": 1,
         },
     ],
 )
-def run_qwen2_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_qwen2")
+@clear_cache_before_run()
+def run_t5_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_t5")
+
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
+        # skip 4-stage pp test for t5_encoder
+        if test_config["pp_size"] > 2 and name == "transformers_t5_encoder_model":
+            continue
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
+            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
             "initial_scale": 1,
         },
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
+            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp16",
             "zero_stage": 1,
             "initial_scale": 1,
         },
-        {
-            "tp_size": 2,
-            "pp_size": 2,
-            "pp_style": "interleaved",
-            "num_model_chunks": 2,
-            "num_microbatches": 4,
-            "enable_all_optimization": False,
-            "precision": "fp16",
-            "zero_stage": 1,
-            "initial_scale": 1,
-        },
     ],
 )
-def run_qwen2_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_qwen2")
+def run_t5_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_t5")
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
-    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
-def check_qwen2(rank, world_size, port):
+def check_t5(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_qwen2_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_t5_test()
 
 
-def check_qwen2_3d(rank, world_size, port):
+def check_t5_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_qwen2_3d_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_t5_3d_test()
 
 
-@pytest.mark.skipif(transformers.__version__ < "4.39.1", reason="Requires transformers version 4.39.1 or later")
+@pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_qwen2():
-    spawn(check_qwen2, 4)
+def test_t5():
+    spawn(check_t5, 4)
 
 
-@pytest.mark.skipif(transformers.__version__ < "4.39.1", reason="Requires transformers version 4.39.1 or later")
+@pytest.mark.largedist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_qwen2_3d():
-    spawn(check_qwen2_3d, 8)
+def test_t5_3d():
+    spawn(check_t5_3d, 8)
 
 
 if __name__ == "__main__":
-    test_qwen2()
-    test_qwen2_3d()
+    test_t5()
+    test_t5_3d()
```

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_sam.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_t5.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_vit.py`

 * *Files 19% similar despite different names*

```diff
@@ -21,242 +21,177 @@
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
-        org_model,
-        sharded_model,
-        sharded_optimizer,
-        data_gen_fn,
-        output_transform_fn,
-        criterion,
-        booster,
+        org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
     # unwrap model
-    t5 = unwrap_model(org_model)
-    sharded_t5 = unwrap_model(sharded_model)
+    vit_model = unwrap_model(org_model, "ViTModel", "vit")
+    shard_vit_model = unwrap_model(sharded_model, "ViTModel", "vit")
 
-    row_layer_for_check = ["shared", "encoder.block[0].layer[0].SelfAttention.q"]
+    # check grad
+    row_layer_for_check = ["encoder.layer[0].attention.attention.query", "embeddings.patch_embeddings.projection"]
+    col_layer_for_check = ["encoder.layer[0].attention.output.dense"]
 
     # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
-    if test_config["precision"] == "fp32":
-        atol, rtol = 1e-5, 1e-3
-    else:
-        atol, rtol = 5e-3, 5e-3
     if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
+        if test_config["precision"] == "fp32":
+            atol, rtol = 2e-5, 1e-3
+        else:
+            atol, rtol = 5e-3, 5e-3
         row_layer_grads = get_grad_tensors_for_check(
-            t5, sharded_t5, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0
+            vit_model, shard_vit_model, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
         )
+        col_layer_grads = get_grad_tensors_for_check(
+            vit_model, shard_vit_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+        )
+        grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
     if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
-            atol, rtol = 1e-5, 1e-3
+            atol, rtol = 2e-3, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
 
-        if org_model.__class__.__name__ != "T5ForConditionalGeneration":
+        if org_model.__class__.__name__ == "ViTModel":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
-
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
-    if test_config["precision"] == "fp32":
-        # TODO he precision in weight checking is too significant.
-        atol, rtol = 1e-3, 1e-3
-    else:
-        atol, rtol = 5e-3, 5e-3
     if stage_manager is None or stage_manager.is_first_stage():
+        if test_config["precision"] == "fp32":
+            atol, rtol = 5e-3, 1e-3
+        else:
+            atol, rtol = 5e-3, 5e-3
         check_weight(
-            t5,
-            sharded_t5,
-            row_layer_for_check,
-            tp_group,
-            atol=atol,
-            rtol=rtol,
-            dim=0,
-            verbose=False,
+            vit_model, shard_vit_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
         )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
     torch.cuda.empty_cache()
 
 
+# TODO: num_microbatch size = 2 inf loss
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
-            "num_microbatches": 2,
-            "enable_metadata_cache": False,
-            "enable_all_optimization": True,
-            "use_lazy_init": True,
-            "precision": "fp16",
-            "initial_scale": 1,
-        },
-        {
-            "tp_size": 1,
-            "pp_size": 2,
             "num_microbatches": 4,
-            "enable_metadata_cache": False,
+            "enable_all_optimization": True,
             "use_lazy_init": False,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
-            "tp_size": 4,
-            "pp_size": 1,
-            "enable_all_optimization": False,
-            "use_lazy_init": False,
-            "precision": "fp32",
-        },
-        {
             "tp_size": 1,
-            "pp_size": 4,
+            "pp_size": 2,
             "num_microbatches": 4,
-            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
+        {"tp_size": 4, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
+        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
         {
             "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
+            "use_lazy_init": False,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
-            "num_microbatches": 2,
-            "enable_metadata_cache": False,
+            "num_microbatches": 4,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
+            "use_lazy_init": False,
             "zero_stage": 1,
             "precision": "fp16",
             "initial_scale": 1,
         },
     ],
 )
-@clear_cache_before_run()
-def run_t5_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_t5")
-
-    for name, (
-        model_fn,
-        data_gen_fn,
-        output_transform_fn,
-        loss_fn,
-        _,
-    ) in sub_model_zoo.items():
-        # skip 4-stage pp test for t5_encoder
-        if test_config["pp_size"] > 2 and name == "transformers_t5_encoder_model":
-            continue
+def run_vit_test(test_config):
+    # TODO: fix bug when settign lazy_init for Conv2D Layers in ViT models
 
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_vit")
+    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
-            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
             "initial_scale": 1,
         },
-        {
-            "tp_size": 2,
-            "pp_size": 2,
-            "num_microbatches": 4,
-            "enable_metadata_cache": False,
-            "enable_all_optimization": False,
-            "use_lazy_init": False,
-            "precision": "fp16",
-            "zero_stage": 1,
-            "initial_scale": 1,
-        },
     ],
 )
-def run_t5_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_t5")
+def run_vit_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_vit")
 
-    for name, (
-        model_fn,
-        data_gen_fn,
-        output_transform_fn,
-        loss_fn,
-        _,
-    ) in sub_model_zoo.items():
+    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
-def check_t5(rank, world_size, port):
+def check_vit(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(
-        rank=rank,
-        world_size=world_size,
-        host="localhost",
-        port=port,
-        backend="nccl",
-    )
-    run_t5_test()
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    run_vit_test()
 
 
-def check_t5_3d(rank, world_size, port):
+def check_vit_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(
-        rank=rank,
-        world_size=world_size,
-        host="localhost",
-        port=port,
-        backend="nccl",
-    )
-    run_t5_3d_test()
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    run_vit_3d_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_t5():
-    spawn(check_t5, 4)
+def test_vit():
+    spawn(check_vit, 4)
 
 
 @pytest.mark.largedist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_t5_3d():
-    spawn(check_t5_3d, 8)
+def test_vit_3d():
+    spawn(check_vit_3d, 8)
 
 
 if __name__ == "__main__":
-    test_t5()
-    test_t5_3d()
+    test_vit()
+    test_vit_3d()
```

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_model/test_shard_vit.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_whisper.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,187 +11,215 @@
     build_model_from_hybrid_plugin,
     check_all_grad_tensors,
     check_loss,
     check_output_hidden_state,
     check_weight,
     get_grad_tensors_for_check,
     run_forward_backward_with_hybrid_plugin,
-    unwrap_model,
 )
 
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
+    # check forward
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
         org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
-    # unwrap model
-    vit_model = unwrap_model(org_model, "ViTModel", "vit")
-    shard_vit_model = unwrap_model(sharded_model, "ViTModel", "vit")
+    # unwarp the model
+    if org_model.__class__.__name__ == "WhisperForConditionalGeneration":
+        whisper = org_model.model
+        sharded_whisper = sharded_model.unwrap().model
+    else:
+        whisper = org_model
+        sharded_whisper = sharded_model.unwrap()
 
     # check grad
-    row_layer_for_check = ["encoder.layer[0].attention.attention.query", "embeddings.patch_embeddings.projection"]
-    col_layer_for_check = ["encoder.layer[0].attention.output.dense"]
+    if org_model.__class__.__name__ == "WhisperForAudioClassification":
+        col_layer_for_check = ["encoder.layers[0].self_attn.q_proj"]
+        row_layer_for_check = ["encoder.layers[0].self_attn.out_proj"]
+    else:
+        col_layer_for_check = [
+            "encoder.layers[0].self_attn.q_proj",
+            # 'decoder.layers[0].self_attn.q_proj'
+        ]
+        row_layer_for_check = [
+            "encoder.layers[0].self_attn.out_proj",
+            #'decoder.layers[0].self_attn.out_proj'
+        ]
 
     # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
-    if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
-        if test_config["precision"] == "fp32":
-            atol, rtol = 2e-5, 1e-3
-        else:
-            atol, rtol = 5e-3, 5e-3
+    if test_config["precision"] == "fp32":
+        atol, rtol = 2e-4, 2e-4
+    else:
+        atol, rtol = 5e-3, 5e-3
+
+    if stage_manager is None or stage_manager.is_first_stage():
         row_layer_grads = get_grad_tensors_for_check(
-            vit_model, shard_vit_model, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
+            whisper, sharded_whisper, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1
         )
         col_layer_grads = get_grad_tensors_for_check(
-            vit_model, shard_vit_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            whisper, sharded_whisper, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0
         )
         grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
     if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
-            atol, rtol = 2e-3, 1e-3
+            atol, rtol = 2e-4, 2e-4
         else:
             atol, rtol = 5e-3, 5e-3
 
-        if org_model.__class__.__name__ == "ViTModel":
+        if org_model.__class__.__name__ == "WhisperModel":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
+
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
+    if test_config["precision"] == "fp32":
+        atol, rtol = 1e-3, 1e-3
+    else:
+        atol, rtol = 5e-3, 5e-3
     if stage_manager is None or stage_manager.is_first_stage():
-        if test_config["precision"] == "fp32":
-            atol, rtol = 5e-3, 1e-3
-        else:
-            atol, rtol = 5e-3, 5e-3
         check_weight(
-            vit_model, shard_vit_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            whisper, sharded_whisper, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+        )
+        check_weight(
+            whisper, sharded_whisper, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
         )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
     torch.cuda.empty_cache()
 
 
-# TODO: num_microbatch size = 2 inf loss
+# TODO fix WhisperForConditionalGeneration enable jit fused operato
+# TODO（jianghai) fix fp16
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
-            "num_microbatches": 4,
+            "num_microbatches": 2,
+            "enable_metadata_cache": False,
             "enable_all_optimization": True,
             "use_lazy_init": False,
-            "precision": "fp16",
+            "precision": "fp32",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 4,
-            "enable_all_optimization": False,
+            "enable_metadata_cache": False,
             "use_lazy_init": False,
             "precision": "fp32",
+            "initial_scale": 1,
         },
-        {"tp_size": 4, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
-        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
         {
-            "tp_size": 2,
+            "tp_size": 4,
             "pp_size": 1,
             "enable_all_optimization": True,
             "use_lazy_init": False,
-            "zero_stage": 2,
-            "precision": "fp16",
-            "initial_scale": 1,
+            "precision": "fp32",
         },
         {
             "tp_size": 1,
-            "pp_size": 2,
+            "pp_size": 4,
             "num_microbatches": 4,
-            "enable_all_optimization": True,
+            "enable_metadata_cache": False,
             "use_lazy_init": False,
-            "zero_stage": 1,
-            "precision": "fp16",
-            "initial_scale": 1,
+            "precision": "fp32",
         },
+        # whisper is not supported fp16 for now.
     ],
 )
-def run_vit_test(test_config):
-    # TODO: fix bug when settign lazy_init for Conv2D Layers in ViT models
-
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_vit")
+def run_whisper_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_whisper")
     for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+        if test_config["pp_size"] > 2 and name == "transformers_whisper_for_audio_classification":
+            continue
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
+            "enable_metadata_cache": False,
+            "enable_all_optimization": False,
+            "use_lazy_init": False,
+            "precision": "fp32",
+            "initial_scale": 1,
+        },
+        {
+            "tp_size": 2,
+            "pp_size": 2,
+            "num_microbatches": 2,
+            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
             "initial_scale": 1,
         },
     ],
 )
-def run_vit_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_vit")
+def run_whisper_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_whisper")
 
     for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
-def check_vit(rank, world_size, port):
+def check_whisper(rank, world_size, port):
     disable_existing_loggers()
     colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_vit_test()
+    run_whisper_test()
 
 
-def check_vit_3d(rank, world_size, port):
+def check_whisper_3d(rank, world_size, port):
     disable_existing_loggers()
     colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_vit_3d_test()
+    run_whisper_3d_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_vit():
-    spawn(check_vit, 4)
+def test_whisper():
+    spawn(check_whisper, 4)
 
 
 @pytest.mark.largedist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_vit_3d():
-    spawn(check_vit_3d, 8)
+def test_whisper_3d():
+    spawn(check_whisper_3d, 8)
 
 
 if __name__ == "__main__":
-    test_vit()
-    test_vit_3d()
+    test_whisper()
+    test_whisper_3d()
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_shard_utils.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.25/tests/test_shardformer/test_with_torch_ddp.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_with_torch_ddp.py`

 * *Files identical despite different names*

