# Comparing `tmp/tensordict_nightly-2024.5.8-cp39-cp39-win_amd64.whl.zip` & `tmp/tensordict_nightly-2024.5.9-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,39 +1,39 @@
-Zip file size: 286278 bytes, number of entries: 37
--rw-rw-rw-  2.0 fat     1459 b- defN 24-May-08 13:49 tensordict/__init__.py
--rw-rw-rw-  2.0 fat     6156 b- defN 24-May-08 13:49 tensordict/_contextlib.py
--rw-rw-rw-  2.0 fat   131725 b- defN 24-May-08 13:49 tensordict/_lazy.py
--rw-rw-rw-  2.0 fat     4717 b- defN 24-May-08 13:49 tensordict/_pytree.py
--rw-rw-rw-  2.0 fat   141711 b- defN 24-May-08 13:49 tensordict/_td.py
--rw-rw-rw-  2.0 fat   114176 b- defN 24-May-08 13:51 tensordict/_tensordict.pyd
--rw-rw-rw-  2.0 fat    21419 b- defN 24-May-08 13:49 tensordict/_torch_func.py
--rw-rw-rw-  2.0 fat   272932 b- defN 24-May-08 13:49 tensordict/base.py
--rw-rw-rw-  2.0 fat    17677 b- defN 24-May-08 13:49 tensordict/functional.py
--rw-rw-rw-  2.0 fat    26903 b- defN 24-May-08 13:49 tensordict/memmap.py
--rw-rw-rw-  2.0 fat    46547 b- defN 24-May-08 13:49 tensordict/persistent.py
--rw-rw-rw-  2.0 fat    96016 b- defN 24-May-08 13:49 tensordict/tensorclass.py
--rw-rw-rw-  2.0 fat     1002 b- defN 24-May-08 13:49 tensordict/tensordict.py
--rw-rw-rw-  2.0 fat    76310 b- defN 24-May-08 13:49 tensordict/utils.py
--rw-rw-rw-  2.0 fat       86 b- defN 24-May-08 13:51 tensordict/version.py
--rw-rw-rw-  2.0 fat     1634 b- defN 24-May-08 13:49 tensordict/nn/__init__.py
--rw-rw-rw-  2.0 fat    54882 b- defN 24-May-08 13:49 tensordict/nn/common.py
--rw-rw-rw-  2.0 fat     5940 b- defN 24-May-08 13:49 tensordict/nn/ensemble.py
--rw-rw-rw-  2.0 fat    25890 b- defN 24-May-08 13:49 tensordict/nn/functional_modules.py
--rw-rw-rw-  2.0 fat    37362 b- defN 24-May-08 13:49 tensordict/nn/params.py
--rw-rw-rw-  2.0 fat    26168 b- defN 24-May-08 13:49 tensordict/nn/probabilistic.py
--rw-rw-rw-  2.0 fat    19947 b- defN 24-May-08 13:49 tensordict/nn/sequence.py
--rw-rw-rw-  2.0 fat    13231 b- defN 24-May-08 13:49 tensordict/nn/utils.py
--rw-rw-rw-  2.0 fat      795 b- defN 24-May-08 13:49 tensordict/nn/distributions/__init__.py
--rw-rw-rw-  2.0 fat     6629 b- defN 24-May-08 13:49 tensordict/nn/distributions/composite.py
--rw-rw-rw-  2.0 fat     9924 b- defN 24-May-08 13:49 tensordict/nn/distributions/continuous.py
--rw-rw-rw-  2.0 fat     2667 b- defN 24-May-08 13:49 tensordict/nn/distributions/discrete.py
--rw-rw-rw-  2.0 fat     6694 b- defN 24-May-08 13:49 tensordict/nn/distributions/truncated_normal.py
--rw-rw-rw-  2.0 fat     1266 b- defN 24-May-08 13:49 tensordict/nn/distributions/utils.py
--rw-rw-rw-  2.0 fat      393 b- defN 24-May-08 13:49 tensordict/prototype/__init__.py
--rw-rw-rw-  2.0 fat     7889 b- defN 24-May-08 13:49 tensordict/prototype/fx.py
--rw-rw-rw-  2.0 fat      796 b- defN 24-May-08 13:49 tensordict/prototype/tensorclass.py
--rw-rw-rw-  2.0 fat     1119 b- defN 24-May-08 13:51 tensordict_nightly-2024.5.8.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    22933 b- defN 24-May-08 13:51 tensordict_nightly-2024.5.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-May-08 13:51 tensordict_nightly-2024.5.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 24-May-08 13:51 tensordict_nightly-2024.5.8.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     3170 b- defN 24-May-08 13:51 tensordict_nightly-2024.5.8.dist-info/RECORD
-37 files, 1208276 bytes uncompressed, 281240 bytes compressed:  76.7%
+Zip file size: 287919 bytes, number of entries: 37
+-rw-rw-rw-  2.0 fat     1459 b- defN 24-May-09 13:49 tensordict/__init__.py
+-rw-rw-rw-  2.0 fat     6156 b- defN 24-May-09 13:49 tensordict/_contextlib.py
+-rw-rw-rw-  2.0 fat   131725 b- defN 24-May-09 13:49 tensordict/_lazy.py
+-rw-rw-rw-  2.0 fat     4717 b- defN 24-May-09 13:49 tensordict/_pytree.py
+-rw-rw-rw-  2.0 fat   143110 b- defN 24-May-09 13:49 tensordict/_td.py
+-rw-rw-rw-  2.0 fat   114176 b- defN 24-May-09 13:51 tensordict/_tensordict.pyd
+-rw-rw-rw-  2.0 fat    21419 b- defN 24-May-09 13:49 tensordict/_torch_func.py
+-rw-rw-rw-  2.0 fat   273027 b- defN 24-May-09 13:49 tensordict/base.py
+-rw-rw-rw-  2.0 fat    17677 b- defN 24-May-09 13:49 tensordict/functional.py
+-rw-rw-rw-  2.0 fat    37194 b- defN 24-May-09 13:49 tensordict/memmap.py
+-rw-rw-rw-  2.0 fat    47935 b- defN 24-May-09 13:49 tensordict/persistent.py
+-rw-rw-rw-  2.0 fat    96016 b- defN 24-May-09 13:49 tensordict/tensorclass.py
+-rw-rw-rw-  2.0 fat     1002 b- defN 24-May-09 13:49 tensordict/tensordict.py
+-rw-rw-rw-  2.0 fat    76507 b- defN 24-May-09 13:49 tensordict/utils.py
+-rw-rw-rw-  2.0 fat       86 b- defN 24-May-09 13:51 tensordict/version.py
+-rw-rw-rw-  2.0 fat     1634 b- defN 24-May-09 13:49 tensordict/nn/__init__.py
+-rw-rw-rw-  2.0 fat    54882 b- defN 24-May-09 13:49 tensordict/nn/common.py
+-rw-rw-rw-  2.0 fat     5940 b- defN 24-May-09 13:49 tensordict/nn/ensemble.py
+-rw-rw-rw-  2.0 fat    25890 b- defN 24-May-09 13:49 tensordict/nn/functional_modules.py
+-rw-rw-rw-  2.0 fat    37362 b- defN 24-May-09 13:49 tensordict/nn/params.py
+-rw-rw-rw-  2.0 fat    26168 b- defN 24-May-09 13:49 tensordict/nn/probabilistic.py
+-rw-rw-rw-  2.0 fat    19947 b- defN 24-May-09 13:49 tensordict/nn/sequence.py
+-rw-rw-rw-  2.0 fat    13231 b- defN 24-May-09 13:49 tensordict/nn/utils.py
+-rw-rw-rw-  2.0 fat      795 b- defN 24-May-09 13:49 tensordict/nn/distributions/__init__.py
+-rw-rw-rw-  2.0 fat     6629 b- defN 24-May-09 13:49 tensordict/nn/distributions/composite.py
+-rw-rw-rw-  2.0 fat     9924 b- defN 24-May-09 13:49 tensordict/nn/distributions/continuous.py
+-rw-rw-rw-  2.0 fat     2667 b- defN 24-May-09 13:49 tensordict/nn/distributions/discrete.py
+-rw-rw-rw-  2.0 fat     6694 b- defN 24-May-09 13:49 tensordict/nn/distributions/truncated_normal.py
+-rw-rw-rw-  2.0 fat     1266 b- defN 24-May-09 13:49 tensordict/nn/distributions/utils.py
+-rw-rw-rw-  2.0 fat      393 b- defN 24-May-09 13:49 tensordict/prototype/__init__.py
+-rw-rw-rw-  2.0 fat     7889 b- defN 24-May-09 13:49 tensordict/prototype/fx.py
+-rw-rw-rw-  2.0 fat      796 b- defN 24-May-09 13:49 tensordict/prototype/tensorclass.py
+-rw-rw-rw-  2.0 fat     1119 b- defN 24-May-09 13:51 tensordict_nightly-2024.5.9.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    22933 b- defN 24-May-09 13:51 tensordict_nightly-2024.5.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-May-09 13:51 tensordict_nightly-2024.5.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 24-May-09 13:51 tensordict_nightly-2024.5.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3170 b- defN 24-May-09 13:51 tensordict_nightly-2024.5.9.dist-info/RECORD
+37 files, 1221646 bytes uncompressed, 282881 bytes compressed:  76.8%
```

## zipnote {}

```diff
@@ -90,23 +90,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict_nightly-2024.5.8.dist-info/LICENSE
+Filename: tensordict_nightly-2024.5.9.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict_nightly-2024.5.8.dist-info/METADATA
+Filename: tensordict_nightly-2024.5.9.dist-info/METADATA
 Comment: 
 
-Filename: tensordict_nightly-2024.5.8.dist-info/WHEEL
+Filename: tensordict_nightly-2024.5.9.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict_nightly-2024.5.8.dist-info/top_level.txt
+Filename: tensordict_nightly-2024.5.9.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict_nightly-2024.5.8.dist-info/RECORD
+Filename: tensordict_nightly-2024.5.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/_td.py

```diff
@@ -2184,31 +2184,50 @@
                 continue
             else:
                 # user did specify location and memmap is in wrong place, so we copy
                 def _populate(
                     dest=dest, value=value, key=key, copy_existing=copy_existing
                 ):
                     filename = None if prefix is None else str(prefix / f"{key}.memmap")
+                    if value.is_nested:
+                        shape = value._nested_tensor_size()
+                        # Make the shape a memmap tensor too
+                        if prefix is not None:
+                            shape_filename = Path(filename)
+                            shape_filename = shape_filename.with_suffix(".shape.memmap")
+                            MemoryMappedTensor.from_tensor(
+                                shape,
+                                filename=shape_filename,
+                                copy_existing=copy_existing,
+                                existsok=True,
+                                copy_data=not like,
+                            )
+                    else:
+                        shape = None
                     dest._tensordict[key] = MemoryMappedTensor.from_tensor(
                         value.data if value.requires_grad else value,
                         filename=filename,
                         copy_existing=copy_existing,
                         existsok=True,
                         copy_data=not like,
+                        shape=shape,
                     )
 
                 if executor is None:
                     _populate()
                 else:
                     futures.append(executor.submit(_populate))
                 if prefix is not None:
                     metadata[key] = {
                         "device": str(value.device),
-                        "shape": list(value.shape),
+                        "shape": list(value.shape)
+                        if not value.is_nested
+                        else value._nested_tensor_size().shape,
                         "dtype": str(value.dtype),
+                        "is_nested": value.is_nested,
                     }
 
         if prefix is not None:
             if executor is None:
                 save_metadata(
                     dest,
                     prefix / "meta.json",
@@ -2254,24 +2273,33 @@
                 or shape is None
             ):
                 # invalid dict means
                 continue
             if (
                 device is None or device != torch.device("meta")
             ) and not torch._guards.active_fake_mode():
+                if entry_metadata.get("is_nested", False):
+                    # The shape is the shape of the shape, get the shape from it
+                    shape = MemoryMappedTensor.from_filename(
+                        (prefix / f"{key}.memmap").with_suffix(".shape.memmap"),
+                        shape=shape,
+                        dtype=torch.long,
+                    )
+                else:
+                    shape = torch.Size(shape)
                 tensor = MemoryMappedTensor.from_filename(
                     dtype=_STRDTYPE2DTYPE[dtype],
-                    shape=torch.Size(entry_metadata["shape"]),
+                    shape=shape,
                     filename=str(prefix / f"{key}.memmap"),
                 )
                 if device is not None:
                     tensor = tensor.to(device, non_blocking=True)
             else:
                 tensor = torch.zeros(
-                    torch.Size(entry_metadata["shape"]),
+                    torch.Size(shape),
                     device=device,
                     dtype=_STRDTYPE2DTYPE[dtype],
                 )
             out._set_str(
                 key,
                 tensor,
                 validated=True,
```

## tensordict/base.py

```diff
@@ -6139,16 +6139,18 @@
         """
         from tensordict import TensorDict
 
         return TensorDict(
             {
                 key: value.clone()
                 if not _is_tensor_collection(value.__class__)
+                else value
+                if is_non_tensor(value)
                 else value.to_tensordict()
-                for key, value in self.items()
+                for key, value in self.items(is_leaf=_is_leaf_nontensor)
             },
             device=self.device,
             batch_size=self.batch_size,
             names=self.names if self._has_names() else None,
         )
 
     def clone(self, recurse: bool = True, **kwargs) -> T:
```

## tensordict/memmap.py

```diff
@@ -16,18 +16,24 @@
 from multiprocessing.context import reduction
 from pathlib import Path
 from typing import Any, Callable, overload
 
 import numpy as np
 import torch
 
-from tensordict.utils import implement_for
+from tensordict.utils import _shape, implement_for
 
 from torch.multiprocessing.reductions import ForkingPickler
 
+NESTED_TENSOR_ERR = (
+    "The PyTorch version isn't compatible with memmap "
+    "nested tensors. Please upgrade to a more recent "
+    "version."
+)
+
 
 class MemoryMappedTensor(torch.Tensor):
     """A Memory-mapped Tensor.
 
     Supports filenames or file handlers.
 
     The main advantage of MemoryMappedTensor resides in its serialization methods,
@@ -56,64 +62,79 @@
           ...     memmap_tensor_empty = MemoryMappedTensor.empty_like(tensor, filename=file.name)
           >>> with tempfile.NamedTemporaryFile() as file:
           ...     memmap_tensor_zero = MemoryMappedTensor.zeros_like(tensor, filename=file.name)
           >>> with tempfile.NamedTemporaryFile() as file:
           ...     memmap_tensor = MemoryMappedTensor.ones_like(tensor, filename=file.name)
     """
 
-    _filename: str | Path
-    _handler: _FileHandler
+    _filename: str | Path = None
+    _handler: _FileHandler = None
     _clear: bool
     index: Any
     parent_shape: torch.Size
 
     def __new__(
         cls,
-        tensor_or_file,
+        source,
         *,
         dtype=None,
         shape=None,
         index=None,
         device=None,
         handler=None,
+        filename=None,
     ):
         if device is not None and torch.device(device).type != "cpu":
             raise ValueError(f"{cls} device must be cpu!")
-        if isinstance(tensor_or_file, str):
+        if isinstance(source, str):
+            if filename is not None:
+                raise TypeError("Duplicated filename argument.")
+            filename = source
+            source = None
+        if filename is not None:
             return cls.from_filename(
-                tensor_or_file,
+                filename,
                 dtype,
                 shape,
                 index,
             )
+        elif isinstance(source, torch.StorageBase):
+            return cls.from_storage(
+                source,
+                dtype=dtype,
+                shape=shape,
+                index=index,
+                device=device,
+                handler=handler,
+                filename=filename,
+            )
         elif handler is not None:
             return cls.from_handler(
                 handler,
                 dtype,
                 shape,
                 index,
             )
-        return super().__new__(cls, tensor_or_file)
+        return super().__new__(cls, source)
 
-    def __init__(
-        self, tensor_or_file, handler=None, dtype=None, shape=None, device=None
-    ):
+    def __init__(self, source, handler=None, dtype=None, shape=None, device=None):
         ...
 
     __torch_function__ = torch._C._disabled_torch_function_impl
 
     @classmethod
     def from_tensor(
         cls,
         input,
         *,
         filename=None,
         existsok=False,
         copy_existing=False,
         copy_data=True,
+        shape=None,
     ):
         """Creates a MemoryMappedTensor with the same content as another tensor.
 
         If the tensor is already a MemoryMappedTensor the original tensor is
         returned if the `filename` argument is `None` or if the two paths match.
         In all other cases, a new :class:`MemoryMappedTensor` is produced.
 
@@ -123,20 +144,22 @@
             filename (path to a file): the path to the file where the tensor
                 should be stored. If none is provided, a file handler is used
                 instead.
             existsok (bool, optional): if ``True``, the file will overwrite
                 an existing file. Defaults to ``False``.
             copy_existing (bool, optional): if ``True`` and the provided input
                 is a MemoryMappedTensor with an associated filename, copying
-                the content to the new location is permitted. Otherwise an
-                exception is thown. This behaviour exists to prevent
-                unadvertedly duplicating data on disk.
+                the content to the new location is permitted. Otherwise, an
+                exception is thrown. This behaviour exists to prevent
+                inadvertently duplicating data on disk.
             copy_data (bool, optional): if ``True``, the content of the tensor
                 will be copied on the storage. Defaults to ``True``.
-
+            shape (torch.Size or torch.Tensor): a shape to override the tensor
+                shape. If a tensor is passed, it must represent the nested shapes of a
+                nested tensor.
         """
         if isinstance(input, MemoryMappedTensor):
             if (filename is None and input._filename is None) or (
                 input._filename is not None
                 and filename is not None
                 and Path(filename).absolute() == Path(input.filename).absolute()
             ):
@@ -157,49 +180,134 @@
             raise TypeError(
                 "Convert input to torch.Tensor before calling MemoryMappedTensor.from_tensor."
             )
         if input.requires_grad:
             raise RuntimeError(
                 "MemoryMappedTensor.from_tensor is incompatible with tensor.requires_grad."
             )
-        shape = input.shape
+        if shape is None:
+            shape = _shape(input, nested_shape=True)
+        if isinstance(shape, torch.Tensor):
+            shape_numel = shape.prod(-1).sum()
+        elif isinstance(shape, torch.Size):
+            shape_numel = shape.numel()
+        else:
+            shape_numel = torch.Size(shape).numel()
         if filename is None:
             if input.dtype.is_floating_point:
-                size = torch.finfo(input.dtype).bits // 8 * shape.numel()
+                size = torch.finfo(input.dtype).bits // 8 * shape_numel
             elif input.dtype.is_complex:
                 raise ValueError(
                     "Complex-valued tensors are not supported by MemoryMappedTensor."
                 )
             elif input.dtype == torch.bool:
-                size = shape.numel()
+                size = shape_numel
             else:
                 # assume integer
-                size = torch.iinfo(input.dtype).bits // 8 * shape.numel()
+                size = torch.iinfo(input.dtype).bits // 8 * shape_numel
             handler = _FileHandler(size)
-            out = torch.frombuffer(memoryview(handler.buffer), dtype=input.dtype)
-            out = out.view(shape)
-            out = cls(out)
+            if isinstance(shape, torch.Tensor):
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(NESTED_TENSOR_ERR)
+                result = torch.frombuffer(memoryview(handler.buffer), dtype=input.dtype)
+                if copy_data:
+                    result.untyped_storage().copy_(input.untyped_storage())
+                result = torch._nested_view_from_buffer(
+                    result,
+                    shape,
+                    *offsets_strides,
+                )
+            else:
+                result = torch.frombuffer(memoryview(handler.buffer), dtype=input.dtype)
+                result = result.view(shape)
+            result = cls(result)
         else:
             handler = None
             if not existsok and os.path.exists(str(filename)):
                 raise RuntimeError(f"The file {filename} already exists.")
-            out = cls(
-                torch.from_file(
-                    str(filename), shared=True, dtype=input.dtype, size=shape.numel()
-                ).view(input.shape)
+            result = torch.from_file(
+                str(filename), shared=True, dtype=input.dtype, size=shape_numel
             )
-        out._handler = handler
-        out._filename = filename
-        out.index = None
-        out.parent_shape = input.shape
+            if isinstance(shape, torch.Tensor):
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(NESTED_TENSOR_ERR)
+                if copy_data:
+                    result.untyped_storage().copy_(input.untyped_storage())
+                result = torch._nested_view_from_buffer(
+                    result,
+                    shape,
+                    *offsets_strides,
+                )
+            else:
+                result = result.view(shape)
+            result = cls(result)
+        result._handler = handler
+        result._filename = filename
+        result.index = None
+        result.parent_shape = shape
         if copy_data:
             if hasattr(input, "full_tensor"):
+                # for DTensors, cheaper than importing DTensor every time
                 input = input.full_tensor()
-            out.copy_(input)
-        return out
+            if not result.is_nested:
+                result.copy_(input)
+        return result
+
+    @classmethod
+    def from_storage(
+        cls,
+        storage,
+        *,
+        shape=None,
+        dtype=None,
+        device=None,
+        index=None,
+        filename=None,
+        handler=None,
+    ):
+        tensor = torch.tensor(storage, dtype=dtype, device=device)
+        if shape is not None:
+            if isinstance(shape, torch.Tensor):
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(
+                        "The PyTorch version isn't compatible with memmap "
+                        "nested tensors. Please upgrade to a more recent "
+                        "version."
+                    )
+                tensor = torch._nested_view_from_buffer(
+                    tensor,
+                    shape,
+                    *offsets_strides,
+                )
+            else:
+                tensor = tensor.view(shape)
+
+        tensor = cls(tensor)
+        if filename is not None:
+            tensor._filename = filename
+        elif handler is not None:
+            tensor._handler = handler
+        if index is not None:
+            return tensor[index]
+        return tensor
 
     @property
     def filename(self):
         """The filename of the tensor, if it has one.
 
         Raises an exception otherwise.
         """
@@ -308,14 +416,18 @@
         """
         shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
         if device is not None:
             device = torch.device(device)
             if device.type != "cpu":
                 raise RuntimeError("Only CPU tensors are supported.")
         result = torch.ones((), dtype=dtype, device=device)
+        if isinstance(shape, torch.Tensor):
+            return cls.empty(
+                shape, device=device, dtype=dtype, filename=filename
+            ).fill_(1)
         if shape:
             if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
                 shape = torch.Size(shape[0])
             else:
                 shape = torch.Size(shape)
             result = result.expand(shape)
         return cls.from_tensor(
@@ -349,14 +461,18 @@
                 is provided, a handler is used.
         """
         shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
         if device is not None:
             device = torch.device(device)
             if device.type != "cpu":
                 raise RuntimeError("Only CPU tensors are supported.")
+        if isinstance(shape, torch.Tensor):
+            return cls.empty(
+                shape, device=device, dtype=dtype, filename=filename
+            ).fill_(0)
         result = torch.zeros((), dtype=dtype, device=device)
         if shape:
             if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
                 shape = torch.Size(shape[0])
             else:
                 shape = torch.Size(shape)
             result = result.expand(shape)
@@ -393,14 +509,102 @@
         """
         shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
         if device is not None:
             device = torch.device(device)
             if device.type != "cpu":
                 raise RuntimeError("Only CPU tensors are supported.")
         result = torch.zeros((), dtype=dtype, device=device)
+        if isinstance(shape, torch.Tensor):
+            # nested tensor
+            shape_numel = shape.prod(-1).sum()
+
+            if filename is None:
+                if dtype.is_floating_point:
+                    size = torch.finfo(dtype).bits // 8 * shape_numel
+                elif dtype.is_complex:
+                    raise ValueError(
+                        "Complex-valued tensors are not supported by MemoryMappedTensor."
+                    )
+                elif dtype == torch.bool:
+                    size = shape_numel
+                else:
+                    # assume integer
+                    size = torch.iinfo(dtype).bits // 8 * shape_numel
+                handler = _FileHandler(size)
+
+                # buffer
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(NESTED_TENSOR_ERR)
+                result = torch.frombuffer(memoryview(handler.buffer), dtype=dtype)
+                result = torch._nested_view_from_buffer(
+                    result,
+                    shape,
+                    *offsets_strides,
+                )
+                result = cls(result)
+                result._handler = handler
+                return result
+            else:
+                result = torch.from_file(
+                    str(filename), shared=True, dtype=dtype, size=shape_numel
+                )
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(NESTED_TENSOR_ERR)
+                result = torch._nested_view_from_buffer(
+                    result,
+                    shape,
+                    *offsets_strides,
+                )
+                result = cls(result)
+                result._filename = filename
+                return result
+            return result
+
+        if shape:
+            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
+                shape = torch.Size(shape[0])
+            else:
+                shape = torch.Size(shape)
+            result = result.expand(shape)
+        result = cls.from_tensor(result, filename=filename)
+        return result
+
+    @classmethod
+    def empty_nested(cls, *args, **kwargs):
+        # noqa: D417
+        """Creates a tensor with empty content, specific shape, dtype and filename.
+
+        Args:
+            shape (nested_shape): the shapes of the tensors.
+
+        Keyword Args:
+            dtype (torch.dtype): the dtype of the tensor.
+            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
+                are accepted, any other device will raise an exception.
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+        """
+        shape = kwargs.pop("shape", args[0])
+        args = (torch.Size([]), *args)
+        _, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
+        if device is not None:
+            device = torch.device(device)
+            if device.type != "cpu":
+                raise RuntimeError("Only CPU tensors are supported.")
+        result = torch.zeros((), dtype=dtype, device=device)
         if shape:
             if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
                 shape = torch.Size(shape[0])
             else:
                 shape = torch.Size(shape)
             result = result.expand(shape)
         result = cls.from_tensor(result, filename=filename)
@@ -450,48 +654,93 @@
     def from_filename(cls, filename, dtype, shape, index=None):
         # noqa: D417
         """Loads a MemoryMappedTensor from a given filename.
 
         Args:
             filename (path or equivalent): the path to the file.
             dtype (torch.dtype): the dtype of the tensor.
-            shape (integers or torch.Size): the shape of the tensor.
+            shape (torch.Size or torch.Tensor): the shape of the tensor. If
+                a tensor is provided, it is assumed that the tensor is a nested_tensor
+                instance.
             index (torch-compatible index type): an index to use to build the
                 tensor.
 
         """
-        shape = torch.Size(shape)
-        tensor = torch.from_file(
-            str(filename), shared=True, dtype=dtype, size=shape.numel()
-        ).view(shape)
+        if isinstance(shape, torch.Tensor):
+            func_offset_stride = getattr(
+                torch, "_nested_compute_contiguous_strides_offsets", None
+            )
+            if func_offset_stride is not None:
+                offsets_strides = func_offset_stride(shape)
+            else:
+                raise RuntimeError(
+                    "The PyTorch version isn't compatible with memmap "
+                    "nested tensors. Please upgrade to a more recent "
+                    "version."
+                )
+            tensor = torch.from_file(
+                str(filename), shared=True, dtype=dtype, size=shape.prod(-1).sum().int()
+            )
+            tensor = torch._nested_view_from_buffer(
+                tensor,
+                shape,
+                *offsets_strides,
+            )
+        else:
+            shape = torch.Size(shape)
+            tensor = torch.from_file(
+                str(filename), shared=True, dtype=dtype, size=shape.numel()
+            ).view(shape)
+
         if index is not None:
             tensor = tensor[index]
         out = cls(tensor)
         out._filename = filename
         out._handler = None
         out.index = index
         out.parent_shape = shape
         return out
 
     @classmethod
-    def from_handler(cls, handler, dtype, shape, index):
+    def from_handler(cls, handler, dtype, shape, index=None):
         # noqa: D417
         """Loads a MemoryMappedTensor from a given handler.
 
         Args:
             handler (compatible file handler): the handler for the tensor.
             dtype (torch.dtype): the dtype of the tensor.
-            shape (integers or torch.Size): the shape of the tensor.
-            index (torch-compatible index type): an index to use to build the
+            shape (torch.Size or torch.Tensor): the shape of the tensor. If
+                a tensor is provided, it is assumed that the tensor is a nested_tensor
+                instance.
+            index (torch-compatible index type, optional): an index to use to build the
                 tensor.
 
         """
-        shape = torch.Size(shape)
         out = torch.frombuffer(memoryview(handler.buffer), dtype=dtype)
-        out = torch.reshape(out, shape)
+        if isinstance(shape, torch.Tensor):
+            func_offset_stride = getattr(
+                torch, "_nested_compute_contiguous_strides_offsets", None
+            )
+            if func_offset_stride is not None:
+                offsets_strides = func_offset_stride(shape)
+            else:
+                raise RuntimeError(
+                    "The PyTorch version isn't compatible with memmap "
+                    "nested tensors. Please upgrade to a more recent "
+                    "version."
+                )
+            out = torch._nested_view_from_buffer(
+                out,
+                shape,
+                *offsets_strides,
+            )
+        else:
+            shape = torch.Size(shape)
+            out = torch.reshape(out, shape)
+
         if index is not None:
             out = out[index]
         out = cls(out)
         out._filename = None
         out._handler = handler
         out.index = index
         out.parent_shape = shape
@@ -579,18 +828,18 @@
 
     def _index_wrap(self, tensor, item, check=False):
         if check:
             if tensor.storage().data_ptr() == self.storage().data_ptr():
                 return self._index_wrap(tensor, item)
             return tensor
         tensor = MemoryMappedTensor(tensor)
-        tensor._handler = self._handler
-        tensor._filename = self._filename
+        tensor._handler = getattr(self, "_handler", None)
+        tensor._filename = getattr(self, "_filename", None)
         tensor.index = item
-        tensor.parent_shape = self.parent_shape
+        tensor.parent_shape = getattr(self, "parent_shape", None)
         return tensor
 
     def unbind(self, dim):
         out = super().unbind(dim)
         if dim < 0:
             dim = self.ndim + dim
         index_base = (slice(None),) * dim
@@ -692,24 +941,27 @@
 
 ForkingPickler.register(MemoryMappedTensor, _reduce_memmap)
 
 
 def _proc_args_const(*args, **kwargs):
     if len(args) > 0:
         # then the first (or the N first) args are the shape
-        if len(args) == 1 and not isinstance(args[0], int):
+        if len(args) == 1 and isinstance(args[0], torch.Tensor):
+            shape = args[0]
+        elif len(args) == 1 and not isinstance(args[0], int):
             shape = torch.Size(args[0])
         else:
             shape = torch.Size(args)
     else:
         # we should have a "shape" keyword arg
         shape = kwargs.pop("shape", None)
         if shape is None:
             raise TypeError("Could not find the shape argument in the arguments.")
-        shape = torch.Size(shape)
+        if not isinstance(shape, torch.Tensor):
+            shape = torch.Size(shape)
     return (
         shape,
         kwargs.pop("device", None),
         kwargs.pop("dtype", None),
         kwargs.pop("fill_value", None),
         kwargs.pop("filename", None),
     )
```

## tensordict/persistent.py

```diff
@@ -35,14 +35,15 @@
     _LOCK_ERROR,
     _parse_to,
     _proc_init,
     _split_tensordict,
     cache,
     expand_right,
     IndexType,
+    is_non_tensor,
     lock_blocked,
     NestedKey,
     NUMPY_TO_TORCH_DTYPE_DICT,
 )
 from torch import multiprocessing as mp
 
 _has_h5 = importlib.util.find_spec("h5py", None) is not None
@@ -67,15 +68,18 @@
 class _PersistentTDKeysView(_TensorDictKeysView):
     def __iter__(self):
         if self.include_nested:
             visitor = _Visitor(lambda key: tuple(key.split("/")))
             self.tensordict.file.visit(visitor)
             if self.leaves_only:
                 for key in visitor:
-                    if self.tensordict._get_metadata(key).get("array", None):
+                    metadata = self.tensordict._get_metadata(key)
+                    if metadata.get("array", False) or metadata.get(
+                        "non_tensor", False
+                    ):
                         yield key
             else:
                 yield from visitor
         else:
             yield from self.tensordict._valid_keys()
 
     def __contains__(self, key):
@@ -235,15 +239,18 @@
     def _process_key(self, key):
         key = _unravel_key_to_tuple(key)
         return "/".join(key)
 
     def _check_batch_size(self, batch_size) -> None:
         for key in self.keys(include_nested=True, leaves_only=True):
             key = self._process_key(key)
-            size = self.file[key].shape
+            array = self.file[key]
+            if _is_non_tensor_h5(array):
+                continue
+            size = array.shape
             if torch.Size(size[: len(batch_size)]) != batch_size:
                 raise ValueError(
                     f"batch size and array size mismatch: array.shape={size}, batch_size={batch_size}."
                 )
 
     def _get_array(self, key, default=NO_DEFAULT):
         try:
@@ -260,18 +267,26 @@
 
         if isinstance(array, (h5py.Dataset,)):
             if self.device is not None:
                 device = self.device
             else:
                 device = torch.device("cpu")
             # we convert to an array first to avoid "Creating a tensor from a list of numpy.ndarrays is extremely slow."
-            array = array[()]
-            out = torch.as_tensor(array, device=device)
-            if self._pin_mem:
-                return out.pin_memory()
+            if not _is_non_tensor_h5(array):
+                array = array[()]
+                out = torch.as_tensor(array, device=device)
+                if self._pin_mem:
+                    out = out.pin_memory()
+            else:
+                from tensordict.tensorclass import NonTensorData
+
+                array = array[()]
+                out = NonTensorData(
+                    data=array, device=device, batch_size=self.batch_size
+                )
             return out
         else:
             out = self._nested_tensordicts.get(key, None)
             if out is None:
                 out = self._nested_tensordicts[key] = PersistentTensorDict(
                     group=array,
                     batch_size=self.batch_size,
@@ -353,17 +368,18 @@
                 "dim": len(shape),
                 "array": True,
             }
         elif (
             isinstance(array, (h5py.Dataset,))
             and array.dtype not in NUMPY_TO_TORCH_DTYPE_DICT
         ):
-            return {}
+            return {"non_tensor": True}
         else:
-            shape = self.get(key).shape
+            val = self.get(key)
+            shape = val.shape
             return {
                 "dtype": None,
                 "shape": shape,
                 "dim": len(shape),
                 "array": False,
             }
 
@@ -375,17 +391,23 @@
             return idx.cpu().detach().numpy()
         if isinstance(idx, (range, list)):
             return np.asarray(idx)
         return idx
 
     def __getitem__(self, item):
         if isinstance(item, str) or (
-            isinstance(item, tuple) and all(isinstance(val, str) for val in item)
+            isinstance(item, tuple) and _unravel_key_to_tuple(item)
         ):
-            return self.get(item)
+            result = self.get(item)
+            if is_non_tensor(result):
+                result_data = getattr(result, "data", NO_DEFAULT)
+                if result_data is NO_DEFAULT:
+                    return result.tolist()
+                return result_data
+            return result
         if isinstance(item, list):
             # convert to tensor
             item = torch.tensor(item)
         return self._get_sub_tensordict(item)
 
     __getitems__ = __getitem__
 
@@ -915,14 +937,22 @@
     def _to_numpy(self, value):
         if hasattr(value, "requires_grad") and value.requires_grad:
             raise RuntimeError("Cannot set a tensor that has requires_grad=True.")
         if isinstance(value, torch.Tensor):
             out = value.cpu().detach().numpy()
         elif isinstance(value, dict):
             out = TensorDict(value, [])
+        elif is_non_tensor(value):
+            value = value.data
+            if isinstance(value, str):
+                return value
+            import h5py
+
+            out = np.array(value)
+            out = out.astype(h5py.opaque_dtype(out.dtype))
         elif is_tensor_collection(value):
             out = value
         elif isinstance(value, (np.ndarray,)):
             out = value
         else:
             raise NotImplementedError(
                 f"Cannot set values of type {value} in a PersistentTensorDict."
@@ -1275,7 +1305,21 @@
             return
         for tensor in tensor_data[1:]:
             if tensor["dim"] <= curr_dim or tensor["shape"][curr_dim] != curr_dim_size:
                 source.batch_size = batch_size
                 return
         batch_size.append(curr_dim_size)
         curr_dim += 1
+
+
+def _is_non_tensor_h5(val):
+    import h5py
+
+    dt = val.dtype
+    if (
+        h5py.check_string_dtype(dt)
+        or h5py.check_vlen_dtype(dt)
+        or h5py.check_enum_dtype(dt)
+        or h5py.check_opaque_dtype(dt)
+    ):
+        return True
+    return False
```

## tensordict/utils.py

```diff
@@ -582,22 +582,24 @@
         return tensor.ndimension()
     elif isinstance(tensor, KeyedJaggedTensor):
         return 1
     else:
         return tensor.ndimension()
 
 
-def _shape(tensor: Tensor) -> torch.Size:
+def _shape(tensor: Tensor, nested_shape=False) -> torch.Size:
     if isinstance(tensor, UninitializedTensorMixin):
         return torch.Size([*getattr(tensor, "batch_size", ()), -1])
     elif not isinstance(tensor, Tensor):
         if type(tensor) is KeyedJaggedTensor:
             return torch.Size([len(tensor.lengths()) // len(tensor.keys())])
         return tensor.shape
     if tensor.is_nested:
+        if nested_shape:
+            return tensor._nested_tensor_size()
         shape = []
         for i in range(tensor.ndim):
             try:
                 shape.append(tensor.size(i))
             except RuntimeError:
                 shape.append(-1)
         return torch.Size(shape)
@@ -1549,15 +1551,15 @@
         out.batch_size = torch.Size(
             [*parent_batch_size, *_shape(tensor)[self_batch_dims:]]
         )
         return out
 
 
 def _set_max_batch_size(source: T, batch_dims=None):
-    """Updates a tensordict with its maximium batch size."""
+    """Updates a tensordict with its maximum batch size."""
     from tensordict.base import _is_tensor_collection
 
     tensor_data = [val for val in source.values() if not is_non_tensor(val)]
 
     for val in tensor_data:
         if _is_tensor_collection(val.__class__):
             _set_max_batch_size(val, batch_dims=batch_dims)
@@ -1567,25 +1569,27 @@
         if batch_dims:
             source.batch_size = source.batch_size[:batch_dims]
             return source
         else:
             return source
 
     curr_dim = 0
+    tensor_shapes = [_shape(_tensor_data) for _tensor_data in tensor_data]
+
     while True:
-        if tensor_data[0].dim() > curr_dim:
-            curr_dim_size = tensor_data[0].size(curr_dim)
+        if len(tensor_shapes[0]) > curr_dim:
+            curr_dim_size = tensor_shapes[0][curr_dim]
         else:
             source.batch_size = batch_size
             return
-        for leaf in tensor_data[1:]:
+        for leaf, shape in zip(tensor_data[1:], tensor_shapes[1:]):
             # if we have a nested empty tensordict we can modify its batch size at will
             if _is_tensor_collection(type(leaf)) and leaf.is_empty():
                 continue
-            if (leaf.dim() <= curr_dim) or (leaf.size(curr_dim) != curr_dim_size):
+            if (len(shape) <= curr_dim) or (shape[curr_dim] != curr_dim_size):
                 source.batch_size = batch_size
                 return
         if batch_dims is None or len(batch_size) < batch_dims:
             batch_size.append(curr_dim_size)
         curr_dim += 1
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2024.05.08'
-git_version = '59cd9a289dedc20bc95182304ed54ebfcf577cfe'
+__version__ = '2024.05.09'
+git_version = '04e52a1c5108d4353120368659c01db919fa3175'
```

## Comparing `tensordict_nightly-2024.5.8.dist-info/LICENSE` & `tensordict_nightly-2024.5.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensordict_nightly-2024.5.8.dist-info/METADATA` & `tensordict_nightly-2024.5.9.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tensordict-nightly
-Version: 2024.5.8
+Version: 2024.5.9
 Summary: UNKNOWN
 Home-page: https://github.com/pytorch/tensordict
 Author: tensordict contributors
 Author-email: vmoens@fb.com
 License: BSD
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.8
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: tensordict-nightly Version: 2024.5.8 Summary:
+Metadata-Version: 2.1 Name: tensordict-nightly Version: 2024.5.9 Summary:
 UNKNOWN Home-page: https://github.com/pytorch/tensordict Author: tensordict
 contributors Author-email: vmoens@fb.com License: BSD Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.8 Classifier: Programming
 Language :: Python :: 3.9 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11 Classifier: Development
 Status :: 4 - Beta Description-Content-Type: text/markdown License-File:
 LICENSE Requires-Dist: torch >=2.4.0.dev Requires-Dist: numpy Requires-Dist:
```

## Comparing `tensordict_nightly-2024.5.8.dist-info/RECORD` & `tensordict_nightly-2024.5.9.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 tensordict/__init__.py,sha256=KnyJ9ME65lcfYfNLZKabmHZOhpuZOCXB0SKPVQ0_F9U,1459
 tensordict/_contextlib.py,sha256=yao_SZSgKUJt6dXHtAc5ZFJzAmm_NQQFYgR1rjDg0k8,6156
 tensordict/_lazy.py,sha256=mtx3QJ1t72Xe0zyPS0JNhh1oktrxrqkplOR0LWVGtvg,131725
 tensordict/_pytree.py,sha256=Lku-2u8EQM1Y14OuaWAtXEYN6mZqB_7y9Y0Op7jK1wk,4717
-tensordict/_td.py,sha256=nkIMQM10sT0am2zY8MsPgvGUWVl46wr82YkG5y1WMMc,141711
-tensordict/_tensordict.pyd,sha256=F5wWuQPP9TDrgJ69cKcYkBPOA8dF3w51kuCYb_eMi18,114176
+tensordict/_td.py,sha256=WFWJsrr8G7m3rSD3MOUHCUQVO039EaVdNu7VYRjlKoc,143110
+tensordict/_tensordict.pyd,sha256=PoZVPJcFObY4aR4YDTfGh2CxxVq9OgLvQMqddH_Sdao,114176
 tensordict/_torch_func.py,sha256=TgFFkV7ubmHZmp3tlGoS90U5egSVW7gMFKpGL23mu-o,21419
-tensordict/base.py,sha256=hYNnt0cXHq1dZg8eT0B7WC7pWK9vZHNDPgPykTpEJeY,272932
+tensordict/base.py,sha256=L2B6omhk3zd9ZmnWEzKQehRLzhp8nKGdRGbaiFmgOpA,273027
 tensordict/functional.py,sha256=DYWrV2knplj22L23m1Bs5TDk97ULiHlL7WYTtqAfgHE,17677
-tensordict/memmap.py,sha256=W551cdXn6P9nRG6eY28KE_P-N0b8RnsxkOaB886YMHg,26903
-tensordict/persistent.py,sha256=K9SRVtHTzjMvG0D3m7gPdNfllp8rv-TLRzXsUnRVneQ,46547
+tensordict/memmap.py,sha256=WlSdwwfzZcpMic0gvVZFNCKjwOl6e9kR9H7M1pLxbGQ,37194
+tensordict/persistent.py,sha256=MSARapK_gKsereP4U3xW8jyIU_ipXyE7zfaymN7AnVE,47935
 tensordict/tensorclass.py,sha256=jsM_Cc7zVBdmfsgXW2gm8OO5ETt4mb-3YG6F5HGupa8,96016
 tensordict/tensordict.py,sha256=daJ7LcwpVeWKzGLqz8_M9Gti3BLAb6iRLIhz-yqX3jA,1002
-tensordict/utils.py,sha256=kEZJExZ-BHLRMhSCDYiWgb1ZgMTMRT5GV7WRddeE2po,76310
-tensordict/version.py,sha256=oD8AGBOsjeTsrBVbHAMuLDMqZ2SeuIPQ6D1uzaFgVrE,86
+tensordict/utils.py,sha256=tQ7vgLLDNWdK1JPjoV77vNiiwcArgpE0kqLha2Kh2k4,76507
+tensordict/version.py,sha256=7YriKmGyOntRDSOFCyAxyjpk-uiRJec4Zw4B2EOZiFQ,86
 tensordict/nn/__init__.py,sha256=nWPo4TqDb1hYILbEc73EHVyv3iy2kumYcoUp1MaXS1g,1634
 tensordict/nn/common.py,sha256=RqjFAF4bm5_B9BB0m7xxgAVWjIXdxrUq-Y3imiNkBYk,54882
 tensordict/nn/ensemble.py,sha256=CWyHKGsN7hmr3HjynEPj5xrCvZggeFy_C7iSpZxHlS4,5940
 tensordict/nn/functional_modules.py,sha256=PVZO3Emvzu_XRjNrjdVVTvgj97GF7eQdZoEgyvlQJO4,25890
 tensordict/nn/params.py,sha256=t2-UcMd-Sq5re2fNtnzFDu8wQXJZmUjegfB9NKzoc5w,37362
 tensordict/nn/probabilistic.py,sha256=2R4cCEtNkZlxnivx5FcjYa0K-51h2QpvfnbUWOSaIpM,26168
 tensordict/nn/sequence.py,sha256=mD0oJplRLlKflEGSopeSjc9MzdOfH00FM8Tm17_T7Sw,19947
@@ -26,12 +26,12 @@
 tensordict/nn/distributions/continuous.py,sha256=Ge1qivY1uB3sWBVTFD6wKQbBrvU3tAa5GV28aqCs9l4,9924
 tensordict/nn/distributions/discrete.py,sha256=VrcrSPr9YyBDKagOphYTAgpiQqlwfpf4-8t3TsrJyPQ,2667
 tensordict/nn/distributions/truncated_normal.py,sha256=f--2ISj15TTUlLcUzMh1e50yADuh-vKMFbrzLqwUv7I,6694
 tensordict/nn/distributions/utils.py,sha256=3vEDATr12hUk9OYYKf4dzPmNMmLzKHQuZPsdWcPVfi4,1266
 tensordict/prototype/__init__.py,sha256=XBECOVFLLCiUsvNwwByWkz-U87nt7oJXPp2iIIA5Ysk,393
 tensordict/prototype/fx.py,sha256=AD6zDHD2g24iuP9v-yzWN_NX1xInK5E8RHvkend0amc,7889
 tensordict/prototype/tensorclass.py,sha256=bzbSJ7uaQVxfUS6uzwyh5SOBrhufxsW_dLzjMbQJLvw,796
-tensordict_nightly-2024.5.8.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
-tensordict_nightly-2024.5.8.dist-info/METADATA,sha256=0HXRACaIEwT326WvvLuI9oJ0UsiBBmjvk6v1v1G1Cnk,22933
-tensordict_nightly-2024.5.8.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-tensordict_nightly-2024.5.8.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
-tensordict_nightly-2024.5.8.dist-info/RECORD,,
+tensordict_nightly-2024.5.9.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
+tensordict_nightly-2024.5.9.dist-info/METADATA,sha256=Hxthp2IXOZIa4nJphq7NkN0oNg66pBPkRqq1grJOAkU,22933
+tensordict_nightly-2024.5.9.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
+tensordict_nightly-2024.5.9.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
+tensordict_nightly-2024.5.9.dist-info/RECORD,,
```

